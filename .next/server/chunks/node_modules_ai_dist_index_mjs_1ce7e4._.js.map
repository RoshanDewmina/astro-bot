{
  "version": 3,
  "sources": [],
  "sections": [
    {"offset": {"line": 5, "column": 0}, "map": {"version":3,"sources":["/turbopack/[project]/node_modules/ai/streams/index.ts","/turbopack/[project]/node_modules/ai/core/index.ts","/turbopack/[project]/node_modules/ai/util/retry-with-exponential-backoff.ts","/turbopack/[project]/node_modules/ai/util/delay.ts","/turbopack/[project]/node_modules/ai/util/retry-error.ts","/turbopack/[project]/node_modules/ai/core/telemetry/assemble-operation-name.ts","/turbopack/[project]/node_modules/ai/core/telemetry/get-base-telemetry-attributes.ts","/turbopack/[project]/node_modules/ai/core/telemetry/get-tracer.ts","/turbopack/[project]/node_modules/ai/core/telemetry/noop-tracer.ts","/turbopack/[project]/node_modules/ai/core/telemetry/record-span.ts","/turbopack/[project]/node_modules/ai/core/telemetry/select-telemetry-attributes.ts","/turbopack/[project]/node_modules/ai/core/embed/embed.ts","/turbopack/[project]/node_modules/ai/core/util/split-array.ts","/turbopack/[project]/node_modules/ai/core/embed/embed-many.ts","/turbopack/[project]/node_modules/ai/core/generate-object/generate-object.ts","/turbopack/[project]/node_modules/ai/core/prompt/convert-to-language-model-prompt.ts","/turbopack/[project]/node_modules/ai/util/download-error.ts","/turbopack/[project]/node_modules/ai/util/download.ts","/turbopack/[project]/node_modules/ai/core/util/detect-image-mimetype.ts","/turbopack/[project]/node_modules/ai/core/prompt/data-content.ts","/turbopack/[project]/node_modules/ai/core/prompt/invalid-data-content-error.ts","/turbopack/[project]/node_modules/ai/core/prompt/invalid-message-role-error.ts","/turbopack/[project]/node_modules/ai/core/prompt/split-data-url.ts","/turbopack/[project]/node_modules/ai/errors/invalid-argument-error.ts","/turbopack/[project]/node_modules/ai/core/prompt/prepare-call-settings.ts","/turbopack/[project]/node_modules/ai/core/prompt/validate-prompt.ts","/turbopack/[project]/node_modules/ai/core/prompt/message.ts","/turbopack/[project]/node_modules/ai/core/types/provider-metadata.ts","/turbopack/[project]/node_modules/ai/core/types/json-value.ts","/turbopack/[project]/node_modules/ai/core/prompt/content-part.ts","/turbopack/[project]/node_modules/ai/core/types/usage.ts","/turbopack/[project]/node_modules/ai/core/util/prepare-response-headers.ts","/turbopack/[project]/node_modules/ai/core/generate-object/inject-json-instruction.ts","/turbopack/[project]/node_modules/ai/core/generate-object/no-object-generated-error.ts","/turbopack/[project]/node_modules/ai/core/generate-object/output-strategy.ts","/turbopack/[project]/node_modules/ai/core/util/async-iterable-stream.ts","/turbopack/[project]/node_modules/ai/core/generate-object/validate-object-generation-input.ts","/turbopack/[project]/node_modules/ai/core/generate-object/stream-object.ts","/turbopack/[project]/node_modules/ai/util/create-resolvable-promise.ts","/turbopack/[project]/node_modules/ai/util/delayed-promise.ts","/turbopack/[project]/node_modules/ai/core/util/now.ts","/turbopack/[project]/node_modules/ai/core/util/prepare-outgoing-http-headers.ts","/turbopack/[project]/node_modules/ai/core/util/write-to-server-response.ts","/turbopack/[project]/node_modules/ai/core/generate-text/generate-text.ts","/turbopack/[project]/node_modules/ai/errors/index.ts","/turbopack/[project]/node_modules/ai/errors/invalid-tool-arguments-error.ts","/turbopack/[project]/node_modules/ai/errors/no-such-tool-error.ts","/turbopack/[project]/node_modules/ai/core/prompt/message-conversion-error.ts","/turbopack/[project]/node_modules/ai/core/prompt/prepare-tools-and-tool-choice.ts","/turbopack/[project]/node_modules/ai/core/util/is-non-empty-object.ts","/turbopack/[project]/node_modules/ai/core/util/split-on-last-whitespace.ts","/turbopack/[project]/node_modules/ai/core/util/remove-text-after-last-whitespace.ts","/turbopack/[project]/node_modules/ai/core/generate-text/parse-tool-call.ts","/turbopack/[project]/node_modules/ai/core/generate-text/to-response-messages.ts","/turbopack/[project]/node_modules/ai/core/generate-text/stream-text.ts","/turbopack/[project]/node_modules/ai/core/util/create-stitchable-stream.ts","/turbopack/[project]/node_modules/ai/core/util/merge-streams.ts","/turbopack/[project]/node_modules/ai/core/generate-text/run-tools-transformation.ts","/turbopack/[project]/node_modules/ai/core/middleware/wrap-language-model.ts","/turbopack/[project]/node_modules/ai/core/prompt/attachments-to-parts.ts","/turbopack/[project]/node_modules/ai/core/prompt/convert-to-core-messages.ts","/turbopack/[project]/node_modules/ai/core/registry/custom-provider.ts","/turbopack/[project]/node_modules/ai/core/registry/no-such-provider-error.ts","/turbopack/[project]/node_modules/ai/core/registry/provider-registry.ts","/turbopack/[project]/node_modules/ai/core/tool/tool.ts","/turbopack/[project]/node_modules/ai/core/util/cosine-similarity.ts","/turbopack/[project]/node_modules/ai/streams/ai-stream.ts","/turbopack/[project]/node_modules/ai/streams/stream-data.ts","/turbopack/[project]/node_modules/ai/util/constants.ts","/turbopack/[project]/node_modules/ai/streams/anthropic-stream.ts","/turbopack/[project]/node_modules/ai/streams/assistant-response.ts","/turbopack/[project]/node_modules/ai/streams/aws-bedrock-stream.ts","/turbopack/[project]/node_modules/ai/streams/cohere-stream.ts","/turbopack/[project]/node_modules/ai/streams/google-generative-ai-stream.ts","/turbopack/[project]/node_modules/ai/streams/huggingface-stream.ts","/turbopack/[project]/node_modules/ai/streams/inkeep-stream.ts","/turbopack/[project]/node_modules/ai/streams/langchain-adapter.ts","/turbopack/[project]/node_modules/ai/streams/llamaindex-adapter.ts","/turbopack/[project]/node_modules/ai/streams/langchain-stream.ts","/turbopack/[project]/node_modules/ai/streams/mistral-stream.ts","/turbopack/[project]/node_modules/ai/streams/openai-stream.ts","/turbopack/[project]/node_modules/ai/streams/replicate-stream.ts","/turbopack/[project]/node_modules/ai/streams/stream-to-response.ts","/turbopack/[project]/node_modules/ai/streams/streaming-text-response.ts"],"sourcesContent":["// forwarding exports from ui-utils:\nexport {\n  formatStreamPart,\n  parseStreamPart,\n  readDataStream,\n  processDataProtocolResponse,\n} from '@ai-sdk/ui-utils';\nexport type {\n  AssistantStatus,\n  UseAssistantOptions,\n  Message,\n  CreateMessage,\n  DataMessage,\n  AssistantMessage,\n  JSONValue,\n  ChatRequest,\n  ChatRequestOptions,\n  Function,\n  FunctionCall,\n  FunctionCallHandler,\n  ToolInvocation,\n  Tool,\n  ToolCall,\n  ToolCallHandler,\n  ToolChoice,\n  StreamPart,\n  IdGenerator,\n  RequestOptions,\n  Attachment,\n} from '@ai-sdk/ui-utils';\n\nimport { generateId as generateIdImpl } from '@ai-sdk/provider-utils';\nexport const generateId = generateIdImpl;\n\n/**\n@deprecated Use `generateId` instead.\n */\n// TODO remove nanoid export (breaking change)\nexport const nanoid = generateIdImpl;\n\nexport * from '../core/index';\nexport * from '../errors/index';\n\nexport * from './ai-stream';\nexport * from './anthropic-stream';\nexport * from './assistant-response';\nexport * from './aws-bedrock-stream';\nexport * from './cohere-stream';\nexport * from './google-generative-ai-stream';\nexport * from './huggingface-stream';\nexport * from './inkeep-stream';\nexport * as LangChainAdapter from './langchain-adapter';\nexport * as LlamaIndexAdapter from './llamaindex-adapter';\nexport * from './langchain-stream';\nexport * from './mistral-stream';\nexport * from './openai-stream';\nexport * from './replicate-stream';\nexport * from './stream-data';\nexport * from './stream-to-response';\nexport * from './streaming-text-response';\n","export { jsonSchema } from '@ai-sdk/ui-utils';\nexport type { DeepPartial, Schema } from '@ai-sdk/ui-utils';\nexport * from './embed';\nexport * from './generate-object';\nexport * from './generate-text';\nexport * from './middleware';\nexport * from './prompt';\nexport * from './registry';\nexport * from './tool';\nexport * from './types';\nexport { cosineSimilarity } from './util/cosine-similarity';\n","import { APICallError } from '@ai-sdk/provider';\nimport { getErrorMessage, isAbortError } from '@ai-sdk/provider-utils';\nimport { delay } from './delay';\nimport { RetryError } from './retry-error';\n\nexport type RetryFunction = <OUTPUT>(\n  fn: () => PromiseLike<OUTPUT>,\n) => PromiseLike<OUTPUT>;\n\n/**\nThe `retryWithExponentialBackoff` strategy retries a failed API call with an exponential backoff.\nYou can configure the maximum number of retries, the initial delay, and the backoff factor.\n */\nexport const retryWithExponentialBackoff =\n  ({\n    maxRetries = 2,\n    initialDelayInMs = 2000,\n    backoffFactor = 2,\n  } = {}): RetryFunction =>\n  async <OUTPUT>(f: () => PromiseLike<OUTPUT>) =>\n    _retryWithExponentialBackoff(f, {\n      maxRetries,\n      delayInMs: initialDelayInMs,\n      backoffFactor,\n    });\n\nasync function _retryWithExponentialBackoff<OUTPUT>(\n  f: () => PromiseLike<OUTPUT>,\n  {\n    maxRetries,\n    delayInMs,\n    backoffFactor,\n  }: { maxRetries: number; delayInMs: number; backoffFactor: number },\n  errors: unknown[] = [],\n): Promise<OUTPUT> {\n  try {\n    return await f();\n  } catch (error) {\n    if (isAbortError(error)) {\n      throw error; // don't retry when the request was aborted\n    }\n\n    if (maxRetries === 0) {\n      throw error; // don't wrap the error when retries are disabled\n    }\n\n    const errorMessage = getErrorMessage(error);\n    const newErrors = [...errors, error];\n    const tryNumber = newErrors.length;\n\n    if (tryNumber > maxRetries) {\n      throw new RetryError({\n        message: `Failed after ${tryNumber} attempts. Last error: ${errorMessage}`,\n        reason: 'maxRetriesExceeded',\n        errors: newErrors,\n      });\n    }\n\n    if (\n      error instanceof Error &&\n      APICallError.isAPICallError(error) &&\n      error.isRetryable === true &&\n      tryNumber <= maxRetries\n    ) {\n      await delay(delayInMs);\n      return _retryWithExponentialBackoff(\n        f,\n        { maxRetries, delayInMs: backoffFactor * delayInMs, backoffFactor },\n        newErrors,\n      );\n    }\n\n    if (tryNumber === 1) {\n      throw error; // don't wrap the error when a non-retryable error occurs on the first try\n    }\n\n    throw new RetryError({\n      message: `Failed after ${tryNumber} attempts with non-retryable error: '${errorMessage}'`,\n      reason: 'errorNotRetryable',\n      errors: newErrors,\n    });\n  }\n}\n","export async function delay(delayInMs?: number): Promise<void> {\n  return delayInMs === undefined\n    ? Promise.resolve()\n    : new Promise(resolve => setTimeout(resolve, delayInMs));\n}\n","import { AISDKError } from '@ai-sdk/provider';\n\nconst name = 'AI_RetryError';\nconst marker = `vercel.ai.error.${name}`;\nconst symbol = Symbol.for(marker);\n\nexport type RetryErrorReason =\n  | 'maxRetriesExceeded'\n  | 'errorNotRetryable'\n  | 'abort';\n\nexport class RetryError extends AISDKError {\n  private readonly [symbol] = true; // used in isInstance\n\n  // note: property order determines debugging output\n  readonly reason: RetryErrorReason;\n  readonly lastError: unknown;\n  readonly errors: Array<unknown>;\n\n  constructor({\n    message,\n    reason,\n    errors,\n  }: {\n    message: string;\n    reason: RetryErrorReason;\n    errors: Array<unknown>;\n  }) {\n    super({ name, message });\n\n    this.reason = reason;\n    this.errors = errors;\n\n    // separate our last error to make debugging via log easier:\n    this.lastError = errors[errors.length - 1];\n  }\n\n  static isInstance(error: unknown): error is RetryError {\n    return AISDKError.hasMarker(error, marker);\n  }\n\n  /**\n   * @deprecated use `isInstance` instead\n   */\n  static isRetryError(error: unknown): error is RetryError {\n    return (\n      error instanceof Error &&\n      error.name === name &&\n      typeof (error as RetryError).reason === 'string' &&\n      Array.isArray((error as RetryError).errors)\n    );\n  }\n\n  /**\n   * @deprecated Do not use this method. It will be removed in the next major version.\n   */\n  toJSON() {\n    return {\n      name: this.name,\n      message: this.message,\n      reason: this.reason,\n      lastError: this.lastError,\n      errors: this.errors,\n    };\n  }\n}\n","import { TelemetrySettings } from './telemetry-settings';\n\nexport function assembleOperationName({\n  operationId,\n  telemetry,\n}: {\n  operationId: string;\n  telemetry?: TelemetrySettings;\n}) {\n  return {\n    // standardized operation and resource name:\n    'operation.name': `${operationId}${\n      telemetry?.functionId != null ? ` ${telemetry.functionId}` : ''\n    }`,\n    'resource.name': telemetry?.functionId,\n\n    // detailed, AI SDK specific data:\n    'ai.operationId': operationId,\n    'ai.telemetry.functionId': telemetry?.functionId,\n  };\n}\n","import { Attributes } from '@opentelemetry/api';\nimport { CallSettings } from '../prompt/call-settings';\nimport { TelemetrySettings } from './telemetry-settings';\n\nexport function getBaseTelemetryAttributes({\n  model,\n  settings,\n  telemetry,\n  headers,\n}: {\n  model: { modelId: string; provider: string };\n  settings: Omit<CallSettings, 'abortSignal' | 'headers'>;\n  telemetry: TelemetrySettings | undefined;\n  headers: Record<string, string | undefined> | undefined;\n}): Attributes {\n  return {\n    'ai.model.provider': model.provider,\n    'ai.model.id': model.modelId,\n\n    // settings:\n    ...Object.entries(settings).reduce((attributes, [key, value]) => {\n      attributes[`ai.settings.${key}`] = value;\n      return attributes;\n    }, {} as Attributes),\n\n    // add metadata as attributes:\n    ...Object.entries(telemetry?.metadata ?? {}).reduce(\n      (attributes, [key, value]) => {\n        attributes[`ai.telemetry.metadata.${key}`] = value;\n        return attributes;\n      },\n      {} as Attributes,\n    ),\n\n    // request headers\n    ...Object.entries(headers ?? {}).reduce((attributes, [key, value]) => {\n      if (value !== undefined) {\n        attributes[`ai.request.headers.${key}`] = value;\n      }\n      return attributes;\n    }, {} as Attributes),\n  };\n}\n","import { Tracer, trace } from '@opentelemetry/api';\nimport { noopTracer } from './noop-tracer';\n\n/**\n * Tracer variable for testing. Tests can set this to a mock tracer.\n */\nlet testTracer: Tracer | undefined = undefined;\n\nexport function setTestTracer(tracer: Tracer | undefined) {\n  testTracer = tracer;\n}\n\nexport function getTracer({ isEnabled }: { isEnabled: boolean }): Tracer {\n  if (!isEnabled) {\n    return noopTracer;\n  }\n\n  if (testTracer) {\n    return testTracer;\n  }\n\n  return trace.getTracer('ai');\n}\n","import { Span, SpanContext, Tracer } from '@opentelemetry/api';\n\n/**\n * Tracer implementation that does nothing (null object).\n */\nexport const noopTracer: Tracer = {\n  startSpan(): Span {\n    return noopSpan;\n  },\n\n  startActiveSpan<F extends (span: Span) => unknown>(\n    name: unknown,\n    arg1: unknown,\n    arg2?: unknown,\n    arg3?: F,\n  ): ReturnType<any> {\n    if (typeof arg1 === 'function') {\n      return arg1(noopSpan);\n    }\n    if (typeof arg2 === 'function') {\n      return arg2(noopSpan);\n    }\n    if (typeof arg3 === 'function') {\n      return arg3(noopSpan);\n    }\n  },\n};\n\nconst noopSpan: Span = {\n  spanContext() {\n    return noopSpanContext;\n  },\n  setAttribute() {\n    return this;\n  },\n  setAttributes() {\n    return this;\n  },\n  addEvent() {\n    return this;\n  },\n  addLink() {\n    return this;\n  },\n  addLinks() {\n    return this;\n  },\n  setStatus() {\n    return this;\n  },\n  updateName() {\n    return this;\n  },\n  end() {\n    return this;\n  },\n  isRecording() {\n    return false;\n  },\n  recordException() {\n    return this;\n  },\n};\n\nconst noopSpanContext: SpanContext = {\n  traceId: '',\n  spanId: '',\n  traceFlags: 0,\n};\n","import { Attributes, Span, Tracer, SpanStatusCode } from '@opentelemetry/api';\n\nexport function recordSpan<T>({\n  name,\n  tracer,\n  attributes,\n  fn,\n  endWhenDone = true,\n}: {\n  name: string;\n  tracer: Tracer;\n  attributes: Attributes;\n  fn: (span: Span) => Promise<T>;\n  endWhenDone?: boolean;\n}) {\n  return tracer.startActiveSpan(name, { attributes }, async span => {\n    try {\n      const result = await fn(span);\n\n      if (endWhenDone) {\n        span.end();\n      }\n\n      return result;\n    } catch (error) {\n      try {\n        if (error instanceof Error) {\n          span.recordException({\n            name: error.name,\n            message: error.message,\n            stack: error.stack,\n          });\n          span.setStatus({\n            code: SpanStatusCode.ERROR,\n            message: error.message,\n          });\n        } else {\n          span.setStatus({ code: SpanStatusCode.ERROR });\n        }\n      } finally {\n        // always stop the span when there is an error:\n        span.end();\n      }\n\n      throw error;\n    }\n  });\n}\n","import type { Attributes, AttributeValue } from '@opentelemetry/api';\nimport type { TelemetrySettings } from './telemetry-settings';\n\nexport function selectTelemetryAttributes({\n  telemetry,\n  attributes,\n}: {\n  telemetry?: TelemetrySettings;\n  attributes: {\n    [attributeKey: string]:\n      | AttributeValue\n      | { input: () => AttributeValue | undefined }\n      | { output: () => AttributeValue | undefined }\n      | undefined;\n  };\n}): Attributes {\n  return Object.entries(attributes).reduce((attributes, [key, value]) => {\n    if (value === undefined) {\n      return attributes;\n    }\n\n    // input value, check if it should be recorded:\n    if (\n      typeof value === 'object' &&\n      'input' in value &&\n      typeof value.input === 'function'\n    ) {\n      // default to true:\n      if (telemetry?.recordInputs === false) {\n        return attributes;\n      }\n\n      const result = value.input();\n\n      return result === undefined\n        ? attributes\n        : { ...attributes, [key]: result };\n    }\n\n    // output value, check if it should be recorded:\n    if (\n      typeof value === 'object' &&\n      'output' in value &&\n      typeof value.output === 'function'\n    ) {\n      // default to true:\n      if (telemetry?.recordOutputs === false) {\n        return attributes;\n      }\n\n      const result = value.output();\n\n      return result === undefined\n        ? attributes\n        : { ...attributes, [key]: result };\n    }\n\n    // value is an attribute value already:\n    return { ...attributes, [key]: value };\n  }, {});\n}\n","import { retryWithExponentialBackoff } from '../../util/retry-with-exponential-backoff';\nimport { assembleOperationName } from '../telemetry/assemble-operation-name';\nimport { getBaseTelemetryAttributes } from '../telemetry/get-base-telemetry-attributes';\nimport { getTracer } from '../telemetry/get-tracer';\nimport { recordSpan } from '../telemetry/record-span';\nimport { selectTelemetryAttributes } from '../telemetry/select-telemetry-attributes';\nimport { TelemetrySettings } from '../telemetry/telemetry-settings';\nimport { EmbeddingModel } from '../types';\nimport { EmbedResult } from './embed-result';\n\n/**\nEmbed a value using an embedding model. The type of the value is defined by the embedding model.\n\n@param model - The embedding model to use.\n@param value - The value that should be embedded.\n\n@param maxRetries - Maximum number of retries. Set to 0 to disable retries. Default: 2.\n@param abortSignal - An optional abort signal that can be used to cancel the call.\n@param headers - Additional HTTP headers to be sent with the request. Only applicable for HTTP-based providers.\n\n@returns A result object that contains the embedding, the value, and additional information.\n */\nexport async function embed<VALUE>({\n  model,\n  value,\n  maxRetries,\n  abortSignal,\n  headers,\n  experimental_telemetry: telemetry,\n}: {\n  /**\nThe embedding model to use.\n     */\n  model: EmbeddingModel<VALUE>;\n\n  /**\nThe value that should be embedded.\n   */\n  value: VALUE;\n\n  /**\nMaximum number of retries per embedding model call. Set to 0 to disable retries.\n\n@default 2\n   */\n  maxRetries?: number;\n\n  /**\nAbort signal.\n */\n  abortSignal?: AbortSignal;\n\n  /**\nAdditional headers to include in the request.\nOnly applicable for HTTP-based providers.\n */\n  headers?: Record<string, string>;\n\n  /**\n   * Optional telemetry configuration (experimental).\n   */\n  experimental_telemetry?: TelemetrySettings;\n}): Promise<EmbedResult<VALUE>> {\n  const baseTelemetryAttributes = getBaseTelemetryAttributes({\n    model,\n    telemetry,\n    headers,\n    settings: { maxRetries },\n  });\n\n  const tracer = getTracer({ isEnabled: telemetry?.isEnabled ?? false });\n\n  return recordSpan({\n    name: 'ai.embed',\n    attributes: selectTelemetryAttributes({\n      telemetry,\n      attributes: {\n        ...assembleOperationName({ operationId: 'ai.embed', telemetry }),\n        ...baseTelemetryAttributes,\n        'ai.value': { input: () => JSON.stringify(value) },\n      },\n    }),\n    tracer,\n    fn: async span => {\n      const retry = retryWithExponentialBackoff({ maxRetries });\n\n      const { embedding, usage, rawResponse } = await retry(() =>\n        // nested spans to align with the embedMany telemetry data:\n        recordSpan({\n          name: 'ai.embed.doEmbed',\n          attributes: selectTelemetryAttributes({\n            telemetry,\n            attributes: {\n              ...assembleOperationName({\n                operationId: 'ai.embed.doEmbed',\n                telemetry,\n              }),\n              ...baseTelemetryAttributes,\n              // specific settings that only make sense on the outer level:\n              'ai.values': { input: () => [JSON.stringify(value)] },\n            },\n          }),\n          tracer,\n          fn: async doEmbedSpan => {\n            const modelResponse = await model.doEmbed({\n              values: [value],\n              abortSignal,\n              headers,\n            });\n\n            const embedding = modelResponse.embeddings[0];\n            const usage = modelResponse.usage ?? { tokens: NaN };\n\n            doEmbedSpan.setAttributes(\n              selectTelemetryAttributes({\n                telemetry,\n                attributes: {\n                  'ai.embeddings': {\n                    output: () =>\n                      modelResponse.embeddings.map(embedding =>\n                        JSON.stringify(embedding),\n                      ),\n                  },\n                  'ai.usage.tokens': usage.tokens,\n                },\n              }),\n            );\n\n            return {\n              embedding,\n              usage,\n              rawResponse: modelResponse.rawResponse,\n            };\n          },\n        }),\n      );\n\n      span.setAttributes(\n        selectTelemetryAttributes({\n          telemetry,\n          attributes: {\n            'ai.embedding': { output: () => JSON.stringify(embedding) },\n            'ai.usage.tokens': usage.tokens,\n          },\n        }),\n      );\n\n      return new DefaultEmbedResult({ value, embedding, usage, rawResponse });\n    },\n  });\n}\n\nclass DefaultEmbedResult<VALUE> implements EmbedResult<VALUE> {\n  readonly value: EmbedResult<VALUE>['value'];\n  readonly embedding: EmbedResult<VALUE>['embedding'];\n  readonly usage: EmbedResult<VALUE>['usage'];\n  readonly rawResponse: EmbedResult<VALUE>['rawResponse'];\n\n  constructor(options: {\n    value: EmbedResult<VALUE>['value'];\n    embedding: EmbedResult<VALUE>['embedding'];\n    usage: EmbedResult<VALUE>['usage'];\n    rawResponse?: EmbedResult<VALUE>['rawResponse'];\n  }) {\n    this.value = options.value;\n    this.embedding = options.embedding;\n    this.usage = options.usage;\n    this.rawResponse = options.rawResponse;\n  }\n}\n","/**\n * Splits an array into chunks of a specified size.\n *\n * @template T - The type of elements in the array.\n * @param {T[]} array - The array to split.\n * @param {number} chunkSize - The size of each chunk.\n * @returns {T[][]} - A new array containing the chunks.\n */\nexport function splitArray<T>(array: T[], chunkSize: number): T[][] {\n  if (chunkSize <= 0) {\n    throw new Error('chunkSize must be greater than 0');\n  }\n\n  const result = [];\n  for (let i = 0; i < array.length; i += chunkSize) {\n    result.push(array.slice(i, i + chunkSize));\n  }\n\n  return result;\n}\n","import { retryWithExponentialBackoff } from '../../util/retry-with-exponential-backoff';\nimport { assembleOperationName } from '../telemetry/assemble-operation-name';\nimport { getBaseTelemetryAttributes } from '../telemetry/get-base-telemetry-attributes';\nimport { getTracer } from '../telemetry/get-tracer';\nimport { recordSpan } from '../telemetry/record-span';\nimport { selectTelemetryAttributes } from '../telemetry/select-telemetry-attributes';\nimport { TelemetrySettings } from '../telemetry/telemetry-settings';\nimport { Embedding, EmbeddingModel } from '../types';\nimport { splitArray } from '../util/split-array';\nimport { EmbedManyResult } from './embed-many-result';\n\n/**\nEmbed several values using an embedding model. The type of the value is defined\nby the embedding model.\n\n`embedMany` automatically splits large requests into smaller chunks if the model\nhas a limit on how many embeddings can be generated in a single call.\n\n@param model - The embedding model to use.\n@param values - The values that should be embedded.\n\n@param maxRetries - Maximum number of retries. Set to 0 to disable retries. Default: 2.\n@param abortSignal - An optional abort signal that can be used to cancel the call.\n@param headers - Additional HTTP headers to be sent with the request. Only applicable for HTTP-based providers.\n\n@returns A result object that contains the embeddings, the value, and additional information.\n */\nexport async function embedMany<VALUE>({\n  model,\n  values,\n  maxRetries,\n  abortSignal,\n  headers,\n  experimental_telemetry: telemetry,\n}: {\n  /**\nThe embedding model to use.\n     */\n  model: EmbeddingModel<VALUE>;\n\n  /**\nThe values that should be embedded.\n   */\n  values: Array<VALUE>;\n\n  /**\nMaximum number of retries per embedding model call. Set to 0 to disable retries.\n\n@default 2\n   */\n  maxRetries?: number;\n\n  /**\nAbort signal.\n */\n  abortSignal?: AbortSignal;\n\n  /**\nAdditional headers to include in the request.\nOnly applicable for HTTP-based providers.\n */\n  headers?: Record<string, string>;\n\n  /**\n   * Optional telemetry configuration (experimental).\n   */\n  experimental_telemetry?: TelemetrySettings;\n}): Promise<EmbedManyResult<VALUE>> {\n  const baseTelemetryAttributes = getBaseTelemetryAttributes({\n    model,\n    telemetry,\n    headers,\n    settings: { maxRetries },\n  });\n\n  const tracer = getTracer({ isEnabled: telemetry?.isEnabled ?? false });\n\n  return recordSpan({\n    name: 'ai.embedMany',\n    attributes: selectTelemetryAttributes({\n      telemetry,\n      attributes: {\n        ...assembleOperationName({ operationId: 'ai.embedMany', telemetry }),\n        ...baseTelemetryAttributes,\n        // specific settings that only make sense on the outer level:\n        'ai.values': {\n          input: () => values.map(value => JSON.stringify(value)),\n        },\n      },\n    }),\n    tracer,\n    fn: async span => {\n      const retry = retryWithExponentialBackoff({ maxRetries });\n      const maxEmbeddingsPerCall = model.maxEmbeddingsPerCall;\n\n      // the model has not specified limits on\n      // how many embeddings can be generated in a single call\n      if (maxEmbeddingsPerCall == null) {\n        const { embeddings, usage } = await retry(() => {\n          // nested spans to align with the embedMany telemetry data:\n          return recordSpan({\n            name: 'ai.embedMany.doEmbed',\n            attributes: selectTelemetryAttributes({\n              telemetry,\n              attributes: {\n                ...assembleOperationName({\n                  operationId: 'ai.embedMany.doEmbed',\n                  telemetry,\n                }),\n                ...baseTelemetryAttributes,\n                // specific settings that only make sense on the outer level:\n                'ai.values': {\n                  input: () => values.map(value => JSON.stringify(value)),\n                },\n              },\n            }),\n            tracer,\n            fn: async doEmbedSpan => {\n              const modelResponse = await model.doEmbed({\n                values,\n                abortSignal,\n                headers,\n              });\n\n              const embeddings = modelResponse.embeddings;\n              const usage = modelResponse.usage ?? { tokens: NaN };\n\n              doEmbedSpan.setAttributes(\n                selectTelemetryAttributes({\n                  telemetry,\n                  attributes: {\n                    'ai.embeddings': {\n                      output: () =>\n                        embeddings.map(embedding => JSON.stringify(embedding)),\n                    },\n                    'ai.usage.tokens': usage.tokens,\n                  },\n                }),\n              );\n\n              return { embeddings, usage };\n            },\n          });\n        });\n\n        span.setAttributes(\n          selectTelemetryAttributes({\n            telemetry,\n            attributes: {\n              'ai.embeddings': {\n                output: () =>\n                  embeddings.map(embedding => JSON.stringify(embedding)),\n              },\n              'ai.usage.tokens': usage.tokens,\n            },\n          }),\n        );\n\n        return new DefaultEmbedManyResult({ values, embeddings, usage });\n      }\n\n      // split the values into chunks that are small enough for the model:\n      const valueChunks = splitArray(values, maxEmbeddingsPerCall);\n\n      // serially embed the chunks:\n      const embeddings: Array<Embedding> = [];\n      let tokens = 0;\n\n      for (const chunk of valueChunks) {\n        const { embeddings: responseEmbeddings, usage } = await retry(() => {\n          // nested spans to align with the embedMany telemetry data:\n          return recordSpan({\n            name: 'ai.embedMany.doEmbed',\n            attributes: selectTelemetryAttributes({\n              telemetry,\n              attributes: {\n                ...assembleOperationName({\n                  operationId: 'ai.embedMany.doEmbed',\n                  telemetry,\n                }),\n                ...baseTelemetryAttributes,\n                // specific settings that only make sense on the outer level:\n                'ai.values': {\n                  input: () => chunk.map(value => JSON.stringify(value)),\n                },\n              },\n            }),\n            tracer,\n            fn: async doEmbedSpan => {\n              const modelResponse = await model.doEmbed({\n                values: chunk,\n                abortSignal,\n                headers,\n              });\n\n              const embeddings = modelResponse.embeddings;\n              const usage = modelResponse.usage ?? { tokens: NaN };\n\n              doEmbedSpan.setAttributes(\n                selectTelemetryAttributes({\n                  telemetry,\n                  attributes: {\n                    'ai.embeddings': {\n                      output: () =>\n                        embeddings.map(embedding => JSON.stringify(embedding)),\n                    },\n                    'ai.usage.tokens': usage.tokens,\n                  },\n                }),\n              );\n\n              return { embeddings, usage };\n            },\n          });\n        });\n\n        embeddings.push(...responseEmbeddings);\n        tokens += usage.tokens;\n      }\n\n      span.setAttributes(\n        selectTelemetryAttributes({\n          telemetry,\n          attributes: {\n            'ai.embeddings': {\n              output: () =>\n                embeddings.map(embedding => JSON.stringify(embedding)),\n            },\n            'ai.usage.tokens': tokens,\n          },\n        }),\n      );\n\n      return new DefaultEmbedManyResult({\n        values,\n        embeddings,\n        usage: { tokens },\n      });\n    },\n  });\n}\n\nclass DefaultEmbedManyResult<VALUE> implements EmbedManyResult<VALUE> {\n  readonly values: EmbedManyResult<VALUE>['values'];\n  readonly embeddings: EmbedManyResult<VALUE>['embeddings'];\n  readonly usage: EmbedManyResult<VALUE>['usage'];\n\n  constructor(options: {\n    values: EmbedManyResult<VALUE>['values'];\n    embeddings: EmbedManyResult<VALUE>['embeddings'];\n    usage: EmbedManyResult<VALUE>['usage'];\n  }) {\n    this.values = options.values;\n    this.embeddings = options.embeddings;\n    this.usage = options.usage;\n  }\n}\n","import { JSONValue } from '@ai-sdk/provider';\nimport { createIdGenerator, safeParseJSON } from '@ai-sdk/provider-utils';\nimport { Schema } from '@ai-sdk/ui-utils';\nimport { z } from 'zod';\nimport { retryWithExponentialBackoff } from '../../util/retry-with-exponential-backoff';\nimport { CallSettings } from '../prompt/call-settings';\nimport { convertToLanguageModelPrompt } from '../prompt/convert-to-language-model-prompt';\nimport { prepareCallSettings } from '../prompt/prepare-call-settings';\nimport { Prompt } from '../prompt/prompt';\nimport { validatePrompt } from '../prompt/validate-prompt';\nimport { assembleOperationName } from '../telemetry/assemble-operation-name';\nimport { getBaseTelemetryAttributes } from '../telemetry/get-base-telemetry-attributes';\nimport { getTracer } from '../telemetry/get-tracer';\nimport { recordSpan } from '../telemetry/record-span';\nimport { selectTelemetryAttributes } from '../telemetry/select-telemetry-attributes';\nimport { TelemetrySettings } from '../telemetry/telemetry-settings';\nimport {\n  CallWarning,\n  FinishReason,\n  LanguageModel,\n  LanguageModelResponseMetadata,\n  LogProbs,\n  ProviderMetadata,\n} from '../types';\nimport { calculateLanguageModelUsage } from '../types/usage';\nimport { prepareResponseHeaders } from '../util/prepare-response-headers';\nimport { GenerateObjectResult } from './generate-object-result';\nimport { injectJsonInstruction } from './inject-json-instruction';\nimport { NoObjectGeneratedError } from './no-object-generated-error';\nimport { getOutputStrategy } from './output-strategy';\nimport { validateObjectGenerationInput } from './validate-object-generation-input';\n\nconst originalGenerateId = createIdGenerator({ prefix: 'aiobj-', size: 24 });\n\n/**\nGenerate a structured, typed object for a given prompt and schema using a language model.\n\nThis function does not stream the output. If you want to stream the output, use `streamObject` instead.\n\n@returns\nA result object that contains the generated object, the finish reason, the token usage, and additional information.\n */\nexport async function generateObject<OBJECT>(\n  options: Omit<CallSettings, 'stopSequences'> &\n    Prompt & {\n      output?: 'object' | undefined;\n\n      /**\nThe language model to use.\n     */\n      model: LanguageModel;\n\n      /**\nThe schema of the object that the model should generate.\n     */\n      schema: z.Schema<OBJECT, z.ZodTypeDef, any> | Schema<OBJECT>;\n\n      /**\nOptional name of the output that should be generated.\nUsed by some providers for additional LLM guidance, e.g.\nvia tool or schema name.\n     */\n      schemaName?: string;\n\n      /**\nOptional description of the output that should be generated.\nUsed by some providers for additional LLM guidance, e.g.\nvia tool or schema description.\n     */\n      schemaDescription?: string;\n\n      /**\nThe mode to use for object generation.\n\nThe schema is converted into a JSON schema and used in one of the following ways\n\n- 'auto': The provider will choose the best mode for the model.\n- 'tool': A tool with the JSON schema as parameters is provided and the provider is instructed to use it.\n- 'json': The JSON schema and an instruction are injected into the prompt. If the provider supports JSON mode, it is enabled. If the provider supports JSON grammars, the grammar is used.\n\nPlease note that most providers do not support all modes.\n\nDefault and recommended: 'auto' (best mode for the model).\n     */\n      mode?: 'auto' | 'json' | 'tool';\n\n      /**\nOptional telemetry configuration (experimental).\n       */\n\n      experimental_telemetry?: TelemetrySettings;\n\n      /**\nAdditional provider-specific metadata. They are passed through\nto the provider from the AI SDK and enable provider-specific\nfunctionality that can be fully encapsulated in the provider.\n */\n      experimental_providerMetadata?: ProviderMetadata;\n\n      /**\n       * Internal. For test use only. May change without notice.\n       */\n      _internal?: {\n        generateId?: () => string;\n        currentDate?: () => Date;\n      };\n    },\n): Promise<GenerateObjectResult<OBJECT>>;\n/**\nGenerate an array with structured, typed elements for a given prompt and element schema using a language model.\n\nThis function does not stream the output. If you want to stream the output, use `streamObject` instead.\n\n@return\nA result object that contains the generated object, the finish reason, the token usage, and additional information.\n */\nexport async function generateObject<ELEMENT>(\n  options: Omit<CallSettings, 'stopSequences'> &\n    Prompt & {\n      output: 'array';\n\n      /**\nThe language model to use.\n     */\n      model: LanguageModel;\n\n      /**\nThe element schema of the array that the model should generate.\n */\n      schema: z.Schema<ELEMENT, z.ZodTypeDef, any> | Schema<ELEMENT>;\n\n      /**\nOptional name of the array that should be generated.\nUsed by some providers for additional LLM guidance, e.g.\nvia tool or schema name.\n     */\n      schemaName?: string;\n\n      /**\nOptional description of the array that should be generated.\nUsed by some providers for additional LLM guidance, e.g.\nvia tool or schema description.\n */\n      schemaDescription?: string;\n\n      /**\nThe mode to use for object generation.\n\nThe schema is converted into a JSON schema and used in one of the following ways\n\n- 'auto': The provider will choose the best mode for the model.\n- 'tool': A tool with the JSON schema as parameters is provided and the provider is instructed to use it.\n- 'json': The JSON schema and an instruction are injected into the prompt. If the provider supports JSON mode, it is enabled. If the provider supports JSON grammars, the grammar is used.\n\nPlease note that most providers do not support all modes.\n\nDefault and recommended: 'auto' (best mode for the model).\n     */\n      mode?: 'auto' | 'json' | 'tool';\n\n      /**\nOptional telemetry configuration (experimental).\n     */\n      experimental_telemetry?: TelemetrySettings;\n\n      /**\nAdditional provider-specific metadata. They are passed through\nto the provider from the AI SDK and enable provider-specific\nfunctionality that can be fully encapsulated in the provider.\n */\n      experimental_providerMetadata?: ProviderMetadata;\n\n      /**\n       * Internal. For test use only. May change without notice.\n       */\n      _internal?: {\n        generateId?: () => string;\n        currentDate?: () => Date;\n      };\n    },\n): Promise<GenerateObjectResult<Array<ELEMENT>>>;\n/**\nGenerate a value from an enum (limited list of string values) using a language model.\n\nThis function does not stream the output.\n\n@return\nA result object that contains the generated value, the finish reason, the token usage, and additional information.\n */\nexport async function generateObject<ENUM extends string>(\n  options: Omit<CallSettings, 'stopSequences'> &\n    Prompt & {\n      output: 'enum';\n\n      /**\nThe language model to use.\n     */\n      model: LanguageModel;\n\n      /**\nThe enum values that the model should use.\n     */\n      enum: Array<ENUM>;\n\n      /**\nThe mode to use for object generation.\n\nThe schema is converted into a JSON schema and used in one of the following ways\n\n- 'auto': The provider will choose the best mode for the model.\n- 'tool': A tool with the JSON schema as parameters is provided and the provider is instructed to use it.\n- 'json': The JSON schema and an instruction are injected into the prompt. If the provider supports JSON mode, it is enabled. If the provider supports JSON grammars, the grammar is used.\n\nPlease note that most providers do not support all modes.\n\nDefault and recommended: 'auto' (best mode for the model).\n     */\n      mode?: 'auto' | 'json' | 'tool';\n\n      /**\nOptional telemetry configuration (experimental).\n     */\n      experimental_telemetry?: TelemetrySettings;\n\n      /**\nAdditional provider-specific metadata. They are passed through\nto the provider from the AI SDK and enable provider-specific\nfunctionality that can be fully encapsulated in the provider.\n */\n      experimental_providerMetadata?: ProviderMetadata;\n\n      /**\n       * Internal. For test use only. May change without notice.\n       */\n      _internal?: {\n        generateId?: () => string;\n        currentDate?: () => Date;\n      };\n    },\n): Promise<GenerateObjectResult<ENUM>>;\n/**\nGenerate JSON with any schema for a given prompt using a language model.\n\nThis function does not stream the output. If you want to stream the output, use `streamObject` instead.\n\n@returns\nA result object that contains the generated object, the finish reason, the token usage, and additional information.\n */\nexport async function generateObject(\n  options: Omit<CallSettings, 'stopSequences'> &\n    Prompt & {\n      output: 'no-schema';\n\n      /**\nThe language model to use.\n     */\n      model: LanguageModel;\n\n      /**\nThe mode to use for object generation. Must be \"json\" for no-schema output.\n     */\n      mode?: 'json';\n\n      /**\nOptional telemetry configuration (experimental).\n       */\n      experimental_telemetry?: TelemetrySettings;\n\n      /**\nAdditional provider-specific metadata. They are passed through\nto the provider from the AI SDK and enable provider-specific\nfunctionality that can be fully encapsulated in the provider.\n */\n      experimental_providerMetadata?: ProviderMetadata;\n\n      /**\n       * Internal. For test use only. May change without notice.\n       */\n      _internal?: {\n        generateId?: () => string;\n        currentDate?: () => Date;\n      };\n    },\n): Promise<GenerateObjectResult<JSONValue>>;\nexport async function generateObject<SCHEMA, RESULT>({\n  model,\n  enum: enumValues, // rename bc enum is reserved by typescript\n  schema: inputSchema,\n  schemaName,\n  schemaDescription,\n  mode,\n  output = 'object',\n  system,\n  prompt,\n  messages,\n  maxRetries,\n  abortSignal,\n  headers,\n  experimental_telemetry: telemetry,\n  experimental_providerMetadata: providerMetadata,\n  _internal: {\n    generateId = originalGenerateId,\n    currentDate = () => new Date(),\n  } = {},\n  ...settings\n}: Omit<CallSettings, 'stopSequences'> &\n  Prompt & {\n    /**\n     * The expected structure of the output.\n     *\n     * - 'object': Generate a single object that conforms to the schema.\n     * - 'array': Generate an array of objects that conform to the schema.\n     * - 'no-schema': Generate any JSON object. No schema is specified.\n     *\n     * Default is 'object' if not specified.\n     */\n    output?: 'object' | 'array' | 'enum' | 'no-schema';\n\n    model: LanguageModel;\n    enum?: Array<SCHEMA>;\n    schema?: z.Schema<SCHEMA, z.ZodTypeDef, any> | Schema<SCHEMA>;\n    schemaName?: string;\n    schemaDescription?: string;\n    mode?: 'auto' | 'json' | 'tool';\n    experimental_telemetry?: TelemetrySettings;\n    experimental_providerMetadata?: ProviderMetadata;\n\n    /**\n     * Internal. For test use only. May change without notice.\n     */\n    _internal?: {\n      generateId?: () => string;\n      currentDate?: () => Date;\n    };\n  }): Promise<GenerateObjectResult<RESULT>> {\n  validateObjectGenerationInput({\n    output,\n    mode,\n    schema: inputSchema,\n    schemaName,\n    schemaDescription,\n    enumValues,\n  });\n\n  const outputStrategy = getOutputStrategy({\n    output,\n    schema: inputSchema,\n    enumValues,\n  });\n\n  // automatically set mode to 'json' for no-schema output\n  if (outputStrategy.type === 'no-schema' && mode === undefined) {\n    mode = 'json';\n  }\n\n  const baseTelemetryAttributes = getBaseTelemetryAttributes({\n    model,\n    telemetry,\n    headers,\n    settings: { ...settings, maxRetries },\n  });\n\n  const tracer = getTracer({ isEnabled: telemetry?.isEnabled ?? false });\n  return recordSpan({\n    name: 'ai.generateObject',\n    attributes: selectTelemetryAttributes({\n      telemetry,\n      attributes: {\n        ...assembleOperationName({\n          operationId: 'ai.generateObject',\n          telemetry,\n        }),\n        ...baseTelemetryAttributes,\n        // specific settings that only make sense on the outer level:\n        'ai.prompt': {\n          input: () => JSON.stringify({ system, prompt, messages }),\n        },\n        'ai.schema':\n          outputStrategy.jsonSchema != null\n            ? { input: () => JSON.stringify(outputStrategy.jsonSchema) }\n            : undefined,\n        'ai.schema.name': schemaName,\n        'ai.schema.description': schemaDescription,\n        'ai.settings.output': outputStrategy.type,\n        'ai.settings.mode': mode,\n      },\n    }),\n    tracer,\n    fn: async span => {\n      const retry = retryWithExponentialBackoff({ maxRetries });\n\n      // use the default provider mode when the mode is set to 'auto' or unspecified\n      if (mode === 'auto' || mode == null) {\n        mode = model.defaultObjectGenerationMode;\n      }\n\n      let result: string;\n      let finishReason: FinishReason;\n      let usage: Parameters<typeof calculateLanguageModelUsage>[0];\n      let warnings: CallWarning[] | undefined;\n      let rawResponse: { headers?: Record<string, string> } | undefined;\n      let response: LanguageModelResponseMetadata;\n      let logprobs: LogProbs | undefined;\n      let resultProviderMetadata: ProviderMetadata | undefined;\n\n      switch (mode) {\n        case 'json': {\n          const validatedPrompt = validatePrompt({\n            system:\n              outputStrategy.jsonSchema == null\n                ? injectJsonInstruction({ prompt: system })\n                : model.supportsStructuredOutputs\n                ? system\n                : injectJsonInstruction({\n                    prompt: system,\n                    schema: outputStrategy.jsonSchema,\n                  }),\n            prompt,\n            messages,\n          });\n\n          const promptMessages = await convertToLanguageModelPrompt({\n            prompt: validatedPrompt,\n            modelSupportsImageUrls: model.supportsImageUrls,\n          });\n\n          const inputFormat = validatedPrompt.type;\n\n          const generateResult = await retry(() =>\n            recordSpan({\n              name: 'ai.generateObject.doGenerate',\n              attributes: selectTelemetryAttributes({\n                telemetry,\n                attributes: {\n                  ...assembleOperationName({\n                    operationId: 'ai.generateObject.doGenerate',\n                    telemetry,\n                  }),\n                  ...baseTelemetryAttributes,\n                  'ai.prompt.format': {\n                    input: () => inputFormat,\n                  },\n                  'ai.prompt.messages': {\n                    input: () => JSON.stringify(promptMessages),\n                  },\n                  'ai.settings.mode': mode,\n\n                  // standardized gen-ai llm span attributes:\n                  'gen_ai.system': model.provider,\n                  'gen_ai.request.model': model.modelId,\n                  'gen_ai.request.frequency_penalty': settings.frequencyPenalty,\n                  'gen_ai.request.max_tokens': settings.maxTokens,\n                  'gen_ai.request.presence_penalty': settings.presencePenalty,\n                  'gen_ai.request.temperature': settings.temperature,\n                  'gen_ai.request.top_k': settings.topK,\n                  'gen_ai.request.top_p': settings.topP,\n                },\n              }),\n              tracer,\n              fn: async span => {\n                const result = await model.doGenerate({\n                  mode: {\n                    type: 'object-json',\n                    schema: outputStrategy.jsonSchema,\n                    name: schemaName,\n                    description: schemaDescription,\n                  },\n                  ...prepareCallSettings(settings),\n                  inputFormat,\n                  prompt: promptMessages,\n                  providerMetadata,\n                  abortSignal,\n                  headers,\n                });\n\n                if (result.text === undefined) {\n                  throw new NoObjectGeneratedError();\n                }\n\n                const responseData = {\n                  id: result.response?.id ?? generateId(),\n                  timestamp: result.response?.timestamp ?? currentDate(),\n                  modelId: result.response?.modelId ?? model.modelId,\n                };\n\n                // Add response information to the span:\n                span.setAttributes(\n                  selectTelemetryAttributes({\n                    telemetry,\n                    attributes: {\n                      'ai.response.finishReason': result.finishReason,\n                      'ai.response.object': { output: () => result.text },\n                      'ai.response.id': responseData.id,\n                      'ai.response.model': responseData.modelId,\n                      'ai.response.timestamp':\n                        responseData.timestamp.toISOString(),\n\n                      'ai.usage.promptTokens': result.usage.promptTokens,\n                      'ai.usage.completionTokens':\n                        result.usage.completionTokens,\n\n                      // deprecated:\n                      'ai.finishReason': result.finishReason,\n                      'ai.result.object': { output: () => result.text },\n\n                      // standardized gen-ai llm span attributes:\n                      'gen_ai.response.finish_reasons': [result.finishReason],\n                      'gen_ai.response.id': responseData.id,\n                      'gen_ai.response.model': responseData.modelId,\n                      'gen_ai.usage.prompt_tokens': result.usage.promptTokens,\n                      'gen_ai.usage.completion_tokens':\n                        result.usage.completionTokens,\n                    },\n                  }),\n                );\n\n                return { ...result, objectText: result.text, responseData };\n              },\n            }),\n          );\n\n          result = generateResult.objectText;\n          finishReason = generateResult.finishReason;\n          usage = generateResult.usage;\n          warnings = generateResult.warnings;\n          rawResponse = generateResult.rawResponse;\n          logprobs = generateResult.logprobs;\n          resultProviderMetadata = generateResult.providerMetadata;\n          response = generateResult.responseData;\n\n          break;\n        }\n\n        case 'tool': {\n          const validatedPrompt = validatePrompt({\n            system,\n            prompt,\n            messages,\n          });\n\n          const promptMessages = await convertToLanguageModelPrompt({\n            prompt: validatedPrompt,\n            modelSupportsImageUrls: model.supportsImageUrls,\n          });\n          const inputFormat = validatedPrompt.type;\n\n          const generateResult = await retry(() =>\n            recordSpan({\n              name: 'ai.generateObject.doGenerate',\n              attributes: selectTelemetryAttributes({\n                telemetry,\n                attributes: {\n                  ...assembleOperationName({\n                    operationId: 'ai.generateObject.doGenerate',\n                    telemetry,\n                  }),\n                  ...baseTelemetryAttributes,\n                  'ai.prompt.format': {\n                    input: () => inputFormat,\n                  },\n                  'ai.prompt.messages': {\n                    input: () => JSON.stringify(promptMessages),\n                  },\n                  'ai.settings.mode': mode,\n\n                  // standardized gen-ai llm span attributes:\n                  'gen_ai.system': model.provider,\n                  'gen_ai.request.model': model.modelId,\n                  'gen_ai.request.frequency_penalty': settings.frequencyPenalty,\n                  'gen_ai.request.max_tokens': settings.maxTokens,\n                  'gen_ai.request.presence_penalty': settings.presencePenalty,\n                  'gen_ai.request.temperature': settings.temperature,\n                  'gen_ai.request.top_k': settings.topK,\n                  'gen_ai.request.top_p': settings.topP,\n                },\n              }),\n              tracer,\n              fn: async span => {\n                const result = await model.doGenerate({\n                  mode: {\n                    type: 'object-tool',\n                    tool: {\n                      type: 'function',\n                      name: schemaName ?? 'json',\n                      description:\n                        schemaDescription ?? 'Respond with a JSON object.',\n                      parameters: outputStrategy.jsonSchema!,\n                    },\n                  },\n                  ...prepareCallSettings(settings),\n                  inputFormat,\n                  prompt: promptMessages,\n                  providerMetadata,\n                  abortSignal,\n                  headers,\n                });\n\n                const objectText = result.toolCalls?.[0]?.args;\n\n                if (objectText === undefined) {\n                  throw new NoObjectGeneratedError();\n                }\n\n                const responseData = {\n                  id: result.response?.id ?? generateId(),\n                  timestamp: result.response?.timestamp ?? currentDate(),\n                  modelId: result.response?.modelId ?? model.modelId,\n                };\n\n                // Add response information to the span:\n                span.setAttributes(\n                  selectTelemetryAttributes({\n                    telemetry,\n                    attributes: {\n                      'ai.response.finishReason': result.finishReason,\n                      'ai.response.object': { output: () => objectText },\n                      'ai.response.id': responseData.id,\n                      'ai.response.model': responseData.modelId,\n                      'ai.response.timestamp':\n                        responseData.timestamp.toISOString(),\n\n                      'ai.usage.promptTokens': result.usage.promptTokens,\n                      'ai.usage.completionTokens':\n                        result.usage.completionTokens,\n\n                      // deprecated:\n                      'ai.finishReason': result.finishReason,\n                      'ai.result.object': { output: () => objectText },\n\n                      // standardized gen-ai llm span attributes:\n                      'gen_ai.response.finish_reasons': [result.finishReason],\n                      'gen_ai.response.id': responseData.id,\n                      'gen_ai.response.model': responseData.modelId,\n                      'gen_ai.usage.input_tokens': result.usage.promptTokens,\n                      'gen_ai.usage.output_tokens':\n                        result.usage.completionTokens,\n                    },\n                  }),\n                );\n\n                return { ...result, objectText, responseData };\n              },\n            }),\n          );\n\n          result = generateResult.objectText;\n          finishReason = generateResult.finishReason;\n          usage = generateResult.usage;\n          warnings = generateResult.warnings;\n          rawResponse = generateResult.rawResponse;\n          logprobs = generateResult.logprobs;\n          resultProviderMetadata = generateResult.providerMetadata;\n          response = generateResult.responseData;\n\n          break;\n        }\n\n        case undefined: {\n          throw new Error(\n            'Model does not have a default object generation mode.',\n          );\n        }\n\n        default: {\n          const _exhaustiveCheck: never = mode;\n          throw new Error(`Unsupported mode: ${_exhaustiveCheck}`);\n        }\n      }\n\n      const parseResult = safeParseJSON({ text: result });\n\n      if (!parseResult.success) {\n        throw parseResult.error;\n      }\n\n      const validationResult = outputStrategy.validateFinalResult(\n        parseResult.value,\n      );\n\n      if (!validationResult.success) {\n        throw validationResult.error;\n      }\n\n      // Add response information to the span:\n      span.setAttributes(\n        selectTelemetryAttributes({\n          telemetry,\n          attributes: {\n            'ai.response.finishReason': finishReason,\n            'ai.response.object': {\n              output: () => JSON.stringify(validationResult.value),\n            },\n\n            'ai.usage.promptTokens': usage.promptTokens,\n            'ai.usage.completionTokens': usage.completionTokens,\n\n            // deprecated:\n            'ai.finishReason': finishReason,\n            'ai.result.object': {\n              output: () => JSON.stringify(validationResult.value),\n            },\n          },\n        }),\n      );\n\n      return new DefaultGenerateObjectResult({\n        object: validationResult.value,\n        finishReason,\n        usage: calculateLanguageModelUsage(usage),\n        warnings,\n        response: {\n          ...response,\n          headers: rawResponse?.headers,\n        },\n        logprobs,\n        providerMetadata: resultProviderMetadata,\n      });\n    },\n  });\n}\n\nclass DefaultGenerateObjectResult<T> implements GenerateObjectResult<T> {\n  readonly object: GenerateObjectResult<T>['object'];\n  readonly finishReason: GenerateObjectResult<T>['finishReason'];\n  readonly usage: GenerateObjectResult<T>['usage'];\n  readonly warnings: GenerateObjectResult<T>['warnings'];\n  readonly rawResponse: GenerateObjectResult<T>['rawResponse'];\n  readonly logprobs: GenerateObjectResult<T>['logprobs'];\n  readonly experimental_providerMetadata: GenerateObjectResult<T>['experimental_providerMetadata'];\n  readonly response: GenerateObjectResult<T>['response'];\n\n  constructor(options: {\n    object: GenerateObjectResult<T>['object'];\n    finishReason: GenerateObjectResult<T>['finishReason'];\n    usage: GenerateObjectResult<T>['usage'];\n    warnings: GenerateObjectResult<T>['warnings'];\n    logprobs: GenerateObjectResult<T>['logprobs'];\n    providerMetadata: GenerateObjectResult<T>['experimental_providerMetadata'];\n    response: GenerateObjectResult<T>['response'];\n  }) {\n    this.object = options.object;\n    this.finishReason = options.finishReason;\n    this.usage = options.usage;\n    this.warnings = options.warnings;\n    this.experimental_providerMetadata = options.providerMetadata;\n    this.response = options.response;\n\n    // deprecated:\n    this.rawResponse = {\n      headers: options.response.headers,\n    };\n    this.logprobs = options.logprobs;\n  }\n\n  toJsonResponse(init?: ResponseInit): Response {\n    return new Response(JSON.stringify(this.object), {\n      status: init?.status ?? 200,\n      headers: prepareResponseHeaders(init, {\n        contentType: 'application/json; charset=utf-8',\n      }),\n    });\n  }\n}\n\n/**\n * @deprecated Use `generateObject` instead.\n */\nexport const experimental_generateObject = generateObject;\n","import {\n  LanguageModelV1FilePart,\n  LanguageModelV1ImagePart,\n  LanguageModelV1Message,\n  LanguageModelV1Prompt,\n  LanguageModelV1TextPart,\n} from '@ai-sdk/provider';\nimport {\n  convertUint8ArrayToBase64,\n  getErrorMessage,\n} from '@ai-sdk/provider-utils';\nimport { download } from '../../util/download';\nimport { CoreMessage } from '../prompt/message';\nimport { detectImageMimeType } from '../util/detect-image-mimetype';\nimport { FilePart, ImagePart, TextPart } from './content-part';\nimport {\n  convertDataContentToBase64String,\n  convertDataContentToUint8Array,\n} from './data-content';\nimport { InvalidMessageRoleError } from './invalid-message-role-error';\nimport { splitDataUrl } from './split-data-url';\nimport { ValidatedPrompt } from './validate-prompt';\n\nexport async function convertToLanguageModelPrompt({\n  prompt,\n  modelSupportsImageUrls = true,\n  downloadImplementation = download,\n}: {\n  prompt: ValidatedPrompt;\n  modelSupportsImageUrls: boolean | undefined;\n  downloadImplementation?: typeof download;\n}): Promise<LanguageModelV1Prompt> {\n  const languageModelMessages: LanguageModelV1Prompt = [];\n\n  if (prompt.system != null) {\n    languageModelMessages.push({ role: 'system', content: prompt.system });\n  }\n\n  const downloadedAssets =\n    modelSupportsImageUrls || prompt.messages == null\n      ? null\n      : await downloadAssets(prompt.messages, downloadImplementation);\n\n  const promptType = prompt.type;\n  switch (promptType) {\n    case 'prompt': {\n      languageModelMessages.push({\n        role: 'user',\n        content: [{ type: 'text', text: prompt.prompt }],\n      });\n      break;\n    }\n\n    case 'messages': {\n      languageModelMessages.push(\n        ...prompt.messages.map(\n          (message): LanguageModelV1Message =>\n            convertToLanguageModelMessage(message, downloadedAssets),\n        ),\n      );\n      break;\n    }\n\n    default: {\n      const _exhaustiveCheck: never = promptType;\n      throw new Error(`Unsupported prompt type: ${_exhaustiveCheck}`);\n    }\n  }\n\n  return languageModelMessages;\n}\n\n/**\n * Convert a CoreMessage to a LanguageModelV1Message.\n *\n * @param message The CoreMessage to convert.\n * @param downloadedAssets A map of URLs to their downloaded data. Only\n *   available if the model does not support URLs, null otherwise.\n */\nexport function convertToLanguageModelMessage(\n  message: CoreMessage,\n  downloadedAssets: Record<\n    string,\n    { mimeType: string | undefined; data: Uint8Array }\n  > | null,\n): LanguageModelV1Message {\n  const role = message.role;\n  switch (role) {\n    case 'system': {\n      return {\n        role: 'system',\n        content: message.content,\n        providerMetadata: message.experimental_providerMetadata,\n      };\n    }\n\n    case 'user': {\n      if (typeof message.content === 'string') {\n        return {\n          role: 'user',\n          content: [{ type: 'text', text: message.content }],\n          providerMetadata: message.experimental_providerMetadata,\n        };\n      }\n\n      return {\n        role: 'user',\n        content: message.content\n          .map(\n            (\n              part,\n            ):\n              | LanguageModelV1TextPart\n              | LanguageModelV1ImagePart\n              | LanguageModelV1FilePart => {\n              switch (part.type) {\n                case 'text': {\n                  return {\n                    type: 'text',\n                    text: part.text,\n                    providerMetadata: part.experimental_providerMetadata,\n                  };\n                }\n\n                case 'image': {\n                  if (part.image instanceof URL) {\n                    if (downloadedAssets == null) {\n                      return {\n                        type: 'image',\n                        image: part.image,\n                        mimeType: part.mimeType,\n                        providerMetadata: part.experimental_providerMetadata,\n                      };\n                    } else {\n                      const downloadedImage =\n                        downloadedAssets[part.image.toString()];\n                      return {\n                        type: 'image',\n                        image: downloadedImage.data,\n                        mimeType: part.mimeType ?? downloadedImage.mimeType,\n                        providerMetadata: part.experimental_providerMetadata,\n                      };\n                    }\n                  }\n\n                  // try to convert string image parts to urls\n                  if (typeof part.image === 'string') {\n                    try {\n                      const url = new URL(part.image);\n\n                      switch (url.protocol) {\n                        case 'http:':\n                        case 'https:': {\n                          if (downloadedAssets == null) {\n                            return {\n                              type: 'image',\n                              image: url,\n                              mimeType: part.mimeType,\n                              providerMetadata:\n                                part.experimental_providerMetadata,\n                            };\n                          } else {\n                            const downloadedImage =\n                              downloadedAssets[url.toString()];\n                            return {\n                              type: 'image',\n                              image: downloadedImage.data,\n                              mimeType:\n                                part.mimeType ?? downloadedImage.mimeType,\n                              providerMetadata:\n                                part.experimental_providerMetadata,\n                            };\n                          }\n                        }\n                        case 'data:': {\n                          try {\n                            const { mimeType, base64Content } = splitDataUrl(\n                              part.image,\n                            );\n\n                            if (mimeType == null || base64Content == null) {\n                              throw new Error('Invalid data URL format');\n                            }\n\n                            return {\n                              type: 'image',\n                              image:\n                                convertDataContentToUint8Array(base64Content),\n                              mimeType,\n                              providerMetadata:\n                                part.experimental_providerMetadata,\n                            };\n                          } catch (error) {\n                            throw new Error(\n                              `Error processing data URL: ${getErrorMessage(\n                                message,\n                              )}`,\n                            );\n                          }\n                        }\n                      }\n                    } catch (_ignored) {\n                      // not a URL\n                    }\n                  }\n\n                  const imageUint8 = convertDataContentToUint8Array(part.image);\n\n                  return {\n                    type: 'image',\n                    image: imageUint8,\n                    mimeType: part.mimeType ?? detectImageMimeType(imageUint8),\n                    providerMetadata: part.experimental_providerMetadata,\n                  };\n                }\n\n                case 'file': {\n                  if (part.data instanceof URL) {\n                    if (downloadedAssets == null) {\n                      return {\n                        type: 'file',\n                        data: part.data,\n                        mimeType: part.mimeType,\n                        providerMetadata: part.experimental_providerMetadata,\n                      };\n                    } else {\n                      const downloadedImage =\n                        downloadedAssets[part.data.toString()];\n                      return {\n                        type: 'file',\n                        data: convertUint8ArrayToBase64(downloadedImage.data),\n                        mimeType: part.mimeType ?? downloadedImage.mimeType,\n                        providerMetadata: part.experimental_providerMetadata,\n                      };\n                    }\n                  }\n\n                  // try to convert string image parts to urls\n                  if (typeof part.data === 'string') {\n                    try {\n                      const url = new URL(part.data);\n\n                      switch (url.protocol) {\n                        case 'http:':\n                        case 'https:': {\n                          if (downloadedAssets == null) {\n                            return {\n                              type: 'file',\n                              data: url,\n                              mimeType: part.mimeType,\n                              providerMetadata:\n                                part.experimental_providerMetadata,\n                            };\n                          } else {\n                            const downloadedImage =\n                              downloadedAssets[url.toString()];\n                            return {\n                              type: 'file',\n                              data: convertUint8ArrayToBase64(\n                                downloadedImage.data,\n                              ),\n                              mimeType:\n                                part.mimeType ?? downloadedImage.mimeType,\n                              providerMetadata:\n                                part.experimental_providerMetadata,\n                            };\n                          }\n                        }\n                        case 'data:': {\n                          try {\n                            const { mimeType, base64Content } = splitDataUrl(\n                              part.data,\n                            );\n\n                            if (mimeType == null || base64Content == null) {\n                              throw new Error('Invalid data URL format');\n                            }\n\n                            return {\n                              type: 'file',\n                              data: convertDataContentToBase64String(\n                                base64Content,\n                              ),\n                              mimeType,\n                              providerMetadata:\n                                part.experimental_providerMetadata,\n                            };\n                          } catch (error) {\n                            throw new Error(\n                              `Error processing data URL: ${getErrorMessage(\n                                message,\n                              )}`,\n                            );\n                          }\n                        }\n                      }\n                    } catch (_ignored) {\n                      // not a URL\n                    }\n                  }\n\n                  const imageBase64 = convertDataContentToBase64String(\n                    part.data,\n                  );\n\n                  return {\n                    type: 'file',\n                    data: imageBase64,\n                    mimeType: part.mimeType,\n                    providerMetadata: part.experimental_providerMetadata,\n                  };\n                }\n              }\n            },\n          )\n          // remove empty text parts:\n          .filter(part => part.type !== 'text' || part.text !== ''),\n        providerMetadata: message.experimental_providerMetadata,\n      };\n    }\n\n    case 'assistant': {\n      if (typeof message.content === 'string') {\n        return {\n          role: 'assistant',\n          content: [{ type: 'text', text: message.content }],\n          providerMetadata: message.experimental_providerMetadata,\n        };\n      }\n\n      return {\n        role: 'assistant',\n        content: message.content\n          .filter(\n            // remove empty text parts:\n            part => part.type !== 'text' || part.text !== '',\n          )\n          .map(part => {\n            const { experimental_providerMetadata, ...rest } = part;\n            return {\n              ...rest,\n              providerMetadata: experimental_providerMetadata,\n            };\n          }),\n        providerMetadata: message.experimental_providerMetadata,\n      };\n    }\n\n    case 'tool': {\n      return {\n        role: 'tool',\n        content: message.content.map(part => ({\n          type: 'tool-result',\n          toolCallId: part.toolCallId,\n          toolName: part.toolName,\n          result: part.result,\n          providerMetadata: part.experimental_providerMetadata,\n        })),\n        providerMetadata: message.experimental_providerMetadata,\n      };\n    }\n\n    default: {\n      const _exhaustiveCheck: never = role;\n      throw new InvalidMessageRoleError({ role: _exhaustiveCheck });\n    }\n  }\n}\n\n/**\n * Downloads images and files from URLs in the messages.\n */\nasync function downloadAssets(\n  messages: CoreMessage[],\n  downloadImplementation: typeof download,\n): Promise<Record<string, { mimeType: string | undefined; data: Uint8Array }>> {\n  const urls = messages\n    .filter(message => message.role === 'user')\n    .map(message => message.content)\n    .filter((content): content is Array<TextPart | ImagePart | FilePart> =>\n      Array.isArray(content),\n    )\n    .flat()\n    .filter(\n      (part): part is ImagePart | FilePart =>\n        part.type === 'image' || part.type === 'file',\n    )\n    .map(part => (part.type === 'image' ? part.image : part.data))\n    .map(part =>\n      // support string urls:\n      typeof part === 'string' &&\n      (part.startsWith('http:') || part.startsWith('https:'))\n        ? new URL(part)\n        : part,\n    )\n    .filter((image): image is URL => image instanceof URL);\n\n  // download in parallel:\n  const downloadedImages = await Promise.all(\n    urls.map(async url => ({\n      url,\n      data: await downloadImplementation({ url }),\n    })),\n  );\n\n  return Object.fromEntries(\n    downloadedImages.map(({ url, data }) => [url.toString(), data]),\n  );\n}\n","import { AISDKError } from '@ai-sdk/provider';\n\nconst name = 'AI_DownloadError';\nconst marker = `vercel.ai.error.${name}`;\nconst symbol = Symbol.for(marker);\n\nexport class DownloadError extends AISDKError {\n  private readonly [symbol] = true; // used in isInstance\n\n  readonly url: string;\n  readonly statusCode?: number;\n  readonly statusText?: string;\n\n  constructor({\n    url,\n    statusCode,\n    statusText,\n    cause,\n    message = cause == null\n      ? `Failed to download ${url}: ${statusCode} ${statusText}`\n      : `Failed to download ${url}: ${cause}`,\n  }: {\n    url: string;\n    statusCode?: number;\n    statusText?: string;\n    message?: string;\n    cause?: unknown;\n  }) {\n    super({ name, message, cause });\n\n    this.url = url;\n    this.statusCode = statusCode;\n    this.statusText = statusText;\n  }\n\n  static isInstance(error: unknown): error is DownloadError {\n    return AISDKError.hasMarker(error, marker);\n  }\n\n  /**\n   * @deprecated use `isInstance` instead\n   */\n  static isDownloadError(error: unknown): error is DownloadError {\n    return (\n      error instanceof Error &&\n      error.name === name &&\n      typeof (error as DownloadError).url === 'string' &&\n      ((error as DownloadError).statusCode == null ||\n        typeof (error as DownloadError).statusCode === 'number') &&\n      ((error as DownloadError).statusText == null ||\n        typeof (error as DownloadError).statusText === 'string')\n    );\n  }\n\n  /**\n   * @deprecated Do not use this method. It will be removed in the next major version.\n   */\n  toJSON() {\n    return {\n      name: this.name,\n      message: this.message,\n      url: this.url,\n      statusCode: this.statusCode,\n      statusText: this.statusText,\n      cause: this.cause,\n    };\n  }\n}\n","import { DownloadError } from './download-error';\n\nexport async function download({\n  url,\n  fetchImplementation = fetch,\n}: {\n  url: URL;\n  fetchImplementation?: typeof fetch;\n}): Promise<{\n  data: Uint8Array;\n  mimeType: string | undefined;\n}> {\n  const urlText = url.toString();\n  try {\n    const response = await fetchImplementation(urlText);\n\n    if (!response.ok) {\n      throw new DownloadError({\n        url: urlText,\n        statusCode: response.status,\n        statusText: response.statusText,\n      });\n    }\n\n    return {\n      data: new Uint8Array(await response.arrayBuffer()),\n      mimeType: response.headers.get('content-type') ?? undefined,\n    };\n  } catch (error) {\n    if (DownloadError.isInstance(error)) {\n      throw error;\n    }\n\n    throw new DownloadError({ url: urlText, cause: error });\n  }\n}\n","const mimeTypeSignatures = [\n  { mimeType: 'image/gif' as const, bytes: [0x47, 0x49, 0x46] },\n  { mimeType: 'image/png' as const, bytes: [0x89, 0x50, 0x4e, 0x47] },\n  { mimeType: 'image/jpeg' as const, bytes: [0xff, 0xd8] },\n  { mimeType: 'image/webp' as const, bytes: [0x52, 0x49, 0x46, 0x46] },\n];\n\nexport function detectImageMimeType(\n  image: Uint8Array,\n): 'image/jpeg' | 'image/png' | 'image/gif' | 'image/webp' | undefined {\n  for (const { bytes, mimeType } of mimeTypeSignatures) {\n    if (\n      image.length >= bytes.length &&\n      bytes.every((byte, index) => image[index] === byte)\n    ) {\n      return mimeType;\n    }\n  }\n\n  return undefined;\n}\n","import {\n  convertBase64ToUint8Array,\n  convertUint8ArrayToBase64,\n} from '@ai-sdk/provider-utils';\nimport { InvalidDataContentError } from './invalid-data-content-error';\nimport { z } from 'zod';\n\n/**\nData content. Can either be a base64-encoded string, a Uint8Array, an ArrayBuffer, or a Buffer.\n */\nexport type DataContent = string | Uint8Array | ArrayBuffer | Buffer;\n\n/**\n@internal\n */\nexport const dataContentSchema: z.ZodType<DataContent> = z.union([\n  z.string(),\n  z.instanceof(Uint8Array),\n  z.instanceof(ArrayBuffer),\n  z.custom(\n    // Buffer might not be available in some environments such as CloudFlare:\n    (value: unknown): value is Buffer =>\n      globalThis.Buffer?.isBuffer(value) ?? false,\n    { message: 'Must be a Buffer' },\n  ),\n]);\n\n/**\nConverts data content to a base64-encoded string.\n\n@param content - Data content to convert.\n@returns Base64-encoded string.\n*/\nexport function convertDataContentToBase64String(content: DataContent): string {\n  if (typeof content === 'string') {\n    return content;\n  }\n\n  if (content instanceof ArrayBuffer) {\n    return convertUint8ArrayToBase64(new Uint8Array(content));\n  }\n\n  return convertUint8ArrayToBase64(content);\n}\n\n/**\nConverts data content to a Uint8Array.\n\n@param content - Data content to convert.\n@returns Uint8Array.\n */\nexport function convertDataContentToUint8Array(\n  content: DataContent,\n): Uint8Array {\n  if (content instanceof Uint8Array) {\n    return content;\n  }\n\n  if (typeof content === 'string') {\n    try {\n      return convertBase64ToUint8Array(content);\n    } catch (error) {\n      throw new InvalidDataContentError({\n        message:\n          'Invalid data content. Content string is not a base64-encoded media.',\n        content,\n        cause: error,\n      });\n    }\n  }\n\n  if (content instanceof ArrayBuffer) {\n    return new Uint8Array(content);\n  }\n\n  throw new InvalidDataContentError({ content });\n}\n\n/**\n * Converts a Uint8Array to a string of text.\n *\n * @param uint8Array - The Uint8Array to convert.\n * @returns The converted string.\n */\nexport function convertUint8ArrayToText(uint8Array: Uint8Array): string {\n  try {\n    return new TextDecoder().decode(uint8Array);\n  } catch (error) {\n    throw new Error('Error decoding Uint8Array to text');\n  }\n}\n","import { AISDKError } from '@ai-sdk/provider';\n\nconst name = 'AI_InvalidDataContentError';\nconst marker = `vercel.ai.error.${name}`;\nconst symbol = Symbol.for(marker);\n\nexport class InvalidDataContentError extends AISDKError {\n  private readonly [symbol] = true; // used in isInstance\n\n  readonly content: unknown;\n\n  constructor({\n    content,\n    cause,\n    message = `Invalid data content. Expected a base64 string, Uint8Array, ArrayBuffer, or Buffer, but got ${typeof content}.`,\n  }: {\n    content: unknown;\n    cause?: unknown;\n    message?: string;\n  }) {\n    super({ name, message, cause });\n\n    this.content = content;\n  }\n\n  static isInstance(error: unknown): error is InvalidDataContentError {\n    return AISDKError.hasMarker(error, marker);\n  }\n\n  /**\n   * @deprecated use `isInstance` instead\n   */\n  static isInvalidDataContentError(\n    error: unknown,\n  ): error is InvalidDataContentError {\n    return (\n      error instanceof Error &&\n      error.name === name &&\n      (error as InvalidDataContentError).content != null\n    );\n  }\n\n  /**\n   * @deprecated Do not use this method. It will be removed in the next major version.\n   */\n  toJSON() {\n    return {\n      name: this.name,\n      message: this.message,\n      stack: this.stack,\n      cause: this.cause,\n      content: this.content,\n    };\n  }\n}\n","import { AISDKError } from '@ai-sdk/provider';\n\nconst name = 'AI_InvalidMessageRoleError';\nconst marker = `vercel.ai.error.${name}`;\nconst symbol = Symbol.for(marker);\n\nexport class InvalidMessageRoleError extends AISDKError {\n  private readonly [symbol] = true; // used in isInstance\n\n  readonly role: string;\n\n  constructor({\n    role,\n    message = `Invalid message role: '${role}'. Must be one of: \"system\", \"user\", \"assistant\", \"tool\".`,\n  }: {\n    role: string;\n    message?: string;\n  }) {\n    super({ name, message });\n\n    this.role = role;\n  }\n\n  static isInstance(error: unknown): error is InvalidMessageRoleError {\n    return AISDKError.hasMarker(error, marker);\n  }\n\n  /**\n   * @deprecated use `isInstance` instead\n   */\n  static isInvalidMessageRoleError(\n    error: unknown,\n  ): error is InvalidMessageRoleError {\n    return (\n      error instanceof Error &&\n      error.name === name &&\n      typeof (error as InvalidMessageRoleError).role === 'string'\n    );\n  }\n\n  /**\n   * @deprecated Do not use this method. It will be removed in the next major version.\n   */\n  toJSON() {\n    return {\n      name: this.name,\n      message: this.message,\n      stack: this.stack,\n\n      role: this.role,\n    };\n  }\n}\n","export function splitDataUrl(dataUrl: string): {\n  mimeType: string | undefined;\n  base64Content: string | undefined;\n} {\n  try {\n    const [header, base64Content] = dataUrl.split(',');\n    return {\n      mimeType: header.split(';')[0].split(':')[1],\n      base64Content,\n    };\n  } catch (error) {\n    return {\n      mimeType: undefined,\n      base64Content: undefined,\n    };\n  }\n}\n","import { AISDKError } from '@ai-sdk/provider';\n\nconst name = 'AI_InvalidArgumentError';\nconst marker = `vercel.ai.error.${name}`;\nconst symbol = Symbol.for(marker);\n\nexport class InvalidArgumentError extends AISDKError {\n  private readonly [symbol] = true; // used in isInstance\n\n  readonly parameter: string;\n  readonly value: unknown;\n\n  constructor({\n    parameter,\n    value,\n    message,\n  }: {\n    parameter: string;\n    value: unknown;\n    message: string;\n  }) {\n    super({\n      name,\n      message: `Invalid argument for parameter ${parameter}: ${message}`,\n    });\n\n    this.parameter = parameter;\n    this.value = value;\n  }\n\n  static isInstance(error: unknown): error is InvalidArgumentError {\n    return AISDKError.hasMarker(error, marker);\n  }\n\n  /**\n   * @deprecated use `isInstance` instead\n   */\n  static isInvalidArgumentError(error: unknown): error is InvalidArgumentError {\n    return (\n      error instanceof Error &&\n      error.name === name &&\n      typeof (error as InvalidArgumentError).parameter === 'string' &&\n      typeof (error as InvalidArgumentError).value === 'string'\n    );\n  }\n\n  toJSON() {\n    return {\n      name: this.name,\n      message: this.message,\n      stack: this.stack,\n\n      parameter: this.parameter,\n      value: this.value,\n    };\n  }\n}\n","import { InvalidArgumentError } from '../../errors/invalid-argument-error';\nimport { CallSettings } from './call-settings';\n\n/**\n * Validates call settings and sets default values.\n */\nexport function prepareCallSettings({\n  maxTokens,\n  temperature,\n  topP,\n  topK,\n  presencePenalty,\n  frequencyPenalty,\n  stopSequences,\n  seed,\n  maxRetries,\n}: Omit<CallSettings, 'abortSignal' | 'headers'>): Omit<\n  CallSettings,\n  'abortSignal' | 'headers'\n> {\n  if (maxTokens != null) {\n    if (!Number.isInteger(maxTokens)) {\n      throw new InvalidArgumentError({\n        parameter: 'maxTokens',\n        value: maxTokens,\n        message: 'maxTokens must be an integer',\n      });\n    }\n\n    if (maxTokens < 1) {\n      throw new InvalidArgumentError({\n        parameter: 'maxTokens',\n        value: maxTokens,\n        message: 'maxTokens must be >= 1',\n      });\n    }\n  }\n\n  if (temperature != null) {\n    if (typeof temperature !== 'number') {\n      throw new InvalidArgumentError({\n        parameter: 'temperature',\n        value: temperature,\n        message: 'temperature must be a number',\n      });\n    }\n  }\n\n  if (topP != null) {\n    if (typeof topP !== 'number') {\n      throw new InvalidArgumentError({\n        parameter: 'topP',\n        value: topP,\n        message: 'topP must be a number',\n      });\n    }\n  }\n\n  if (topK != null) {\n    if (typeof topK !== 'number') {\n      throw new InvalidArgumentError({\n        parameter: 'topK',\n        value: topK,\n        message: 'topK must be a number',\n      });\n    }\n  }\n\n  if (presencePenalty != null) {\n    if (typeof presencePenalty !== 'number') {\n      throw new InvalidArgumentError({\n        parameter: 'presencePenalty',\n        value: presencePenalty,\n        message: 'presencePenalty must be a number',\n      });\n    }\n  }\n\n  if (frequencyPenalty != null) {\n    if (typeof frequencyPenalty !== 'number') {\n      throw new InvalidArgumentError({\n        parameter: 'frequencyPenalty',\n        value: frequencyPenalty,\n        message: 'frequencyPenalty must be a number',\n      });\n    }\n  }\n\n  if (seed != null) {\n    if (!Number.isInteger(seed)) {\n      throw new InvalidArgumentError({\n        parameter: 'seed',\n        value: seed,\n        message: 'seed must be an integer',\n      });\n    }\n  }\n\n  if (maxRetries != null) {\n    if (!Number.isInteger(maxRetries)) {\n      throw new InvalidArgumentError({\n        parameter: 'maxRetries',\n        value: maxRetries,\n        message: 'maxRetries must be an integer',\n      });\n    }\n\n    if (maxRetries < 0) {\n      throw new InvalidArgumentError({\n        parameter: 'maxRetries',\n        value: maxRetries,\n        message: 'maxRetries must be >= 0',\n      });\n    }\n  }\n\n  return {\n    maxTokens,\n    temperature: temperature ?? 0,\n    topP,\n    topK,\n    presencePenalty,\n    frequencyPenalty,\n    stopSequences:\n      stopSequences != null && stopSequences.length > 0\n        ? stopSequences\n        : undefined,\n    seed,\n    maxRetries: maxRetries ?? 2,\n  };\n}\n","import { InvalidPromptError } from '@ai-sdk/provider';\nimport { safeValidateTypes } from '@ai-sdk/provider-utils';\nimport { z } from 'zod';\nimport { CoreMessage, coreMessageSchema } from './message';\nimport { Prompt } from './prompt';\n\nexport type ValidatedPrompt =\n  | {\n      type: 'prompt';\n      prompt: string;\n      messages: undefined;\n      system?: string;\n    }\n  | {\n      type: 'messages';\n      prompt: undefined;\n      messages: CoreMessage[];\n      system?: string;\n    };\n\nexport function validatePrompt(prompt: Prompt): ValidatedPrompt {\n  if (prompt.prompt == null && prompt.messages == null) {\n    throw new InvalidPromptError({\n      prompt,\n      message: 'prompt or messages must be defined',\n    });\n  }\n\n  if (prompt.prompt != null && prompt.messages != null) {\n    throw new InvalidPromptError({\n      prompt,\n      message: 'prompt and messages cannot be defined at the same time',\n    });\n  }\n\n  // validate that system is a string\n  if (prompt.system != null && typeof prompt.system !== 'string') {\n    throw new InvalidPromptError({\n      prompt,\n      message: 'system must be a string',\n    });\n  }\n\n  // type: prompt\n  if (prompt.prompt != null) {\n    // validate that prompt is a string\n    if (typeof prompt.prompt !== 'string') {\n      throw new InvalidPromptError({\n        prompt,\n        message: 'prompt must be a string',\n      });\n    }\n\n    return {\n      type: 'prompt',\n      prompt: prompt.prompt,\n      messages: undefined,\n      system: prompt.system,\n    };\n  }\n\n  // type: messages\n  if (prompt.messages != null) {\n    const validationResult = safeValidateTypes({\n      value: prompt.messages,\n      schema: z.array(coreMessageSchema),\n    });\n\n    if (!validationResult.success) {\n      throw new InvalidPromptError({\n        prompt,\n        message: 'messages must be an array of CoreMessage',\n        cause: validationResult.error,\n      });\n    }\n\n    return {\n      type: 'messages',\n      prompt: undefined,\n      messages: prompt.messages!, // only possible case bc of checks above\n      system: prompt.system,\n    };\n  }\n\n  throw new Error('unreachable');\n}\n","import { z } from 'zod';\nimport { ProviderMetadata } from '../types';\nimport { providerMetadataSchema } from '../types/provider-metadata';\nimport {\n  FilePart,\n  filePartSchema,\n  ImagePart,\n  imagePartSchema,\n  TextPart,\n  textPartSchema,\n  ToolCallPart,\n  toolCallPartSchema,\n  ToolResultPart,\n  toolResultPartSchema,\n} from './content-part';\n\n/**\n A system message. It can contain system information.\n\n Note: using the \"system\" part of the prompt is strongly preferred\n to increase the resilience against prompt injection attacks,\n and because not all providers support several system messages.\n */\nexport type CoreSystemMessage = {\n  role: 'system';\n  content: string;\n\n  /**\nAdditional provider-specific metadata. They are passed through\nto the provider from the AI SDK and enable provider-specific\nfunctionality that can be fully encapsulated in the provider.\n */\n  experimental_providerMetadata?: ProviderMetadata;\n};\n\nexport const coreSystemMessageSchema: z.ZodType<CoreSystemMessage> = z.object({\n  role: z.literal('system'),\n  content: z.string(),\n  experimental_providerMetadata: providerMetadataSchema.optional(),\n});\n\n/**\n * @deprecated Use `CoreMessage` instead.\n */\nexport type ExperimentalMessage = CoreMessage;\n\n/**\nA user message. It can contain text or a combination of text and images.\n */\nexport type CoreUserMessage = {\n  role: 'user';\n  content: UserContent;\n\n  /**\nAdditional provider-specific metadata. They are passed through\nto the provider from the AI SDK and enable provider-specific\nfunctionality that can be fully encapsulated in the provider.\n */\n  experimental_providerMetadata?: ProviderMetadata;\n};\n\nexport const coreUserMessageSchema: z.ZodType<CoreUserMessage> = z.object({\n  role: z.literal('user'),\n  content: z.union([\n    z.string(),\n    z.array(z.union([textPartSchema, imagePartSchema, filePartSchema])),\n  ]),\n  experimental_providerMetadata: providerMetadataSchema.optional(),\n});\n\n/**\n * @deprecated Use `CoreUserMessage` instead.\n */\nexport type ExperimentalUserMessage = CoreUserMessage;\n\n/**\nContent of a user message. It can be a string or an array of text and image parts.\n */\nexport type UserContent = string | Array<TextPart | ImagePart | FilePart>;\n\n/**\nAn assistant message. It can contain text, tool calls, or a combination of text and tool calls.\n */\nexport type CoreAssistantMessage = {\n  role: 'assistant';\n  content: AssistantContent;\n\n  /**\nAdditional provider-specific metadata. They are passed through\nto the provider from the AI SDK and enable provider-specific\nfunctionality that can be fully encapsulated in the provider.\n */\n  experimental_providerMetadata?: ProviderMetadata;\n};\n\nexport const coreAssistantMessageSchema: z.ZodType<CoreAssistantMessage> =\n  z.object({\n    role: z.literal('assistant'),\n    content: z.union([\n      z.string(),\n      z.array(z.union([textPartSchema, toolCallPartSchema])),\n    ]),\n    experimental_providerMetadata: providerMetadataSchema.optional(),\n  });\n\n/**\n * @deprecated Use `CoreAssistantMessage` instead.\n */\nexport type ExperimentalAssistantMessage = CoreAssistantMessage;\n\n/**\nContent of an assistant message. It can be a string or an array of text and tool call parts.\n */\nexport type AssistantContent = string | Array<TextPart | ToolCallPart>;\n\n/**\nA tool message. It contains the result of one or more tool calls.\n */\nexport type CoreToolMessage = {\n  role: 'tool';\n  content: ToolContent;\n\n  /**\nAdditional provider-specific metadata. They are passed through\nto the provider from the AI SDK and enable provider-specific\nfunctionality that can be fully encapsulated in the provider.\n */\n  experimental_providerMetadata?: ProviderMetadata;\n};\n\nexport const coreToolMessageSchema: z.ZodType<CoreToolMessage> = z.object({\n  role: z.literal('tool'),\n  content: z.array(toolResultPartSchema),\n  experimental_providerMetadata: providerMetadataSchema.optional(),\n});\n\n/**\n * @deprecated Use `CoreToolMessage` instead.\n */\nexport type ExperimentalToolMessage = CoreToolMessage;\n\n/**\nContent of a tool message. It is an array of tool result parts.\n */\nexport type ToolContent = Array<ToolResultPart>;\n\n/**\nA message that can be used in the `messages` field of a prompt.\nIt can be a user message, an assistant message, or a tool message.\n */\nexport type CoreMessage =\n  | CoreSystemMessage\n  | CoreUserMessage\n  | CoreAssistantMessage\n  | CoreToolMessage;\n\nexport const coreMessageSchema: z.ZodType<CoreMessage> = z.union([\n  coreSystemMessageSchema,\n  coreUserMessageSchema,\n  coreAssistantMessageSchema,\n  coreToolMessageSchema,\n]);\n","import { LanguageModelV1ProviderMetadata } from '@ai-sdk/provider';\nimport { z } from 'zod';\nimport { jsonValueSchema } from './json-value';\n\n/**\nAdditional provider-specific metadata. They are passed through\nto the provider from the AI SDK and enable provider-specific\nfunctionality that can be fully encapsulated in the provider.\n */\nexport type ProviderMetadata = LanguageModelV1ProviderMetadata;\n\nexport const providerMetadataSchema: z.ZodType<ProviderMetadata> = z.record(\n  z.string(),\n  z.record(z.string(), jsonValueSchema),\n);\n","import { JSONValue } from '@ai-sdk/provider';\nimport { z } from 'zod';\n\nexport const jsonValueSchema: z.ZodType<JSONValue> = z.lazy(() =>\n  z.union([\n    z.null(),\n    z.string(),\n    z.number(),\n    z.boolean(),\n    z.record(z.string(), jsonValueSchema),\n    z.array(jsonValueSchema),\n  ]),\n);\n","import { z } from 'zod';\nimport {\n  ProviderMetadata,\n  providerMetadataSchema,\n} from '../types/provider-metadata';\nimport { DataContent, dataContentSchema } from './data-content';\n\n/**\nText content part of a prompt. It contains a string of text.\n */\nexport interface TextPart {\n  type: 'text';\n\n  /**\nThe text content.\n   */\n  text: string;\n\n  /**\nAdditional provider-specific metadata. They are passed through\nto the provider from the AI SDK and enable provider-specific\nfunctionality that can be fully encapsulated in the provider.\n */\n  experimental_providerMetadata?: ProviderMetadata;\n}\n\nexport const textPartSchema: z.ZodType<TextPart> = z.object({\n  type: z.literal('text'),\n  text: z.string(),\n  experimental_providerMetadata: providerMetadataSchema.optional(),\n});\n\n/**\nImage content part of a prompt. It contains an image.\n */\nexport interface ImagePart {\n  type: 'image';\n\n  /**\nImage data. Can either be:\n\n- data: a base64-encoded string, a Uint8Array, an ArrayBuffer, or a Buffer\n- URL: a URL that points to the image\n   */\n  image: DataContent | URL;\n\n  /**\nOptional mime type of the image.\n   */\n  mimeType?: string;\n\n  /**\nAdditional provider-specific metadata. They are passed through\nto the provider from the AI SDK and enable provider-specific\nfunctionality that can be fully encapsulated in the provider.\n */\n  experimental_providerMetadata?: ProviderMetadata;\n}\n\nexport const imagePartSchema: z.ZodType<ImagePart> = z.object({\n  type: z.literal('image'),\n  image: z.union([dataContentSchema, z.instanceof(URL)]),\n  mimeType: z.string().optional(),\n  experimental_providerMetadata: providerMetadataSchema.optional(),\n});\n\n/**\nFile content part of a prompt. It contains a file.\n */\nexport interface FilePart {\n  type: 'file';\n\n  /**\nFile data. Can either be:\n\n- data: a base64-encoded string, a Uint8Array, an ArrayBuffer, or a Buffer\n- URL: a URL that points to the image\n   */\n  data: DataContent | URL;\n\n  /**\nMime type of the file.\n   */\n  mimeType: string;\n\n  /**\nAdditional provider-specific metadata. They are passed through\nto the provider from the AI SDK and enable provider-specific\nfunctionality that can be fully encapsulated in the provider.\n */\n  experimental_providerMetadata?: ProviderMetadata;\n}\n\nexport const filePartSchema: z.ZodType<FilePart> = z.object({\n  type: z.literal('file'),\n  data: z.union([dataContentSchema, z.instanceof(URL)]),\n  mimeType: z.string(),\n  experimental_providerMetadata: providerMetadataSchema.optional(),\n});\n\n/**\nTool call content part of a prompt. It contains a tool call (usually generated by the AI model).\n */\nexport interface ToolCallPart {\n  type: 'tool-call';\n\n  /**\nID of the tool call. This ID is used to match the tool call with the tool result.\n */\n  toolCallId: string;\n\n  /**\nName of the tool that is being called.\n */\n  toolName: string;\n\n  /**\nArguments of the tool call. This is a JSON-serializable object that matches the tool's input schema.\n   */\n  args: unknown;\n\n  /**\nAdditional provider-specific metadata. They are passed through\nto the provider from the AI SDK and enable provider-specific\nfunctionality that can be fully encapsulated in the provider.\n */\n  experimental_providerMetadata?: ProviderMetadata;\n}\n\nexport const toolCallPartSchema: z.ZodType<ToolCallPart> = z.object({\n  type: z.literal('tool-call'),\n  toolCallId: z.string(),\n  toolName: z.string(),\n  args: z.unknown(),\n}) as z.ZodType<ToolCallPart>; // necessary bc args is optional on Zod type\n\n/**\nTool result content part of a prompt. It contains the result of the tool call with the matching ID.\n */\nexport interface ToolResultPart {\n  type: 'tool-result';\n\n  /**\nID of the tool call that this result is associated with.\n */\n  toolCallId: string;\n\n  /**\nName of the tool that generated this result.\n  */\n  toolName: string;\n\n  /**\nResult of the tool call. This is a JSON-serializable object.\n   */\n  result: unknown;\n\n  /**\nOptional flag if the result is an error or an error message.\n   */\n  isError?: boolean;\n\n  /**\nAdditional provider-specific metadata. They are passed through\nto the provider from the AI SDK and enable provider-specific\nfunctionality that can be fully encapsulated in the provider.\n */\n  experimental_providerMetadata?: ProviderMetadata;\n}\n\nexport const toolResultPartSchema: z.ZodType<ToolResultPart> = z.object({\n  type: z.literal('tool-result'),\n  toolCallId: z.string(),\n  toolName: z.string(),\n  result: z.unknown(),\n  isError: z.boolean().optional(),\n  experimental_providerMetadata: providerMetadataSchema.optional(),\n}) as z.ZodType<ToolResultPart>; // necessary bc result is optional on Zod type\n","/**\nRepresents the number of tokens used in a prompt and completion.\n */\nexport type LanguageModelUsage = {\n  /**\nThe number of tokens used in the prompt.\n   */\n  promptTokens: number;\n\n  /**\nThe number of tokens used in the completion.\n */\n  completionTokens: number;\n\n  /**\nThe total number of tokens used (promptTokens + completionTokens).\n   */\n  totalTokens: number;\n};\n\n/**\nRepresents the number of tokens used in a prompt and completion.\n\n@deprecated Use `LanguageModelUsage` instead.\n */\nexport type CompletionTokenUsage = LanguageModelUsage;\n\n/**\nRepresents the number of tokens used in an embedding.\n */\nexport type EmbeddingModelUsage = {\n  /**\nThe number of tokens used in the embedding.\n   */\n  tokens: number;\n};\n\n/**\nRepresents the number of tokens used in an embedding.\n\n@deprecated Use `EmbeddingModelUsage` instead.\n */\nexport type EmbeddingTokenUsage = EmbeddingModelUsage;\n\nexport function calculateLanguageModelUsage(usage: {\n  promptTokens: number;\n  completionTokens: number;\n}): LanguageModelUsage {\n  return {\n    promptTokens: usage.promptTokens,\n    completionTokens: usage.completionTokens,\n    totalTokens: usage.promptTokens + usage.completionTokens,\n  };\n}\n","export function prepareResponseHeaders(\n  init: ResponseInit | undefined,\n  {\n    contentType,\n    dataStreamVersion,\n  }: { contentType: string; dataStreamVersion?: 'v1' | undefined },\n) {\n  const headers = new Headers(init?.headers ?? {});\n\n  if (!headers.has('Content-Type')) {\n    headers.set('Content-Type', contentType);\n  }\n\n  if (dataStreamVersion !== undefined) {\n    headers.set('X-Vercel-AI-Data-Stream', dataStreamVersion);\n  }\n\n  return headers;\n}\n","import { JSONSchema7 } from 'json-schema';\n\nconst DEFAULT_SCHEMA_PREFIX = 'JSON schema:';\nconst DEFAULT_SCHEMA_SUFFIX =\n  'You MUST answer with a JSON object that matches the JSON schema above.';\nconst DEFAULT_GENERIC_SUFFIX = 'You MUST answer with JSON.';\n\nexport function injectJsonInstruction({\n  prompt,\n  schema,\n  schemaPrefix = schema != null ? DEFAULT_SCHEMA_PREFIX : undefined,\n  schemaSuffix = schema != null\n    ? DEFAULT_SCHEMA_SUFFIX\n    : DEFAULT_GENERIC_SUFFIX,\n}: {\n  prompt?: string;\n  schema?: JSONSchema7;\n  schemaPrefix?: string;\n  schemaSuffix?: string;\n}): string {\n  return [\n    prompt != null && prompt.length > 0 ? prompt : undefined,\n    prompt != null && prompt.length > 0 ? '' : undefined, // add a newline if prompt is not null\n    schemaPrefix,\n    schema != null ? JSON.stringify(schema) : undefined,\n    schemaSuffix,\n  ]\n    .filter(line => line != null)\n    .join('\\n');\n}\n","import { AISDKError } from '@ai-sdk/provider';\n\nconst name = 'AI_NoObjectGeneratedError';\nconst marker = `vercel.ai.error.${name}`;\nconst symbol = Symbol.for(marker);\n\n/**\nThrown when the AI provider fails to generate a parsable object.\n */\nexport class NoObjectGeneratedError extends AISDKError {\n  private readonly [symbol] = true; // used in isInstance\n\n  constructor({ message = 'No object generated.' }: { message?: string } = {}) {\n    super({ name, message });\n  }\n\n  static isInstance(error: unknown): error is NoObjectGeneratedError {\n    return AISDKError.hasMarker(error, marker);\n  }\n\n  /**\n   * @deprecated Use isInstance instead.\n   */\n  static isNoObjectGeneratedError(\n    error: unknown,\n  ): error is NoObjectGeneratedError {\n    return error instanceof Error && error.name === name;\n  }\n\n  /**\n   * @deprecated Do not use this method. It will be removed in the next major version.\n   */\n  toJSON() {\n    return {\n      name: this.name,\n      cause: this.cause,\n      message: this.message,\n      stack: this.stack,\n    };\n  }\n}\n","import {\n  isJSONArray,\n  isJSONObject,\n  JSONObject,\n  JSONValue,\n  TypeValidationError,\n  UnsupportedFunctionalityError,\n} from '@ai-sdk/provider';\nimport { safeValidateTypes, ValidationResult } from '@ai-sdk/provider-utils';\nimport { asSchema, DeepPartial, Schema } from '@ai-sdk/ui-utils';\nimport { NoObjectGeneratedError } from './no-object-generated-error';\nimport { JSONSchema7 } from 'json-schema';\nimport { ObjectStreamPart } from './stream-object-result';\nimport {\n  AsyncIterableStream,\n  createAsyncIterableStream,\n} from '../util/async-iterable-stream';\nimport { z } from 'zod';\n\nexport interface OutputStrategy<PARTIAL, RESULT, ELEMENT_STREAM> {\n  readonly type: 'object' | 'array' | 'enum' | 'no-schema';\n  readonly jsonSchema: JSONSchema7 | undefined;\n\n  validatePartialResult({\n    value,\n    textDelta,\n    isFinalDelta,\n  }: {\n    value: JSONValue;\n    textDelta: string;\n    isFirstDelta: boolean;\n    isFinalDelta: boolean;\n    latestObject: PARTIAL | undefined;\n  }): ValidationResult<{\n    partial: PARTIAL;\n    textDelta: string;\n  }>;\n  validateFinalResult(value: JSONValue | undefined): ValidationResult<RESULT>;\n\n  createElementStream(\n    originalStream: ReadableStream<ObjectStreamPart<PARTIAL>>,\n  ): ELEMENT_STREAM;\n}\n\nconst noSchemaOutputStrategy: OutputStrategy<JSONValue, JSONValue, never> = {\n  type: 'no-schema',\n  jsonSchema: undefined,\n\n  validatePartialResult({ value, textDelta }) {\n    return { success: true, value: { partial: value, textDelta } };\n  },\n\n  validateFinalResult(\n    value: JSONValue | undefined,\n  ): ValidationResult<JSONValue> {\n    return value === undefined\n      ? { success: false, error: new NoObjectGeneratedError() }\n      : { success: true, value };\n  },\n\n  createElementStream() {\n    throw new UnsupportedFunctionalityError({\n      functionality: 'element streams in no-schema mode',\n    });\n  },\n};\n\nconst objectOutputStrategy = <OBJECT>(\n  schema: Schema<OBJECT>,\n): OutputStrategy<DeepPartial<OBJECT>, OBJECT, never> => ({\n  type: 'object',\n  jsonSchema: schema.jsonSchema,\n\n  validatePartialResult({ value, textDelta }) {\n    return {\n      success: true,\n      value: {\n        // Note: currently no validation of partial results:\n        partial: value as DeepPartial<OBJECT>,\n        textDelta,\n      },\n    };\n  },\n\n  validateFinalResult(value: JSONValue | undefined): ValidationResult<OBJECT> {\n    return safeValidateTypes({ value, schema });\n  },\n\n  createElementStream() {\n    throw new UnsupportedFunctionalityError({\n      functionality: 'element streams in object mode',\n    });\n  },\n});\n\nconst arrayOutputStrategy = <ELEMENT>(\n  schema: Schema<ELEMENT>,\n): OutputStrategy<ELEMENT[], ELEMENT[], AsyncIterableStream<ELEMENT>> => {\n  // remove $schema from schema.jsonSchema:\n  const { $schema, ...itemSchema } = schema.jsonSchema;\n\n  return {\n    type: 'enum',\n\n    // wrap in object that contains array of elements, since most LLMs will not\n    // be able to generate an array directly:\n    // possible future optimization: use arrays directly when model supports grammar-guided generation\n    jsonSchema: {\n      $schema: 'http://json-schema.org/draft-07/schema#',\n      type: 'object',\n      properties: {\n        elements: { type: 'array', items: itemSchema },\n      },\n      required: ['elements'],\n      additionalProperties: false,\n    },\n\n    validatePartialResult({ value, latestObject, isFirstDelta, isFinalDelta }) {\n      // check that the value is an object that contains an array of elements:\n      if (!isJSONObject(value) || !isJSONArray(value.elements)) {\n        return {\n          success: false,\n          error: new TypeValidationError({\n            value,\n            cause: 'value must be an object that contains an array of elements',\n          }),\n        };\n      }\n\n      const inputArray = value.elements as Array<JSONObject>;\n      const resultArray: Array<ELEMENT> = [];\n\n      for (let i = 0; i < inputArray.length; i++) {\n        const element = inputArray[i];\n        const result = safeValidateTypes({ value: element, schema });\n\n        // special treatment for last processed element:\n        // ignore parse or validation failures, since they indicate that the\n        // last element is incomplete and should not be included in the result,\n        // unless it is the final delta\n        if (i === inputArray.length - 1 && !isFinalDelta) {\n          continue;\n        }\n\n        if (!result.success) {\n          return result;\n        }\n\n        resultArray.push(result.value);\n      }\n\n      // calculate delta:\n      const publishedElementCount = latestObject?.length ?? 0;\n\n      let textDelta = '';\n\n      if (isFirstDelta) {\n        textDelta += '[';\n      }\n\n      if (publishedElementCount > 0) {\n        textDelta += ',';\n      }\n\n      textDelta += resultArray\n        .slice(publishedElementCount) // only new elements\n        .map(element => JSON.stringify(element))\n        .join(',');\n\n      if (isFinalDelta) {\n        textDelta += ']';\n      }\n\n      return {\n        success: true,\n        value: {\n          partial: resultArray,\n          textDelta,\n        },\n      };\n    },\n\n    validateFinalResult(\n      value: JSONValue | undefined,\n    ): ValidationResult<Array<ELEMENT>> {\n      // check that the value is an object that contains an array of elements:\n      if (!isJSONObject(value) || !isJSONArray(value.elements)) {\n        return {\n          success: false,\n          error: new TypeValidationError({\n            value,\n            cause: 'value must be an object that contains an array of elements',\n          }),\n        };\n      }\n\n      const inputArray = value.elements as Array<JSONObject>;\n\n      // check that each element in the array is of the correct type:\n      for (const element of inputArray) {\n        const result = safeValidateTypes({ value: element, schema });\n        if (!result.success) {\n          return result;\n        }\n      }\n\n      return { success: true, value: inputArray as Array<ELEMENT> };\n    },\n\n    createElementStream(\n      originalStream: ReadableStream<ObjectStreamPart<ELEMENT[]>>,\n    ) {\n      let publishedElements = 0;\n\n      return createAsyncIterableStream(originalStream, {\n        transform(chunk, controller) {\n          switch (chunk.type) {\n            case 'object': {\n              const array = chunk.object;\n\n              // publish new elements one by one:\n              for (; publishedElements < array.length; publishedElements++) {\n                controller.enqueue(array[publishedElements]);\n              }\n\n              break;\n            }\n\n            case 'text-delta':\n            case 'finish':\n              break;\n\n            case 'error':\n              controller.error(chunk.error);\n              break;\n\n            default: {\n              const _exhaustiveCheck: never = chunk;\n              throw new Error(`Unsupported chunk type: ${_exhaustiveCheck}`);\n            }\n          }\n        },\n      });\n    },\n  };\n};\n\nconst enumOutputStrategy = <ENUM extends string>(\n  enumValues: Array<ENUM>,\n): OutputStrategy<ENUM, ENUM, never> => {\n  return {\n    type: 'enum',\n\n    // wrap in object that contains result, since most LLMs will not\n    // be able to generate an enum value directly:\n    // possible future optimization: use enums directly when model supports top-level enums\n    jsonSchema: {\n      $schema: 'http://json-schema.org/draft-07/schema#',\n      type: 'object',\n      properties: {\n        result: { type: 'string', enum: enumValues },\n      },\n      required: ['result'],\n      additionalProperties: false,\n    },\n\n    validateFinalResult(value: JSONValue | undefined): ValidationResult<ENUM> {\n      // check that the value is an object that contains an array of elements:\n      if (!isJSONObject(value) || typeof value.result !== 'string') {\n        return {\n          success: false,\n          error: new TypeValidationError({\n            value,\n            cause:\n              'value must be an object that contains a string in the \"result\" property.',\n          }),\n        };\n      }\n\n      const result = value.result as string;\n\n      return enumValues.includes(result as ENUM)\n        ? { success: true, value: result as ENUM }\n        : {\n            success: false,\n            error: new TypeValidationError({\n              value,\n              cause: 'value must be a string in the enum',\n            }),\n          };\n    },\n\n    validatePartialResult() {\n      // no streaming in enum mode\n      throw new UnsupportedFunctionalityError({\n        functionality: 'partial results in enum mode',\n      });\n    },\n\n    createElementStream() {\n      // no streaming in enum mode\n      throw new UnsupportedFunctionalityError({\n        functionality: 'element streams in enum mode',\n      });\n    },\n  };\n};\n\nexport function getOutputStrategy<SCHEMA>({\n  output,\n  schema,\n  enumValues,\n}: {\n  output: 'object' | 'array' | 'enum' | 'no-schema';\n  schema?: z.Schema<SCHEMA, z.ZodTypeDef, any> | Schema<SCHEMA>;\n  enumValues?: Array<SCHEMA>;\n}): OutputStrategy<any, any, any> {\n  switch (output) {\n    case 'object':\n      return objectOutputStrategy(asSchema(schema!));\n    case 'array':\n      return arrayOutputStrategy(asSchema(schema!));\n    case 'enum':\n      return enumOutputStrategy(enumValues! as Array<string>);\n    case 'no-schema':\n      return noSchemaOutputStrategy;\n    default: {\n      const _exhaustiveCheck: never = output;\n      throw new Error(`Unsupported output: ${_exhaustiveCheck}`);\n    }\n  }\n}\n","export type AsyncIterableStream<T> = AsyncIterable<T> & ReadableStream<T>;\n\nexport function createAsyncIterableStream<S, T>(\n  source: ReadableStream<S>,\n  transformer: Transformer<S, T>,\n): AsyncIterableStream<T> {\n  const transformedStream: any = source.pipeThrough(\n    new TransformStream(transformer),\n  );\n\n  transformedStream[Symbol.asyncIterator] = () => {\n    const reader = transformedStream.getReader();\n    return {\n      async next(): Promise<IteratorResult<string>> {\n        const { done, value } = await reader.read();\n        return done ? { done: true, value: undefined } : { done: false, value };\n      },\n    };\n  };\n\n  return transformedStream;\n}\n","import { z } from 'zod';\nimport { InvalidArgumentError } from '../../errors/invalid-argument-error';\nimport { Schema } from '@ai-sdk/ui-utils';\n\nexport function validateObjectGenerationInput({\n  output,\n  mode,\n  schema,\n  schemaName,\n  schemaDescription,\n  enumValues,\n}: {\n  output?: 'object' | 'array' | 'enum' | 'no-schema';\n  schema?: z.Schema<any, z.ZodTypeDef, any> | Schema<any>;\n  schemaName?: string;\n  schemaDescription?: string;\n  enumValues?: Array<unknown>;\n  mode?: 'auto' | 'json' | 'tool';\n}) {\n  if (\n    output != null &&\n    output !== 'object' &&\n    output !== 'array' &&\n    output !== 'enum' &&\n    output !== 'no-schema'\n  ) {\n    throw new InvalidArgumentError({\n      parameter: 'output',\n      value: output,\n      message: 'Invalid output type.',\n    });\n  }\n\n  if (output === 'no-schema') {\n    if (mode === 'auto' || mode === 'tool') {\n      throw new InvalidArgumentError({\n        parameter: 'mode',\n        value: mode,\n        message: 'Mode must be \"json\" for no-schema output.',\n      });\n    }\n\n    if (schema != null) {\n      throw new InvalidArgumentError({\n        parameter: 'schema',\n        value: schema,\n        message: 'Schema is not supported for no-schema output.',\n      });\n    }\n\n    if (schemaDescription != null) {\n      throw new InvalidArgumentError({\n        parameter: 'schemaDescription',\n        value: schemaDescription,\n        message: 'Schema description is not supported for no-schema output.',\n      });\n    }\n\n    if (schemaName != null) {\n      throw new InvalidArgumentError({\n        parameter: 'schemaName',\n        value: schemaName,\n        message: 'Schema name is not supported for no-schema output.',\n      });\n    }\n\n    if (enumValues != null) {\n      throw new InvalidArgumentError({\n        parameter: 'enumValues',\n        value: enumValues,\n        message: 'Enum values are not supported for no-schema output.',\n      });\n    }\n  }\n\n  if (output === 'object') {\n    if (schema == null) {\n      throw new InvalidArgumentError({\n        parameter: 'schema',\n        value: schema,\n        message: 'Schema is required for object output.',\n      });\n    }\n\n    if (enumValues != null) {\n      throw new InvalidArgumentError({\n        parameter: 'enumValues',\n        value: enumValues,\n        message: 'Enum values are not supported for object output.',\n      });\n    }\n  }\n\n  if (output === 'array') {\n    if (schema == null) {\n      throw new InvalidArgumentError({\n        parameter: 'schema',\n        value: schema,\n        message: 'Element schema is required for array output.',\n      });\n    }\n\n    if (enumValues != null) {\n      throw new InvalidArgumentError({\n        parameter: 'enumValues',\n        value: enumValues,\n        message: 'Enum values are not supported for array output.',\n      });\n    }\n  }\n\n  if (output === 'enum') {\n    if (schema != null) {\n      throw new InvalidArgumentError({\n        parameter: 'schema',\n        value: schema,\n        message: 'Schema is not supported for enum output.',\n      });\n    }\n\n    if (schemaDescription != null) {\n      throw new InvalidArgumentError({\n        parameter: 'schemaDescription',\n        value: schemaDescription,\n        message: 'Schema description is not supported for enum output.',\n      });\n    }\n\n    if (schemaName != null) {\n      throw new InvalidArgumentError({\n        parameter: 'schemaName',\n        value: schemaName,\n        message: 'Schema name is not supported for enum output.',\n      });\n    }\n\n    if (enumValues == null) {\n      throw new InvalidArgumentError({\n        parameter: 'enumValues',\n        value: enumValues,\n        message: 'Enum values are required for enum output.',\n      });\n    }\n\n    for (const value of enumValues) {\n      if (typeof value !== 'string') {\n        throw new InvalidArgumentError({\n          parameter: 'enumValues',\n          value,\n          message: 'Enum values must be strings.',\n        });\n      }\n    }\n  }\n}\n","import {\n  JSONValue,\n  LanguageModelV1CallOptions,\n  LanguageModelV1FinishReason,\n  LanguageModelV1StreamPart,\n} from '@ai-sdk/provider';\nimport {\n  DeepPartial,\n  Schema,\n  isDeepEqualData,\n  parsePartialJson,\n} from '@ai-sdk/ui-utils';\nimport { Span } from '@opentelemetry/api';\nimport { ServerResponse } from 'http';\nimport { z } from 'zod';\nimport { createResolvablePromise } from '../../util/create-resolvable-promise';\nimport { DelayedPromise } from '../../util/delayed-promise';\nimport { retryWithExponentialBackoff } from '../../util/retry-with-exponential-backoff';\nimport { CallSettings } from '../prompt/call-settings';\nimport { convertToLanguageModelPrompt } from '../prompt/convert-to-language-model-prompt';\nimport { prepareCallSettings } from '../prompt/prepare-call-settings';\nimport { Prompt } from '../prompt/prompt';\nimport { validatePrompt } from '../prompt/validate-prompt';\nimport { assembleOperationName } from '../telemetry/assemble-operation-name';\nimport { getBaseTelemetryAttributes } from '../telemetry/get-base-telemetry-attributes';\nimport { getTracer } from '../telemetry/get-tracer';\nimport { recordSpan } from '../telemetry/record-span';\nimport { selectTelemetryAttributes } from '../telemetry/select-telemetry-attributes';\nimport { TelemetrySettings } from '../telemetry/telemetry-settings';\nimport {\n  CallWarning,\n  FinishReason,\n  LanguageModel,\n  LanguageModelResponseMetadataWithHeaders,\n  LogProbs,\n  ProviderMetadata,\n} from '../types';\nimport {\n  LanguageModelUsage,\n  calculateLanguageModelUsage,\n} from '../types/usage';\nimport {\n  AsyncIterableStream,\n  createAsyncIterableStream,\n} from '../util/async-iterable-stream';\nimport { now as originalNow } from '../util/now';\nimport { prepareResponseHeaders } from '../util/prepare-response-headers';\nimport { injectJsonInstruction } from './inject-json-instruction';\nimport { OutputStrategy, getOutputStrategy } from './output-strategy';\nimport { ObjectStreamPart, StreamObjectResult } from './stream-object-result';\nimport { validateObjectGenerationInput } from './validate-object-generation-input';\nimport { createIdGenerator } from '@ai-sdk/provider-utils';\nimport { prepareOutgoingHttpHeaders } from '../util/prepare-outgoing-http-headers';\nimport { writeToServerResponse } from '../util/write-to-server-response';\n\nconst originalGenerateId = createIdGenerator({ prefix: 'aiobj-', size: 24 });\n\ntype OnFinishCallback<RESULT> = (event: {\n  /**\nThe token usage of the generated response.\n*/\n  usage: LanguageModelUsage;\n\n  /**\nThe generated object. Can be undefined if the final object does not match the schema.\n*/\n  object: RESULT | undefined;\n\n  /**\nOptional error object. This is e.g. a TypeValidationError when the final object does not match the schema.\n*/\n  error: unknown | undefined;\n\n  /**\nOptional raw response data.\n\n@deprecated Use `response` instead.\n       */\n  rawResponse?: {\n    /**\nResponse headers.\n   */\n    headers?: Record<string, string>;\n  };\n\n  /**\nResponse metadata.\n */\n  response: LanguageModelResponseMetadataWithHeaders;\n\n  /**\nWarnings from the model provider (e.g. unsupported settings).\n*/\n  warnings?: CallWarning[];\n\n  /**\nAdditional provider-specific metadata. They are passed through\nfrom the provider to the AI SDK and enable provider-specific\nresults that can be fully encapsulated in the provider.\n*/\n  experimental_providerMetadata: ProviderMetadata | undefined;\n}) => Promise<void> | void;\n\n/**\nGenerate a structured, typed object for a given prompt and schema using a language model.\n\nThis function streams the output. If you do not want to stream the output, use `generateObject` instead.\n\n@return\nA result object for accessing the partial object stream and additional information.\n */\nexport async function streamObject<OBJECT>(\n  options: Omit<CallSettings, 'stopSequences'> &\n    Prompt & {\n      output?: 'object' | undefined;\n\n      /**\nThe language model to use.\n     */\n      model: LanguageModel;\n\n      /**\nThe schema of the object that the model should generate.\n */\n      schema: z.Schema<OBJECT, z.ZodTypeDef, any> | Schema<OBJECT>;\n\n      /**\nOptional name of the output that should be generated.\nUsed by some providers for additional LLM guidance, e.g.\nvia tool or schema name.\n     */\n      schemaName?: string;\n\n      /**\nOptional description of the output that should be generated.\nUsed by some providers for additional LLM guidance, e.g.\nvia tool or schema description.\n */\n      schemaDescription?: string;\n\n      /**\nThe mode to use for object generation.\n\nThe schema is converted into a JSON schema and used in one of the following ways\n\n- 'auto': The provider will choose the best mode for the model.\n- 'tool': A tool with the JSON schema as parameters is provided and the provider is instructed to use it.\n- 'json': The JSON schema and an instruction are injected into the prompt. If the provider supports JSON mode, it is enabled. If the provider supports JSON grammars, the grammar is used.\n\nPlease note that most providers do not support all modes.\n\nDefault and recommended: 'auto' (best mode for the model).\n     */\n      mode?: 'auto' | 'json' | 'tool';\n\n      /**\nOptional telemetry configuration (experimental).\n     */\n      experimental_telemetry?: TelemetrySettings;\n\n      /**\nAdditional provider-specific metadata. They are passed through\nto the provider from the AI SDK and enable provider-specific\nfunctionality that can be fully encapsulated in the provider.\n */\n      experimental_providerMetadata?: ProviderMetadata;\n\n      /**\nCallback that is called when the LLM response and the final object validation are finished.\n     */\n      onFinish?: OnFinishCallback<OBJECT>;\n\n      /**\n       * Internal. For test use only. May change without notice.\n       */\n      _internal?: {\n        generateId?: () => string;\n        currentDate?: () => Date;\n        now?: () => number;\n      };\n    },\n): Promise<StreamObjectResult<DeepPartial<OBJECT>, OBJECT, never>>;\n/**\nGenerate an array with structured, typed elements for a given prompt and element schema using a language model.\n\nThis function streams the output. If you do not want to stream the output, use `generateObject` instead.\n\n@return\nA result object for accessing the partial object stream and additional information.\n */\nexport async function streamObject<ELEMENT>(\n  options: Omit<CallSettings, 'stopSequences'> &\n    Prompt & {\n      output: 'array';\n\n      /**\nThe language model to use.\n     */\n      model: LanguageModel;\n\n      /**\nThe element schema of the array that the model should generate.\n */\n      schema: z.Schema<ELEMENT, z.ZodTypeDef, any> | Schema<ELEMENT>;\n\n      /**\nOptional name of the array that should be generated.\nUsed by some providers for additional LLM guidance, e.g.\nvia tool or schema name.\n     */\n      schemaName?: string;\n\n      /**\nOptional description of the array that should be generated.\nUsed by some providers for additional LLM guidance, e.g.\nvia tool or schema description.\n */\n      schemaDescription?: string;\n\n      /**\nThe mode to use for object generation.\n\nThe schema is converted into a JSON schema and used in one of the following ways\n\n- 'auto': The provider will choose the best mode for the model.\n- 'tool': A tool with the JSON schema as parameters is provided and the provider is instructed to use it.\n- 'json': The JSON schema and an instruction are injected into the prompt. If the provider supports JSON mode, it is enabled. If the provider supports JSON grammars, the grammar is used.\n\nPlease note that most providers do not support all modes.\n\nDefault and recommended: 'auto' (best mode for the model).\n     */\n      mode?: 'auto' | 'json' | 'tool';\n\n      /**\nOptional telemetry configuration (experimental).\n     */\n      experimental_telemetry?: TelemetrySettings;\n\n      /**\nAdditional provider-specific metadata. They are passed through\nto the provider from the AI SDK and enable provider-specific\nfunctionality that can be fully encapsulated in the provider.\n */\n      experimental_providerMetadata?: ProviderMetadata;\n\n      /**\nCallback that is called when the LLM response and the final object validation are finished.\n     */\n      onFinish?: OnFinishCallback<Array<ELEMENT>>;\n\n      /**\n       * Internal. For test use only. May change without notice.\n       */\n      _internal?: {\n        generateId?: () => string;\n        currentDate?: () => Date;\n        now?: () => number;\n      };\n    },\n): Promise<\n  StreamObjectResult<\n    Array<ELEMENT>,\n    Array<ELEMENT>,\n    AsyncIterableStream<ELEMENT>\n  >\n>;\n/**\nGenerate JSON with any schema for a given prompt using a language model.\n\nThis function streams the output. If you do not want to stream the output, use `generateObject` instead.\n\n@return\nA result object for accessing the partial object stream and additional information.\n */\nexport async function streamObject(\n  options: Omit<CallSettings, 'stopSequences'> &\n    Prompt & {\n      output: 'no-schema';\n\n      /**\nThe language model to use.\n     */\n      model: LanguageModel;\n\n      /**\nThe mode to use for object generation. Must be \"json\" for no-schema output.\n     */\n      mode?: 'json';\n\n      /**\nOptional telemetry configuration (experimental).\n     */\n      experimental_telemetry?: TelemetrySettings;\n\n      /**\nAdditional provider-specific metadata. They are passed through\nto the provider from the AI SDK and enable provider-specific\nfunctionality that can be fully encapsulated in the provider.\n */\n      experimental_providerMetadata?: ProviderMetadata;\n\n      /**\nCallback that is called when the LLM response and the final object validation are finished.\n     */\n      onFinish?: OnFinishCallback<JSONValue>;\n\n      /**\n       * Internal. For test use only. May change without notice.\n       */\n      _internal?: {\n        generateId?: () => string;\n        currentDate?: () => Date;\n        now?: () => number;\n      };\n    },\n): Promise<StreamObjectResult<JSONValue, JSONValue, never>>;\nexport async function streamObject<SCHEMA, PARTIAL, RESULT, ELEMENT_STREAM>({\n  model,\n  schema: inputSchema,\n  schemaName,\n  schemaDescription,\n  mode,\n  output = 'object',\n  system,\n  prompt,\n  messages,\n  maxRetries,\n  abortSignal,\n  headers,\n  experimental_telemetry: telemetry,\n  experimental_providerMetadata: providerMetadata,\n  onFinish,\n  _internal: {\n    generateId = originalGenerateId,\n    currentDate = () => new Date(),\n    now = originalNow,\n  } = {},\n  ...settings\n}: Omit<CallSettings, 'stopSequences'> &\n  Prompt & {\n    /**\n     * The expected structure of the output.\n     *\n     * - 'object': Generate a single object that conforms to the schema.\n     * - 'array': Generate an array of objects that conform to the schema.\n     * - 'no-schema': Generate any JSON object. No schema is specified.\n     *\n     * Default is 'object' if not specified.\n     */\n    output?: 'object' | 'array' | 'no-schema';\n\n    model: LanguageModel;\n    schema?: z.Schema<SCHEMA, z.ZodTypeDef, any> | Schema<SCHEMA>;\n    schemaName?: string;\n    schemaDescription?: string;\n    mode?: 'auto' | 'json' | 'tool';\n    experimental_telemetry?: TelemetrySettings;\n    experimental_providerMetadata?: ProviderMetadata;\n    onFinish?: OnFinishCallback<RESULT>;\n    _internal?: {\n      generateId?: () => string;\n      currentDate?: () => Date;\n      now?: () => number;\n    };\n  }): Promise<StreamObjectResult<PARTIAL, RESULT, ELEMENT_STREAM>> {\n  validateObjectGenerationInput({\n    output,\n    mode,\n    schema: inputSchema,\n    schemaName,\n    schemaDescription,\n  });\n\n  const outputStrategy = getOutputStrategy({ output, schema: inputSchema });\n\n  // automatically set mode to 'json' for no-schema output\n  if (outputStrategy.type === 'no-schema' && mode === undefined) {\n    mode = 'json';\n  }\n\n  const baseTelemetryAttributes = getBaseTelemetryAttributes({\n    model,\n    telemetry,\n    headers,\n    settings: { ...settings, maxRetries },\n  });\n\n  const tracer = getTracer({ isEnabled: telemetry?.isEnabled ?? false });\n\n  const retry = retryWithExponentialBackoff({ maxRetries });\n\n  return recordSpan({\n    name: 'ai.streamObject',\n    attributes: selectTelemetryAttributes({\n      telemetry,\n      attributes: {\n        ...assembleOperationName({\n          operationId: 'ai.streamObject',\n          telemetry,\n        }),\n        ...baseTelemetryAttributes,\n        // specific settings that only make sense on the outer level:\n        'ai.prompt': {\n          input: () => JSON.stringify({ system, prompt, messages }),\n        },\n        'ai.schema':\n          outputStrategy.jsonSchema != null\n            ? { input: () => JSON.stringify(outputStrategy.jsonSchema) }\n            : undefined,\n        'ai.schema.name': schemaName,\n        'ai.schema.description': schemaDescription,\n        'ai.settings.output': outputStrategy.type,\n        'ai.settings.mode': mode,\n      },\n    }),\n    tracer,\n    endWhenDone: false,\n    fn: async rootSpan => {\n      // use the default provider mode when the mode is set to 'auto' or unspecified\n      if (mode === 'auto' || mode == null) {\n        mode = model.defaultObjectGenerationMode;\n      }\n\n      let callOptions: LanguageModelV1CallOptions;\n      let transformer: Transformer<\n        LanguageModelV1StreamPart,\n        string | Omit<LanguageModelV1StreamPart, 'text-delta'>\n      >;\n\n      switch (mode) {\n        case 'json': {\n          const validatedPrompt = validatePrompt({\n            system:\n              outputStrategy.jsonSchema == null\n                ? injectJsonInstruction({ prompt: system })\n                : model.supportsStructuredOutputs\n                ? system\n                : injectJsonInstruction({\n                    prompt: system,\n                    schema: outputStrategy.jsonSchema,\n                  }),\n            prompt,\n            messages,\n          });\n\n          callOptions = {\n            mode: {\n              type: 'object-json',\n              schema: outputStrategy.jsonSchema,\n              name: schemaName,\n              description: schemaDescription,\n            },\n            ...prepareCallSettings(settings),\n            inputFormat: validatedPrompt.type,\n            prompt: await convertToLanguageModelPrompt({\n              prompt: validatedPrompt,\n              modelSupportsImageUrls: model.supportsImageUrls,\n            }),\n            providerMetadata,\n            abortSignal,\n            headers,\n          };\n\n          transformer = {\n            transform: (chunk, controller) => {\n              switch (chunk.type) {\n                case 'text-delta':\n                  controller.enqueue(chunk.textDelta);\n                  break;\n                case 'response-metadata':\n                case 'finish':\n                case 'error':\n                  controller.enqueue(chunk);\n                  break;\n              }\n            },\n          };\n\n          break;\n        }\n\n        case 'tool': {\n          const validatedPrompt = validatePrompt({\n            system,\n            prompt,\n            messages,\n          });\n\n          callOptions = {\n            mode: {\n              type: 'object-tool',\n              tool: {\n                type: 'function',\n                name: schemaName ?? 'json',\n                description: schemaDescription ?? 'Respond with a JSON object.',\n                parameters: outputStrategy.jsonSchema!,\n              },\n            },\n            ...prepareCallSettings(settings),\n            inputFormat: validatedPrompt.type,\n            prompt: await convertToLanguageModelPrompt({\n              prompt: validatedPrompt,\n              modelSupportsImageUrls: model.supportsImageUrls,\n            }),\n            providerMetadata,\n            abortSignal,\n            headers,\n          };\n\n          transformer = {\n            transform(chunk, controller) {\n              switch (chunk.type) {\n                case 'tool-call-delta':\n                  controller.enqueue(chunk.argsTextDelta);\n                  break;\n                case 'response-metadata':\n                case 'finish':\n                case 'error':\n                  controller.enqueue(chunk);\n                  break;\n              }\n            },\n          };\n\n          break;\n        }\n\n        case undefined: {\n          throw new Error(\n            'Model does not have a default object generation mode.',\n          );\n        }\n\n        default: {\n          const _exhaustiveCheck: never = mode;\n          throw new Error(`Unsupported mode: ${_exhaustiveCheck}`);\n        }\n      }\n\n      const {\n        result: { stream, warnings, rawResponse },\n        doStreamSpan,\n        startTimestampMs,\n      } = await retry(() =>\n        recordSpan({\n          name: 'ai.streamObject.doStream',\n          attributes: selectTelemetryAttributes({\n            telemetry,\n            attributes: {\n              ...assembleOperationName({\n                operationId: 'ai.streamObject.doStream',\n                telemetry,\n              }),\n              ...baseTelemetryAttributes,\n              'ai.prompt.format': {\n                input: () => callOptions.inputFormat,\n              },\n              'ai.prompt.messages': {\n                input: () => JSON.stringify(callOptions.prompt),\n              },\n              'ai.settings.mode': mode,\n\n              // standardized gen-ai llm span attributes:\n              'gen_ai.system': model.provider,\n              'gen_ai.request.model': model.modelId,\n              'gen_ai.request.frequency_penalty': settings.frequencyPenalty,\n              'gen_ai.request.max_tokens': settings.maxTokens,\n              'gen_ai.request.presence_penalty': settings.presencePenalty,\n              'gen_ai.request.temperature': settings.temperature,\n              'gen_ai.request.top_k': settings.topK,\n              'gen_ai.request.top_p': settings.topP,\n            },\n          }),\n          tracer,\n          endWhenDone: false,\n          fn: async doStreamSpan => ({\n            startTimestampMs: now(),\n            doStreamSpan,\n            result: await model.doStream(callOptions),\n          }),\n        }),\n      );\n\n      return new DefaultStreamObjectResult<PARTIAL, RESULT, ELEMENT_STREAM>({\n        outputStrategy,\n        stream: stream.pipeThrough(new TransformStream(transformer)),\n        warnings,\n        rawResponse,\n        onFinish,\n        rootSpan,\n        doStreamSpan,\n        telemetry,\n        startTimestampMs,\n        modelId: model.modelId,\n        now,\n        currentDate,\n        generateId,\n      });\n    },\n  });\n}\n\nclass DefaultStreamObjectResult<PARTIAL, RESULT, ELEMENT_STREAM>\n  implements StreamObjectResult<PARTIAL, RESULT, ELEMENT_STREAM>\n{\n  private readonly originalStream: ReadableStream<ObjectStreamPart<PARTIAL>>;\n  private readonly objectPromise: DelayedPromise<RESULT>;\n\n  readonly warnings: StreamObjectResult<\n    PARTIAL,\n    RESULT,\n    ELEMENT_STREAM\n  >['warnings'];\n  readonly usage: StreamObjectResult<PARTIAL, RESULT, ELEMENT_STREAM>['usage'];\n  readonly experimental_providerMetadata: StreamObjectResult<\n    PARTIAL,\n    RESULT,\n    ELEMENT_STREAM\n  >['experimental_providerMetadata'];\n  readonly rawResponse: StreamObjectResult<\n    PARTIAL,\n    RESULT,\n    ELEMENT_STREAM\n  >['rawResponse'];\n  readonly outputStrategy: OutputStrategy<PARTIAL, RESULT, ELEMENT_STREAM>;\n  readonly response: StreamObjectResult<\n    PARTIAL,\n    RESULT,\n    ELEMENT_STREAM\n  >['response'];\n\n  constructor({\n    stream,\n    warnings,\n    rawResponse,\n    outputStrategy,\n    onFinish,\n    rootSpan,\n    doStreamSpan,\n    telemetry,\n    startTimestampMs,\n    modelId,\n    now,\n    currentDate,\n    generateId,\n  }: {\n    stream: ReadableStream<\n      string | Omit<LanguageModelV1StreamPart, 'text-delta'>\n    >;\n    warnings: StreamObjectResult<PARTIAL, RESULT, ELEMENT_STREAM>['warnings'];\n    rawResponse: StreamObjectResult<\n      PARTIAL,\n      RESULT,\n      ELEMENT_STREAM\n    >['rawResponse'];\n    outputStrategy: OutputStrategy<PARTIAL, RESULT, ELEMENT_STREAM>;\n    onFinish: OnFinishCallback<RESULT> | undefined;\n    rootSpan: Span;\n    doStreamSpan: Span;\n    telemetry: TelemetrySettings | undefined;\n    startTimestampMs: number;\n    modelId: string;\n    now: () => number;\n    currentDate: () => Date;\n    generateId: () => string;\n  }) {\n    this.warnings = warnings;\n    this.rawResponse = rawResponse;\n    this.outputStrategy = outputStrategy;\n\n    // initialize object promise\n    this.objectPromise = new DelayedPromise<RESULT>();\n\n    // initialize usage promise\n    const { resolve: resolveUsage, promise: usagePromise } =\n      createResolvablePromise<LanguageModelUsage>();\n    this.usage = usagePromise;\n\n    // initialize response promise\n    const { resolve: resolveResponse, promise: responsePromise } =\n      createResolvablePromise<LanguageModelResponseMetadataWithHeaders>();\n    this.response = responsePromise;\n\n    // initialize experimental_providerMetadata promise\n    const {\n      resolve: resolveProviderMetadata,\n      promise: providerMetadataPromise,\n    } = createResolvablePromise<ProviderMetadata | undefined>();\n    this.experimental_providerMetadata = providerMetadataPromise;\n\n    // store information for onFinish callback:\n    let usage: LanguageModelUsage | undefined;\n    let finishReason: LanguageModelV1FinishReason | undefined;\n    let providerMetadata: ProviderMetadata | undefined;\n    let object: RESULT | undefined;\n    let error: unknown | undefined;\n\n    // pipe chunks through a transformation stream that extracts metadata:\n    let accumulatedText = '';\n    let textDelta = '';\n    let response: {\n      id: string;\n      timestamp: Date;\n      modelId: string;\n    } = {\n      id: generateId(),\n      timestamp: currentDate(),\n      modelId,\n    };\n\n    // Keep track of raw parse result before type validation, since e.g. Zod might\n    // change the object by mapping properties.\n    let latestObjectJson: JSONValue | undefined = undefined;\n    let latestObject: PARTIAL | undefined = undefined;\n    let isFirstChunk = true;\n    let isFirstDelta = true;\n\n    const self = this;\n    this.originalStream = stream.pipeThrough(\n      new TransformStream<\n        string | ObjectStreamInputPart,\n        ObjectStreamPart<PARTIAL>\n      >({\n        async transform(chunk, controller): Promise<void> {\n          // Telemetry event for first chunk:\n          if (isFirstChunk) {\n            const msToFirstChunk = now() - startTimestampMs;\n\n            isFirstChunk = false;\n\n            doStreamSpan.addEvent('ai.stream.firstChunk', {\n              'ai.stream.msToFirstChunk': msToFirstChunk,\n            });\n\n            doStreamSpan.setAttributes({\n              'ai.stream.msToFirstChunk': msToFirstChunk,\n            });\n          }\n\n          // process partial text chunks\n          if (typeof chunk === 'string') {\n            accumulatedText += chunk;\n            textDelta += chunk;\n\n            const { value: currentObjectJson, state: parseState } =\n              parsePartialJson(accumulatedText);\n\n            if (\n              currentObjectJson !== undefined &&\n              !isDeepEqualData(latestObjectJson, currentObjectJson)\n            ) {\n              const validationResult = outputStrategy.validatePartialResult({\n                value: currentObjectJson,\n                textDelta,\n                latestObject,\n                isFirstDelta,\n                isFinalDelta: parseState === 'successful-parse',\n              });\n\n              if (\n                validationResult.success &&\n                !isDeepEqualData(latestObject, validationResult.value.partial)\n              ) {\n                // inside inner check to correctly parse the final element in array mode:\n                latestObjectJson = currentObjectJson;\n                latestObject = validationResult.value.partial;\n\n                controller.enqueue({\n                  type: 'object',\n                  object: latestObject,\n                });\n\n                controller.enqueue({\n                  type: 'text-delta',\n                  textDelta: validationResult.value.textDelta,\n                });\n\n                textDelta = '';\n                isFirstDelta = false;\n              }\n            }\n\n            return;\n          }\n\n          switch (chunk.type) {\n            case 'response-metadata': {\n              response = {\n                id: chunk.id ?? response.id,\n                timestamp: chunk.timestamp ?? response.timestamp,\n                modelId: chunk.modelId ?? response.modelId,\n              };\n              break;\n            }\n\n            case 'finish': {\n              // send final text delta:\n              if (textDelta !== '') {\n                controller.enqueue({ type: 'text-delta', textDelta });\n              }\n\n              // store finish reason for telemetry:\n              finishReason = chunk.finishReason;\n\n              // store usage and metadata for promises and onFinish callback:\n              usage = calculateLanguageModelUsage(chunk.usage);\n              providerMetadata = chunk.providerMetadata;\n\n              controller.enqueue({ ...chunk, usage, response });\n\n              // resolve promises that can be resolved now:\n              resolveUsage(usage);\n              resolveProviderMetadata(providerMetadata);\n              resolveResponse({\n                ...response,\n                headers: rawResponse?.headers,\n              });\n\n              // resolve the object promise with the latest object:\n              const validationResult =\n                outputStrategy.validateFinalResult(latestObjectJson);\n\n              if (validationResult.success) {\n                object = validationResult.value;\n                self.objectPromise.resolve(object);\n              } else {\n                error = validationResult.error;\n                self.objectPromise.reject(error);\n              }\n\n              break;\n            }\n\n            default: {\n              controller.enqueue(chunk);\n              break;\n            }\n          }\n        },\n\n        // invoke onFinish callback and resolve toolResults promise when the stream is about to close:\n        async flush(controller) {\n          try {\n            const finalUsage = usage ?? {\n              promptTokens: NaN,\n              completionTokens: NaN,\n              totalTokens: NaN,\n            };\n\n            doStreamSpan.setAttributes(\n              selectTelemetryAttributes({\n                telemetry,\n                attributes: {\n                  'ai.response.finishReason': finishReason,\n                  'ai.response.object': {\n                    output: () => JSON.stringify(object),\n                  },\n                  'ai.response.id': response.id,\n                  'ai.response.model': response.modelId,\n                  'ai.response.timestamp': response.timestamp.toISOString(),\n\n                  'ai.usage.promptTokens': finalUsage.promptTokens,\n                  'ai.usage.completionTokens': finalUsage.completionTokens,\n\n                  // deprecated\n                  'ai.finishReason': finishReason,\n                  'ai.result.object': { output: () => JSON.stringify(object) },\n\n                  // standardized gen-ai llm span attributes:\n                  'gen_ai.response.finish_reasons': [finishReason],\n                  'gen_ai.response.id': response.id,\n                  'gen_ai.response.model': response.modelId,\n                  'gen_ai.usage.input_tokens': finalUsage.promptTokens,\n                  'gen_ai.usage.output_tokens': finalUsage.completionTokens,\n                },\n              }),\n            );\n\n            // finish doStreamSpan before other operations for correct timing:\n            doStreamSpan.end();\n\n            // Add response information to the root span:\n            rootSpan.setAttributes(\n              selectTelemetryAttributes({\n                telemetry,\n                attributes: {\n                  'ai.usage.promptTokens': finalUsage.promptTokens,\n                  'ai.usage.completionTokens': finalUsage.completionTokens,\n                  'ai.response.object': {\n                    output: () => JSON.stringify(object),\n                  },\n\n                  // deprecated\n                  'ai.result.object': { output: () => JSON.stringify(object) },\n                },\n              }),\n            );\n\n            // call onFinish callback:\n            await onFinish?.({\n              usage: finalUsage,\n              object,\n              error,\n              rawResponse,\n              response: {\n                ...response,\n                headers: rawResponse?.headers,\n              },\n              warnings,\n              experimental_providerMetadata: providerMetadata,\n            });\n          } catch (error) {\n            controller.error(error);\n          } finally {\n            rootSpan.end();\n          }\n        },\n      }),\n    );\n  }\n\n  get object(): Promise<RESULT> {\n    return this.objectPromise.value;\n  }\n\n  get partialObjectStream(): AsyncIterableStream<PARTIAL> {\n    return createAsyncIterableStream(this.originalStream, {\n      transform(chunk, controller) {\n        switch (chunk.type) {\n          case 'object':\n            controller.enqueue(chunk.object);\n            break;\n\n          case 'text-delta':\n          case 'finish':\n            break;\n\n          case 'error':\n            controller.error(chunk.error);\n            break;\n\n          default: {\n            const _exhaustiveCheck: never = chunk;\n            throw new Error(`Unsupported chunk type: ${_exhaustiveCheck}`);\n          }\n        }\n      },\n    });\n  }\n\n  get elementStream(): ELEMENT_STREAM {\n    return this.outputStrategy.createElementStream(this.originalStream);\n  }\n\n  get textStream(): AsyncIterableStream<string> {\n    return createAsyncIterableStream(this.originalStream, {\n      transform(chunk, controller) {\n        switch (chunk.type) {\n          case 'text-delta':\n            controller.enqueue(chunk.textDelta);\n            break;\n\n          case 'object':\n          case 'finish':\n            break;\n\n          case 'error':\n            controller.error(chunk.error);\n            break;\n\n          default: {\n            const _exhaustiveCheck: never = chunk;\n            throw new Error(`Unsupported chunk type: ${_exhaustiveCheck}`);\n          }\n        }\n      },\n    });\n  }\n\n  get fullStream(): AsyncIterableStream<ObjectStreamPart<PARTIAL>> {\n    return createAsyncIterableStream(this.originalStream, {\n      transform(chunk, controller) {\n        controller.enqueue(chunk);\n      },\n    });\n  }\n\n  pipeTextStreamToResponse(response: ServerResponse, init?: ResponseInit) {\n    writeToServerResponse({\n      response,\n      status: init?.status,\n      statusText: init?.statusText,\n      headers: prepareOutgoingHttpHeaders(init, {\n        contentType: 'text/plain; charset=utf-8',\n      }),\n      stream: this.textStream.pipeThrough(new TextEncoderStream()),\n    });\n  }\n\n  toTextStreamResponse(init?: ResponseInit): Response {\n    return new Response(this.textStream.pipeThrough(new TextEncoderStream()), {\n      status: init?.status ?? 200,\n      headers: prepareResponseHeaders(init, {\n        contentType: 'text/plain; charset=utf-8',\n      }),\n    });\n  }\n}\n\n/**\n * @deprecated Use `streamObject` instead.\n */\nexport const experimental_streamObject = streamObject;\n\nexport type ObjectStreamInputPart =\n  | {\n      type: 'error';\n      error: unknown;\n    }\n  | {\n      type: 'response-metadata';\n      id?: string;\n      timestamp?: Date;\n      modelId?: string;\n    }\n  | {\n      type: 'finish';\n      finishReason: FinishReason;\n      logprobs?: LogProbs;\n      usage: LanguageModelUsage;\n      providerMetadata?: ProviderMetadata;\n    };\n","/**\n * Creates a Promise with externally accessible resolve and reject functions.\n *\n * @template T - The type of the value that the Promise will resolve to.\n * @returns An object containing:\n *   - promise: A Promise that can be resolved or rejected externally.\n *   - resolve: A function to resolve the Promise with a value of type T.\n *   - reject: A function to reject the Promise with an error.\n */\nexport function createResolvablePromise<T = any>(): {\n  promise: Promise<T>;\n  resolve: (value: T) => void;\n  reject: (error: unknown) => void;\n} {\n  let resolve: (value: T) => void;\n  let reject: (error: unknown) => void;\n\n  const promise = new Promise<T>((res, rej) => {\n    resolve = res;\n    reject = rej;\n  });\n\n  return {\n    promise,\n    resolve: resolve!,\n    reject: reject!,\n  };\n}\n","/**\n * Delayed promise. It is only constructed once the value is accessed.\n * This is useful to avoid unhandled promise rejections when the promise is created\n * but not accessed.\n */\nexport class DelayedPromise<T> {\n  private status:\n    | { type: 'pending' }\n    | { type: 'resolved'; value: T }\n    | { type: 'rejected'; error: unknown } = { type: 'pending' };\n  private promise: Promise<T> | undefined;\n  private _resolve: undefined | ((value: T) => void) = undefined;\n  private _reject: undefined | ((error: unknown) => void) = undefined;\n\n  get value(): Promise<T> {\n    if (this.promise) {\n      return this.promise;\n    }\n\n    this.promise = new Promise<T>((resolve, reject) => {\n      if (this.status.type === 'resolved') {\n        resolve(this.status.value);\n      } else if (this.status.type === 'rejected') {\n        reject(this.status.error);\n      }\n\n      this._resolve = resolve;\n      this._reject = reject;\n    });\n\n    return this.promise;\n  }\n\n  resolve(value: T): void {\n    this.status = { type: 'resolved', value };\n\n    if (this.promise) {\n      this._resolve?.(value);\n    }\n  }\n\n  reject(error: unknown): void {\n    this.status = { type: 'rejected', error };\n\n    if (this.promise) {\n      this._reject?.(error);\n    }\n  }\n}\n","// Shim for performance.now() to support environments that don't have it:\nexport function now(): number {\n  return globalThis?.performance?.now() ?? Date.now();\n}\n","export function prepareOutgoingHttpHeaders(\n  init: ResponseInit | undefined,\n  {\n    contentType,\n    dataStreamVersion,\n  }: { contentType: string; dataStreamVersion?: 'v1' | undefined },\n) {\n  const headers: Record<string, string | number | string[]> = {};\n\n  if (init?.headers != null) {\n    for (const [key, value] of Object.entries(init.headers)) {\n      headers[key] = value;\n    }\n  }\n\n  if (headers['Content-Type'] == null) {\n    headers['Content-Type'] = contentType;\n  }\n\n  if (dataStreamVersion !== undefined) {\n    headers['X-Vercel-AI-Data-Stream'] = dataStreamVersion;\n  }\n\n  return headers;\n}\n","import { ServerResponse } from 'node:http';\n\n/**\n * Writes the content of a stream to a server response.\n */\nexport function writeToServerResponse({\n  response,\n  status,\n  statusText,\n  headers,\n  stream,\n}: {\n  response: ServerResponse;\n  status?: number;\n  statusText?: string;\n  headers?: Record<string, string | number | string[]>;\n  stream: ReadableStream<Uint8Array>;\n}): void {\n  response.writeHead(status ?? 200, statusText, headers);\n\n  const reader = stream.getReader();\n  const read = async () => {\n    try {\n      while (true) {\n        const { done, value } = await reader.read();\n        if (done) break;\n        response.write(value);\n      }\n    } catch (error) {\n      throw error;\n    } finally {\n      response.end();\n    }\n  };\n\n  read();\n}\n","import { createIdGenerator } from '@ai-sdk/provider-utils';\nimport { Tracer } from '@opentelemetry/api';\nimport { InvalidArgumentError } from '../../errors';\nimport { retryWithExponentialBackoff } from '../../util/retry-with-exponential-backoff';\nimport { CoreAssistantMessage, CoreToolMessage } from '../prompt';\nimport { CallSettings } from '../prompt/call-settings';\nimport {\n  convertToLanguageModelMessage,\n  convertToLanguageModelPrompt,\n} from '../prompt/convert-to-language-model-prompt';\nimport { prepareCallSettings } from '../prompt/prepare-call-settings';\nimport { prepareToolsAndToolChoice } from '../prompt/prepare-tools-and-tool-choice';\nimport { Prompt } from '../prompt/prompt';\nimport { validatePrompt } from '../prompt/validate-prompt';\nimport { assembleOperationName } from '../telemetry/assemble-operation-name';\nimport { getBaseTelemetryAttributes } from '../telemetry/get-base-telemetry-attributes';\nimport { getTracer } from '../telemetry/get-tracer';\nimport { recordSpan } from '../telemetry/record-span';\nimport { selectTelemetryAttributes } from '../telemetry/select-telemetry-attributes';\nimport { TelemetrySettings } from '../telemetry/telemetry-settings';\nimport { CoreTool } from '../tool/tool';\nimport { CoreToolChoice, LanguageModel, ProviderMetadata } from '../types';\nimport {\n  LanguageModelUsage,\n  calculateLanguageModelUsage,\n} from '../types/usage';\nimport { removeTextAfterLastWhitespace } from '../util/remove-text-after-last-whitespace';\nimport { GenerateTextResult } from './generate-text-result';\nimport { parseToolCall } from './parse-tool-call';\nimport { StepResult } from './step-result';\nimport { toResponseMessages } from './to-response-messages';\nimport { ToToolCallArray } from './tool-call';\nimport { ToToolResultArray } from './tool-result';\n\nconst originalGenerateId = createIdGenerator({ prefix: 'aitxt-', size: 24 });\n\n/**\nGenerate a text and call tools for a given prompt using a language model.\n\nThis function does not stream the output. If you want to stream the output, use `streamText` instead.\n\n@param model - The language model to use.\n\n@param tools - Tools that are accessible to and can be called by the model. The model needs to support calling tools.\n@param toolChoice - The tool choice strategy. Default: 'auto'.\n\n@param system - A system message that will be part of the prompt.\n@param prompt - A simple text prompt. You can either use `prompt` or `messages` but not both.\n@param messages - A list of messages. You can either use `prompt` or `messages` but not both.\n\n@param maxTokens - Maximum number of tokens to generate.\n@param temperature - Temperature setting.\nThe value is passed through to the provider. The range depends on the provider and model.\nIt is recommended to set either `temperature` or `topP`, but not both.\n@param topP - Nucleus sampling.\nThe value is passed through to the provider. The range depends on the provider and model.\nIt is recommended to set either `temperature` or `topP`, but not both.\n@param topK - Only sample from the top K options for each subsequent token.\nUsed to remove \"long tail\" low probability responses.\nRecommended for advanced use cases only. You usually only need to use temperature.\n@param presencePenalty - Presence penalty setting.\nIt affects the likelihood of the model to repeat information that is already in the prompt.\nThe value is passed through to the provider. The range depends on the provider and model.\n@param frequencyPenalty - Frequency penalty setting.\nIt affects the likelihood of the model to repeatedly use the same words or phrases.\nThe value is passed through to the provider. The range depends on the provider and model.\n@param stopSequences - Stop sequences.\nIf set, the model will stop generating text when one of the stop sequences is generated.\n@param seed - The seed (integer) to use for random sampling.\nIf set and supported by the model, calls will generate deterministic results.\n\n@param maxRetries - Maximum number of retries. Set to 0 to disable retries. Default: 2.\n@param abortSignal - An optional abort signal that can be used to cancel the call.\n@param headers - Additional HTTP headers to be sent with the request. Only applicable for HTTP-based providers.\n\n@param maxSteps - Maximum number of sequential LLM calls (steps), e.g. when you use tool calls.\n\n@param onStepFinish - Callback that is called when each step (LLM call) is finished, including intermediate steps.\n\n@returns\nA result object that contains the generated text, the results of the tool calls, and additional information.\n */\nexport async function generateText<TOOLS extends Record<string, CoreTool>>({\n  model,\n  tools,\n  toolChoice,\n  system,\n  prompt,\n  messages,\n  maxRetries,\n  abortSignal,\n  headers,\n  maxAutomaticRoundtrips = 0,\n  maxToolRoundtrips = maxAutomaticRoundtrips,\n  maxSteps = maxToolRoundtrips != null ? maxToolRoundtrips + 1 : 1,\n  experimental_continuationSteps,\n  experimental_continueSteps: continueSteps = experimental_continuationSteps ??\n    false,\n  experimental_telemetry: telemetry,\n  experimental_providerMetadata: providerMetadata,\n  _internal: {\n    generateId = originalGenerateId,\n    currentDate = () => new Date(),\n  } = {},\n  onStepFinish,\n  ...settings\n}: CallSettings &\n  Prompt & {\n    /**\nThe language model to use.\n     */\n    model: LanguageModel;\n\n    /**\nThe tools that the model can call. The model needs to support calling tools.\n*/\n    tools?: TOOLS;\n\n    /**\nThe tool choice strategy. Default: 'auto'.\n     */\n    toolChoice?: CoreToolChoice<TOOLS>;\n\n    /**\n@deprecated Use `maxToolRoundtrips` instead.\n     */\n    maxAutomaticRoundtrips?: number;\n\n    /**\nMaximum number of automatic roundtrips for tool calls.\n\nAn automatic tool call roundtrip is another LLM call with the\ntool call results when all tool calls of the last assistant\nmessage have results.\n\nA maximum number is required to prevent infinite loops in the\ncase of misconfigured tools.\n\nBy default, it's set to 0, which will disable the feature.\n\n@deprecated Use `maxSteps` instead (which is `maxToolRoundtrips` + 1).\n     */\n    maxToolRoundtrips?: number;\n\n    /**\nMaximum number of sequential LLM calls (steps), e.g. when you use tool calls. Must be at least 1.\n\nA maximum number is required to prevent infinite loops in the case of misconfigured tools.\n\nBy default, it's set to 1, which means that only a single LLM call is made.\n     */\n    maxSteps?: number;\n\n    /**\n@deprecated Use `experimental_continueSteps` instead.\n     */\n    experimental_continuationSteps?: boolean;\n\n    /**\nWhen enabled, the model will perform additional steps if the finish reason is \"length\" (experimental).\n\nBy default, it's set to false.\n     */\n    experimental_continueSteps?: boolean;\n\n    /**\nOptional telemetry configuration (experimental).\n     */\n    experimental_telemetry?: TelemetrySettings;\n\n    /**\nAdditional provider-specific metadata. They are passed through\nto the provider from the AI SDK and enable provider-specific\nfunctionality that can be fully encapsulated in the provider.\n */\n    experimental_providerMetadata?: ProviderMetadata;\n\n    /**\n    Callback that is called when each step (LLM call) is finished, including intermediate steps.\n    */\n    onStepFinish?: (event: StepResult<TOOLS>) => Promise<void> | void;\n\n    /**\n     * Internal. For test use only. May change without notice.\n     */\n    _internal?: {\n      generateId?: () => string;\n      currentDate?: () => Date;\n    };\n  }): Promise<GenerateTextResult<TOOLS>> {\n  if (maxSteps < 1) {\n    throw new InvalidArgumentError({\n      parameter: 'maxSteps',\n      value: maxSteps,\n      message: 'maxSteps must be at least 1',\n    });\n  }\n\n  const baseTelemetryAttributes = getBaseTelemetryAttributes({\n    model,\n    telemetry,\n    headers,\n    settings: { ...settings, maxRetries },\n  });\n\n  const tracer = getTracer({ isEnabled: telemetry?.isEnabled ?? false });\n  return recordSpan({\n    name: 'ai.generateText',\n    attributes: selectTelemetryAttributes({\n      telemetry,\n      attributes: {\n        ...assembleOperationName({\n          operationId: 'ai.generateText',\n          telemetry,\n        }),\n        ...baseTelemetryAttributes,\n        // specific settings that only make sense on the outer level:\n        'ai.prompt': {\n          input: () => JSON.stringify({ system, prompt, messages }),\n        },\n        'ai.settings.maxSteps': maxSteps,\n      },\n    }),\n    tracer,\n    fn: async span => {\n      const retry = retryWithExponentialBackoff({ maxRetries });\n      const validatedPrompt = validatePrompt({\n        system,\n        prompt,\n        messages,\n      });\n\n      const mode = {\n        type: 'regular' as const,\n        ...prepareToolsAndToolChoice({ tools, toolChoice }),\n      };\n      const callSettings = prepareCallSettings(settings);\n      const promptMessages = await convertToLanguageModelPrompt({\n        prompt: validatedPrompt,\n        modelSupportsImageUrls: model.supportsImageUrls,\n      });\n\n      let currentModelResponse: Awaited<\n        ReturnType<LanguageModel['doGenerate']>\n      > & { response: { id: string; timestamp: Date; modelId: string } };\n      let currentToolCalls: ToToolCallArray<TOOLS> = [];\n      let currentToolResults: ToToolResultArray<TOOLS> = [];\n      let stepCount = 0;\n      const responseMessages: Array<CoreAssistantMessage | CoreToolMessage> =\n        [];\n      let text = '';\n      const steps: GenerateTextResult<TOOLS>['steps'] = [];\n      const usage: LanguageModelUsage = {\n        completionTokens: 0,\n        promptTokens: 0,\n        totalTokens: 0,\n      };\n\n      let stepType: 'initial' | 'tool-result' | 'continue' | 'done' = 'initial';\n\n      do {\n        // once we have a 2nd step, we need to switch to messages format:\n        const currentInputFormat =\n          stepCount === 0 ? validatedPrompt.type : 'messages';\n\n        currentModelResponse = await retry(() =>\n          recordSpan({\n            name: 'ai.generateText.doGenerate',\n            attributes: selectTelemetryAttributes({\n              telemetry,\n              attributes: {\n                ...assembleOperationName({\n                  operationId: 'ai.generateText.doGenerate',\n                  telemetry,\n                }),\n                ...baseTelemetryAttributes,\n                'ai.prompt.format': { input: () => currentInputFormat },\n                'ai.prompt.messages': {\n                  input: () => JSON.stringify(promptMessages),\n                },\n\n                // standardized gen-ai llm span attributes:\n                'gen_ai.system': model.provider,\n                'gen_ai.request.model': model.modelId,\n                'gen_ai.request.frequency_penalty': settings.frequencyPenalty,\n                'gen_ai.request.max_tokens': settings.maxTokens,\n                'gen_ai.request.presence_penalty': settings.presencePenalty,\n                'gen_ai.request.stop_sequences': settings.stopSequences,\n                'gen_ai.request.temperature': settings.temperature,\n                'gen_ai.request.top_k': settings.topK,\n                'gen_ai.request.top_p': settings.topP,\n              },\n            }),\n            tracer,\n            fn: async span => {\n              const result = await model.doGenerate({\n                mode,\n                ...callSettings,\n                inputFormat: currentInputFormat,\n                prompt: promptMessages,\n                providerMetadata,\n                abortSignal,\n                headers,\n              });\n\n              // Fill in default values:\n              const responseData = {\n                id: result.response?.id ?? generateId(),\n                timestamp: result.response?.timestamp ?? currentDate(),\n                modelId: result.response?.modelId ?? model.modelId,\n              };\n\n              // Add response information to the span:\n              span.setAttributes(\n                selectTelemetryAttributes({\n                  telemetry,\n                  attributes: {\n                    'ai.response.finishReason': result.finishReason,\n                    'ai.response.text': {\n                      output: () => result.text,\n                    },\n                    'ai.response.toolCalls': {\n                      output: () => JSON.stringify(result.toolCalls),\n                    },\n                    'ai.response.id': responseData.id,\n                    'ai.response.model': responseData.modelId,\n                    'ai.response.timestamp':\n                      responseData.timestamp.toISOString(),\n\n                    'ai.usage.promptTokens': result.usage.promptTokens,\n                    'ai.usage.completionTokens': result.usage.completionTokens,\n\n                    // deprecated:\n                    'ai.finishReason': result.finishReason,\n                    'ai.result.text': {\n                      output: () => result.text,\n                    },\n                    'ai.result.toolCalls': {\n                      output: () => JSON.stringify(result.toolCalls),\n                    },\n\n                    // standardized gen-ai llm span attributes:\n                    'gen_ai.response.finish_reasons': [result.finishReason],\n                    'gen_ai.response.id': responseData.id,\n                    'gen_ai.response.model': responseData.modelId,\n                    'gen_ai.usage.input_tokens': result.usage.promptTokens,\n                    'gen_ai.usage.output_tokens': result.usage.completionTokens,\n                  },\n                }),\n              );\n\n              return { ...result, response: responseData };\n            },\n          }),\n        );\n\n        // parse tool calls:\n        currentToolCalls = (currentModelResponse.toolCalls ?? []).map(\n          modelToolCall => parseToolCall({ toolCall: modelToolCall, tools }),\n        );\n\n        // execute tools:\n        currentToolResults =\n          tools == null\n            ? []\n            : await executeTools({\n                toolCalls: currentToolCalls,\n                tools,\n                tracer,\n                telemetry,\n              });\n\n        // token usage:\n        const currentUsage = calculateLanguageModelUsage(\n          currentModelResponse.usage,\n        );\n        usage.completionTokens += currentUsage.completionTokens;\n        usage.promptTokens += currentUsage.promptTokens;\n        usage.totalTokens += currentUsage.totalTokens;\n\n        // check if another step is needed:\n        let nextStepType: 'done' | 'continue' | 'tool-result' = 'done';\n        if (++stepCount < maxSteps) {\n          if (\n            continueSteps &&\n            currentModelResponse.finishReason === 'length' &&\n            // only use continue when there are no tool calls:\n            currentToolCalls.length === 0\n          ) {\n            nextStepType = 'continue';\n          } else if (\n            // there are tool calls:\n            currentToolCalls.length > 0 &&\n            // all current tool calls have results:\n            currentToolResults.length === currentToolCalls.length\n          ) {\n            nextStepType = 'tool-result';\n          }\n        }\n\n        // text:\n        const stepText =\n          nextStepType === 'continue'\n            ? removeTextAfterLastWhitespace(currentModelResponse.text ?? '')\n            : currentModelResponse.text ?? '';\n\n        // text updates\n        text =\n          nextStepType === 'continue' || stepType === 'continue'\n            ? text + stepText\n            : stepText;\n\n        // Add step information:\n        const currentStep: StepResult<TOOLS> = {\n          stepType,\n          text: stepText,\n          toolCalls: currentToolCalls,\n          toolResults: currentToolResults,\n          finishReason: currentModelResponse.finishReason,\n          usage: currentUsage,\n          warnings: currentModelResponse.warnings,\n          logprobs: currentModelResponse.logprobs,\n          response: {\n            ...currentModelResponse.response,\n            headers: currentModelResponse.rawResponse?.headers,\n          },\n          experimental_providerMetadata: currentModelResponse.providerMetadata,\n          isContinued: nextStepType === 'continue',\n        };\n        steps.push(currentStep);\n        await onStepFinish?.(currentStep);\n\n        // append to messages for potential next step:\n        if (stepType === 'continue') {\n          // continue step: update the last assistant message\n          // continue is only possible when there are no tool calls,\n          // so we can assume that there is a single last assistant message:\n          const lastResponseMessage =\n            responseMessages.pop() as CoreAssistantMessage;\n          promptMessages.pop();\n          if (typeof lastResponseMessage.content === 'string') {\n            lastResponseMessage.content = text;\n          } else {\n            lastResponseMessage.content.push({\n              text: stepText,\n              type: 'text',\n            });\n          }\n          responseMessages.push(lastResponseMessage);\n          promptMessages.push(\n            convertToLanguageModelMessage(lastResponseMessage, null),\n          );\n        } else if (nextStepType === 'continue') {\n          const newResponseMessages = toResponseMessages({\n            text,\n            toolCalls: currentToolCalls,\n            toolResults: currentToolResults,\n          });\n\n          responseMessages.push(...newResponseMessages);\n          promptMessages.push(\n            ...newResponseMessages.map(message =>\n              convertToLanguageModelMessage(message, null),\n            ),\n          );\n        } else {\n          // next step is either done or tool-result:\n          const newResponseMessages = toResponseMessages({\n            text: currentModelResponse.text,\n            toolCalls: currentToolCalls,\n            toolResults: currentToolResults,\n          });\n\n          responseMessages.push(...newResponseMessages);\n          promptMessages.push(\n            ...newResponseMessages.map(message =>\n              convertToLanguageModelMessage(message, null),\n            ),\n          );\n        }\n\n        stepType = nextStepType;\n      } while (stepType !== 'done');\n\n      // Add response information to the span:\n      span.setAttributes(\n        selectTelemetryAttributes({\n          telemetry,\n          attributes: {\n            'ai.response.finishReason': currentModelResponse.finishReason,\n            'ai.response.text': {\n              output: () => currentModelResponse.text,\n            },\n            'ai.response.toolCalls': {\n              output: () => JSON.stringify(currentModelResponse.toolCalls),\n            },\n\n            'ai.usage.promptTokens': currentModelResponse.usage.promptTokens,\n            'ai.usage.completionTokens':\n              currentModelResponse.usage.completionTokens,\n\n            // deprecated:\n            'ai.finishReason': currentModelResponse.finishReason,\n            'ai.result.text': {\n              output: () => currentModelResponse.text,\n            },\n            'ai.result.toolCalls': {\n              output: () => JSON.stringify(currentModelResponse.toolCalls),\n            },\n          },\n        }),\n      );\n\n      return new DefaultGenerateTextResult({\n        text,\n        toolCalls: currentToolCalls,\n        toolResults: currentToolResults,\n        finishReason: currentModelResponse.finishReason,\n        usage,\n        warnings: currentModelResponse.warnings,\n        response: {\n          ...currentModelResponse.response,\n          headers: currentModelResponse.rawResponse?.headers,\n        },\n        logprobs: currentModelResponse.logprobs,\n        responseMessages,\n        steps,\n        providerMetadata: currentModelResponse.providerMetadata,\n      });\n    },\n  });\n}\n\nasync function executeTools<TOOLS extends Record<string, CoreTool>>({\n  toolCalls,\n  tools,\n  tracer,\n  telemetry,\n}: {\n  toolCalls: ToToolCallArray<TOOLS>;\n  tools: TOOLS;\n  tracer: Tracer;\n  telemetry: TelemetrySettings | undefined;\n}): Promise<ToToolResultArray<TOOLS>> {\n  const toolResults = await Promise.all(\n    toolCalls.map(async toolCall => {\n      const tool = tools[toolCall.toolName];\n\n      if (tool?.execute == null) {\n        return undefined;\n      }\n\n      const result = await recordSpan({\n        name: 'ai.toolCall',\n        attributes: selectTelemetryAttributes({\n          telemetry,\n          attributes: {\n            ...assembleOperationName({\n              operationId: 'ai.toolCall',\n              telemetry,\n            }),\n            'ai.toolCall.name': toolCall.toolName,\n            'ai.toolCall.id': toolCall.toolCallId,\n            'ai.toolCall.args': {\n              output: () => JSON.stringify(toolCall.args),\n            },\n          },\n        }),\n        tracer,\n        fn: async span => {\n          const result = await tool.execute!(toolCall.args);\n\n          try {\n            span.setAttributes(\n              selectTelemetryAttributes({\n                telemetry,\n                attributes: {\n                  'ai.toolCall.result': {\n                    output: () => JSON.stringify(result),\n                  },\n                },\n              }),\n            );\n          } catch (ignored) {\n            // JSON stringify might fail if the result is not serializable,\n            // in which case we just ignore it. In the future we might want to\n            // add an optional serialize method to the tool interface and warn\n            // if the result is not serializable.\n          }\n\n          return result;\n        },\n      });\n\n      return {\n        toolCallId: toolCall.toolCallId,\n        toolName: toolCall.toolName,\n        args: toolCall.args,\n        result,\n      } as ToToolResultArray<TOOLS>[number];\n    }),\n  );\n\n  return toolResults.filter(\n    (result): result is NonNullable<typeof result> => result != null,\n  );\n}\n\nclass DefaultGenerateTextResult<TOOLS extends Record<string, CoreTool>>\n  implements GenerateTextResult<TOOLS>\n{\n  readonly text: GenerateTextResult<TOOLS>['text'];\n  readonly toolCalls: GenerateTextResult<TOOLS>['toolCalls'];\n  readonly toolResults: GenerateTextResult<TOOLS>['toolResults'];\n  readonly finishReason: GenerateTextResult<TOOLS>['finishReason'];\n  readonly usage: GenerateTextResult<TOOLS>['usage'];\n  readonly warnings: GenerateTextResult<TOOLS>['warnings'];\n  readonly responseMessages: GenerateTextResult<TOOLS>['responseMessages'];\n  readonly roundtrips: GenerateTextResult<TOOLS>['roundtrips'];\n  readonly steps: GenerateTextResult<TOOLS>['steps'];\n  readonly rawResponse: GenerateTextResult<TOOLS>['rawResponse'];\n  readonly logprobs: GenerateTextResult<TOOLS>['logprobs'];\n  readonly experimental_providerMetadata: GenerateTextResult<TOOLS>['experimental_providerMetadata'];\n  readonly response: GenerateTextResult<TOOLS>['response'];\n\n  constructor(options: {\n    text: GenerateTextResult<TOOLS>['text'];\n    toolCalls: GenerateTextResult<TOOLS>['toolCalls'];\n    toolResults: GenerateTextResult<TOOLS>['toolResults'];\n    finishReason: GenerateTextResult<TOOLS>['finishReason'];\n    usage: GenerateTextResult<TOOLS>['usage'];\n    warnings: GenerateTextResult<TOOLS>['warnings'];\n    logprobs: GenerateTextResult<TOOLS>['logprobs'];\n    responseMessages: GenerateTextResult<TOOLS>['responseMessages'];\n    steps: GenerateTextResult<TOOLS>['steps'];\n    providerMetadata: GenerateTextResult<TOOLS>['experimental_providerMetadata'];\n    response: GenerateTextResult<TOOLS>['response'];\n  }) {\n    this.text = options.text;\n    this.toolCalls = options.toolCalls;\n    this.toolResults = options.toolResults;\n    this.finishReason = options.finishReason;\n    this.usage = options.usage;\n    this.warnings = options.warnings;\n    this.response = options.response;\n    this.responseMessages = options.responseMessages;\n    this.roundtrips = options.steps;\n    this.steps = options.steps;\n    this.experimental_providerMetadata = options.providerMetadata;\n\n    // deprecated:\n    this.rawResponse = {\n      headers: options.response.headers,\n    };\n    this.logprobs = options.logprobs;\n  }\n}\n\n/**\n * @deprecated Use `generateText` instead.\n */\nexport const experimental_generateText = generateText;\n","export {\n  AISDKError,\n  APICallError,\n  EmptyResponseBodyError,\n  InvalidPromptError,\n  InvalidResponseDataError,\n  JSONParseError,\n  LoadAPIKeyError,\n  NoContentGeneratedError,\n  NoSuchModelError,\n  TypeValidationError,\n  UnsupportedFunctionalityError,\n} from '@ai-sdk/provider';\n\nexport { InvalidArgumentError } from './invalid-argument-error';\nexport { InvalidToolArgumentsError } from './invalid-tool-arguments-error';\nexport { NoSuchToolError } from './no-such-tool-error';\n\nexport { NoObjectGeneratedError } from '../core/generate-object/no-object-generated-error';\nexport { InvalidDataContentError } from '../core/prompt/invalid-data-content-error';\nexport { InvalidMessageRoleError } from '../core/prompt/invalid-message-role-error';\nexport { MessageConversionError } from '../core/prompt/message-conversion-error';\nexport { DownloadError } from '../util/download-error';\nexport { RetryError } from '../util/retry-error';\n","import { AISDKError, getErrorMessage } from '@ai-sdk/provider';\n\nconst name = 'AI_InvalidToolArgumentsError';\nconst marker = `vercel.ai.error.${name}`;\nconst symbol = Symbol.for(marker);\n\nexport class InvalidToolArgumentsError extends AISDKError {\n  private readonly [symbol] = true; // used in isInstance\n\n  readonly toolName: string;\n  readonly toolArgs: string;\n\n  constructor({\n    toolArgs,\n    toolName,\n    cause,\n    message = `Invalid arguments for tool ${toolName}: ${getErrorMessage(\n      cause,\n    )}`,\n  }: {\n    message?: string;\n    toolArgs: string;\n    toolName: string;\n    cause: unknown;\n  }) {\n    super({ name, message, cause });\n\n    this.toolArgs = toolArgs;\n    this.toolName = toolName;\n  }\n\n  static isInstance(error: unknown): error is InvalidToolArgumentsError {\n    return AISDKError.hasMarker(error, marker);\n  }\n\n  /**\n   * @deprecated use `isInstance` instead\n   */\n  static isInvalidToolArgumentsError(\n    error: unknown,\n  ): error is InvalidToolArgumentsError {\n    return (\n      error instanceof Error &&\n      error.name === name &&\n      typeof (error as InvalidToolArgumentsError).toolName === 'string' &&\n      typeof (error as InvalidToolArgumentsError).toolArgs === 'string'\n    );\n  }\n\n  /**\n   * @deprecated Do not use this method. It will be removed in the next major version.\n   */\n  toJSON() {\n    return {\n      name: this.name,\n      message: this.message,\n      cause: this.cause,\n      stack: this.stack,\n\n      toolName: this.toolName,\n      toolArgs: this.toolArgs,\n    };\n  }\n}\n","import { AISDKError } from '@ai-sdk/provider';\n\nconst name = 'AI_NoSuchToolError';\nconst marker = `vercel.ai.error.${name}`;\nconst symbol = Symbol.for(marker);\n\nexport class NoSuchToolError extends AISDKError {\n  private readonly [symbol] = true; // used in isInstance\n\n  readonly toolName: string;\n  readonly availableTools: string[] | undefined;\n\n  constructor({\n    toolName,\n    availableTools = undefined,\n    message = `Model tried to call unavailable tool '${toolName}'. ${\n      availableTools === undefined\n        ? 'No tools are available.'\n        : `Available tools: ${availableTools.join(', ')}.`\n    }`,\n  }: {\n    toolName: string;\n    availableTools?: string[] | undefined;\n    message?: string;\n  }) {\n    super({ name, message });\n\n    this.toolName = toolName;\n    this.availableTools = availableTools;\n  }\n\n  static isInstance(error: unknown): error is NoSuchToolError {\n    return AISDKError.hasMarker(error, marker);\n  }\n\n  /**\n   * @deprecated use `isInstance` instead\n   */\n  static isNoSuchToolError(error: unknown): error is NoSuchToolError {\n    return (\n      error instanceof Error &&\n      error.name === name &&\n      'toolName' in error &&\n      error.toolName != undefined &&\n      typeof error.name === 'string'\n    );\n  }\n\n  /**\n   * @deprecated Do not use this method. It will be removed in the next major version.\n   */\n  toJSON() {\n    return {\n      name: this.name,\n      message: this.message,\n      stack: this.stack,\n\n      toolName: this.toolName,\n      availableTools: this.availableTools,\n    };\n  }\n}\n","import { AISDKError } from '@ai-sdk/provider';\nimport { ConvertibleMessage } from './convert-to-core-messages';\n\nconst name = 'AI_MessageConversionError';\nconst marker = `vercel.ai.error.${name}`;\nconst symbol = Symbol.for(marker);\n\nexport class MessageConversionError extends AISDKError {\n  private readonly [symbol] = true; // used in isInstance\n\n  readonly originalMessage: ConvertibleMessage;\n\n  constructor({\n    originalMessage,\n    message,\n  }: {\n    originalMessage: ConvertibleMessage;\n    message: string;\n  }) {\n    super({ name, message });\n\n    this.originalMessage = originalMessage;\n  }\n\n  static isInstance(error: unknown): error is MessageConversionError {\n    return AISDKError.hasMarker(error, marker);\n  }\n}\n","import {\n  LanguageModelV1FunctionTool,\n  LanguageModelV1ToolChoice,\n} from '@ai-sdk/provider';\nimport { asSchema } from '@ai-sdk/ui-utils';\nimport { CoreTool } from '../tool/tool';\nimport { CoreToolChoice } from '../types/language-model';\nimport { isNonEmptyObject } from '../util/is-non-empty-object';\n\nexport function prepareToolsAndToolChoice<\n  TOOLS extends Record<string, CoreTool>,\n>({\n  tools,\n  toolChoice,\n}: {\n  tools: TOOLS | undefined;\n  toolChoice: CoreToolChoice<TOOLS> | undefined;\n}): {\n  tools: LanguageModelV1FunctionTool[] | undefined;\n  toolChoice: LanguageModelV1ToolChoice | undefined;\n} {\n  if (!isNonEmptyObject(tools)) {\n    return {\n      tools: undefined,\n      toolChoice: undefined,\n    };\n  }\n\n  return {\n    tools: Object.entries(tools).map(([name, tool]) => ({\n      type: 'function' as const,\n      name,\n      description: tool.description,\n      parameters: asSchema(tool.parameters).jsonSchema,\n    })),\n    toolChoice:\n      toolChoice == null\n        ? { type: 'auto' }\n        : typeof toolChoice === 'string'\n        ? { type: toolChoice }\n        : { type: 'tool' as const, toolName: toolChoice.toolName as string },\n  };\n}\n","export function isNonEmptyObject(\n  object: Record<string, unknown> | undefined | null,\n): object is Record<string, unknown> {\n  return object != null && Object.keys(object).length > 0;\n}\n","const lastWhitespaceRegexp = /^([\\s\\S]*?)(\\s+)(\\S*)$/;\n\n/**\n * Splits the text on the last whitespace.\n *\n * Whitespace is defined as one or more whitespace characters,\n * e.g. space, tab, newline, etc.\n *\n * @param text - The text to split.\n * @returns The prefix, whitespace, and suffix. Undefined if there is no whitespace.\n */\nexport function splitOnLastWhitespace(text: string):\n  | {\n      prefix: string;\n      whitespace: string;\n      suffix: string;\n    }\n  | undefined {\n  const match = text.match(lastWhitespaceRegexp);\n  return match\n    ? { prefix: match[1], whitespace: match[2], suffix: match[3] }\n    : undefined;\n}\n","import { splitOnLastWhitespace } from './split-on-last-whitespace';\n\nexport function removeTextAfterLastWhitespace(text: string): string {\n  const match = splitOnLastWhitespace(text);\n  return match ? match.prefix + match.whitespace : text;\n}\n","import { LanguageModelV1FunctionToolCall } from '@ai-sdk/provider';\nimport { safeParseJSON, safeValidateTypes } from '@ai-sdk/provider-utils';\nimport { Schema, asSchema } from '@ai-sdk/ui-utils';\nimport { InvalidToolArgumentsError } from '../../errors/invalid-tool-arguments-error';\nimport { NoSuchToolError } from '../../errors/no-such-tool-error';\nimport { CoreTool } from '../tool';\nimport { inferParameters } from '../tool/tool';\nimport { ToToolCall } from './tool-call';\n\nexport function parseToolCall<TOOLS extends Record<string, CoreTool>>({\n  toolCall,\n  tools,\n}: {\n  toolCall: LanguageModelV1FunctionToolCall;\n  tools?: TOOLS;\n}): ToToolCall<TOOLS> {\n  const toolName = toolCall.toolName as keyof TOOLS & string;\n\n  if (tools == null) {\n    throw new NoSuchToolError({ toolName: toolCall.toolName });\n  }\n\n  const tool = tools[toolName];\n\n  if (tool == null) {\n    throw new NoSuchToolError({\n      toolName: toolCall.toolName,\n      availableTools: Object.keys(tools),\n    });\n  }\n\n  const schema = asSchema(tool.parameters) as Schema<\n    inferParameters<TOOLS[keyof TOOLS]['parameters']>\n  >;\n\n  // when the tool call has no arguments, we try passing an empty object to the schema\n  // (many LLMs generate empty strings for tool calls with no arguments)\n  const parseResult =\n    toolCall.args.trim() === ''\n      ? safeValidateTypes({ value: {}, schema })\n      : safeParseJSON({ text: toolCall.args, schema });\n\n  if (parseResult.success === false) {\n    throw new InvalidToolArgumentsError({\n      toolName,\n      toolArgs: toolCall.args,\n      cause: parseResult.error,\n    });\n  }\n\n  return {\n    type: 'tool-call',\n    toolCallId: toolCall.toolCallId,\n    toolName,\n    args: parseResult.value,\n  };\n}\n","import { CoreAssistantMessage, CoreToolMessage } from '../prompt';\nimport { CoreTool } from '../tool/tool';\nimport { ToToolCallArray } from './tool-call';\nimport { ToToolResultArray } from './tool-result';\n\n/**\nConverts the result of a `generateText` call to a list of response messages.\n */\nexport function toResponseMessages<TOOLS extends Record<string, CoreTool>>({\n  text = '',\n  toolCalls,\n  toolResults,\n}: {\n  text: string | undefined;\n  toolCalls: ToToolCallArray<TOOLS>;\n  toolResults: ToToolResultArray<TOOLS>;\n}): Array<CoreAssistantMessage | CoreToolMessage> {\n  const responseMessages: Array<CoreAssistantMessage | CoreToolMessage> = [];\n\n  responseMessages.push({\n    role: 'assistant',\n    content: [{ type: 'text', text }, ...toolCalls],\n  });\n\n  if (toolResults.length > 0) {\n    responseMessages.push({\n      role: 'tool',\n      content: toolResults.map(result => ({\n        type: 'tool-result',\n        toolCallId: result.toolCallId,\n        toolName: result.toolName,\n        result: result.result,\n      })),\n    });\n  }\n\n  return responseMessages;\n}\n","import {\n  LanguageModelV1Message,\n  LanguageModelV1Prompt,\n} from '@ai-sdk/provider';\nimport { createIdGenerator } from '@ai-sdk/provider-utils';\nimport { Span } from '@opentelemetry/api';\nimport { ServerResponse } from 'node:http';\nimport {\n  AIStreamCallbacksAndOptions,\n  CoreAssistantMessage,\n  CoreToolMessage,\n  formatStreamPart,\n  InvalidArgumentError,\n  StreamData,\n  TextStreamPart,\n} from '../../streams';\nimport { createResolvablePromise } from '../../util/create-resolvable-promise';\nimport { retryWithExponentialBackoff } from '../../util/retry-with-exponential-backoff';\nimport { CallSettings } from '../prompt/call-settings';\nimport {\n  convertToLanguageModelMessage,\n  convertToLanguageModelPrompt,\n} from '../prompt/convert-to-language-model-prompt';\nimport { prepareCallSettings } from '../prompt/prepare-call-settings';\nimport { prepareToolsAndToolChoice } from '../prompt/prepare-tools-and-tool-choice';\nimport { Prompt } from '../prompt/prompt';\nimport { validatePrompt } from '../prompt/validate-prompt';\nimport { assembleOperationName } from '../telemetry/assemble-operation-name';\nimport { getBaseTelemetryAttributes } from '../telemetry/get-base-telemetry-attributes';\nimport { getTracer } from '../telemetry/get-tracer';\nimport { recordSpan } from '../telemetry/record-span';\nimport { selectTelemetryAttributes } from '../telemetry/select-telemetry-attributes';\nimport { TelemetrySettings } from '../telemetry/telemetry-settings';\nimport { CoreTool } from '../tool';\nimport {\n  CallWarning,\n  CoreToolChoice,\n  FinishReason,\n  LanguageModel,\n  LogProbs,\n  ProviderMetadata,\n} from '../types';\nimport { LanguageModelUsage } from '../types/usage';\nimport {\n  AsyncIterableStream,\n  createAsyncIterableStream,\n} from '../util/async-iterable-stream';\nimport { createStitchableStream } from '../util/create-stitchable-stream';\nimport { mergeStreams } from '../util/merge-streams';\nimport { now as originalNow } from '../util/now';\nimport { prepareOutgoingHttpHeaders } from '../util/prepare-outgoing-http-headers';\nimport { prepareResponseHeaders } from '../util/prepare-response-headers';\nimport { splitOnLastWhitespace } from '../util/split-on-last-whitespace';\nimport { writeToServerResponse } from '../util/write-to-server-response';\nimport {\n  runToolsTransformation,\n  SingleRequestTextStreamPart,\n} from './run-tools-transformation';\nimport { StepResult } from './step-result';\nimport { StreamTextResult } from './stream-text-result';\nimport { toResponseMessages } from './to-response-messages';\nimport { ToToolCall } from './tool-call';\nimport { ToToolResult } from './tool-result';\n\nconst originalGenerateId = createIdGenerator({ prefix: 'aitxt-', size: 24 });\n\n/**\nGenerate a text and call tools for a given prompt using a language model.\n\nThis function streams the output. If you do not want to stream the output, use `generateText` instead.\n\n@param model - The language model to use.\n@param tools - Tools that are accessible to and can be called by the model. The model needs to support calling tools.\n\n@param system - A system message that will be part of the prompt.\n@param prompt - A simple text prompt. You can either use `prompt` or `messages` but not both.\n@param messages - A list of messages. You can either use `prompt` or `messages` but not both.\n\n@param maxTokens - Maximum number of tokens to generate.\n@param temperature - Temperature setting.\nThe value is passed through to the provider. The range depends on the provider and model.\nIt is recommended to set either `temperature` or `topP`, but not both.\n@param topP - Nucleus sampling.\nThe value is passed through to the provider. The range depends on the provider and model.\nIt is recommended to set either `temperature` or `topP`, but not both.\n@param topK - Only sample from the top K options for each subsequent token.\nUsed to remove \"long tail\" low probability responses.\nRecommended for advanced use cases only. You usually only need to use temperature.\n@param presencePenalty - Presence penalty setting.\nIt affects the likelihood of the model to repeat information that is already in the prompt.\nThe value is passed through to the provider. The range depends on the provider and model.\n@param frequencyPenalty - Frequency penalty setting.\nIt affects the likelihood of the model to repeatedly use the same words or phrases.\nThe value is passed through to the provider. The range depends on the provider and model.\n@param stopSequences - Stop sequences.\nIf set, the model will stop generating text when one of the stop sequences is generated.\n@param seed - The seed (integer) to use for random sampling.\nIf set and supported by the model, calls will generate deterministic results.\n\n@param maxRetries - Maximum number of retries. Set to 0 to disable retries. Default: 2.\n@param abortSignal - An optional abort signal that can be used to cancel the call.\n@param headers - Additional HTTP headers to be sent with the request. Only applicable for HTTP-based providers.\n\n@param maxSteps - Maximum number of sequential LLM calls (steps), e.g. when you use tool calls.\n\n@param onChunk - Callback that is called for each chunk of the stream. The stream processing will pause until the callback promise is resolved.\n@param onStepFinish - Callback that is called when each step (LLM call) is finished, including intermediate steps.\n@param onFinish - Callback that is called when the LLM response and all request tool executions\n(for tools that have an `execute` function) are finished.\n\n@return\nA result object for accessing different stream types and additional information.\n */\nexport async function streamText<TOOLS extends Record<string, CoreTool>>({\n  model,\n  tools,\n  toolChoice,\n  system,\n  prompt,\n  messages,\n  maxRetries,\n  abortSignal,\n  headers,\n  maxToolRoundtrips = 0,\n  maxSteps = maxToolRoundtrips != null ? maxToolRoundtrips + 1 : 1,\n  experimental_continueSteps: continueSteps = false,\n  experimental_telemetry: telemetry,\n  experimental_providerMetadata: providerMetadata,\n  experimental_toolCallStreaming: toolCallStreaming = false,\n  onChunk,\n  onFinish,\n  onStepFinish,\n  _internal: {\n    now = originalNow,\n    generateId = originalGenerateId,\n    currentDate = () => new Date(),\n  } = {},\n  ...settings\n}: CallSettings &\n  Prompt & {\n    /**\nThe language model to use.\n     */\n    model: LanguageModel;\n\n    /**\nThe tools that the model can call. The model needs to support calling tools.\n    */\n    tools?: TOOLS;\n\n    /**\nThe tool choice strategy. Default: 'auto'.\n     */\n    toolChoice?: CoreToolChoice<TOOLS>;\n\n    /**\nMaximum number of automatic roundtrips for tool calls.\n\nAn automatic tool call roundtrip is another LLM call with the\ntool call results when all tool calls of the last assistant\nmessage have results.\n\nA maximum number is required to prevent infinite loops in the\ncase of misconfigured tools.\n\nBy default, it's set to 0, which will disable the feature.\n\n@deprecated Use `maxSteps` instead (which is `maxToolRoundtrips` + 1).\n     */\n    maxToolRoundtrips?: number;\n\n    /**\nMaximum number of sequential LLM calls (steps), e.g. when you use tool calls. Must be at least 1.\n\nA maximum number is required to prevent infinite loops in the case of misconfigured tools.\n\nBy default, it's set to 1, which means that only a single LLM call is made.\n */\n    maxSteps?: number;\n\n    /**\nWhen enabled, the model will perform additional steps if the finish reason is \"length\" (experimental).\n\nBy default, it's set to false.\n     */\n    experimental_continueSteps?: boolean;\n\n    /**\nOptional telemetry configuration (experimental).\n     */\n    experimental_telemetry?: TelemetrySettings;\n\n    /**\nAdditional provider-specific metadata. They are passed through\nto the provider from the AI SDK and enable provider-specific\nfunctionality that can be fully encapsulated in the provider.\n */\n    experimental_providerMetadata?: ProviderMetadata;\n\n    /**\nEnable streaming of tool call deltas as they are generated. Disabled by default.\n     */\n    experimental_toolCallStreaming?: boolean;\n\n    /**\nCallback that is called for each chunk of the stream. The stream processing will pause until the callback promise is resolved.\n     */\n    onChunk?: (event: {\n      chunk: Extract<\n        TextStreamPart<TOOLS>,\n        {\n          type:\n            | 'text-delta'\n            | 'tool-call'\n            | 'tool-call-streaming-start'\n            | 'tool-call-delta'\n            | 'tool-result';\n        }\n      >;\n    }) => Promise<void> | void;\n\n    /**\nCallback that is called when the LLM response and all request tool executions\n(for tools that have an `execute` function) are finished.\n\nThe usage is the combined usage of all steps.\n     */\n    onFinish?: (\n      event: Omit<StepResult<TOOLS>, 'stepType' | 'isContinued'> & {\n        /**\nDetails for all steps.\n       */\n        readonly steps: StepResult<TOOLS>[];\n\n        /**\nThe response messages that were generated during the call. It consists of an assistant message,\npotentially containing tool calls.\n\nWhen there are tool results, there is an additional tool message with the tool results that are available.\nIf there are tools that do not have execute functions, they are not included in the tool results and\nneed to be added separately.\n     */\n        readonly responseMessages: Array<\n          CoreAssistantMessage | CoreToolMessage\n        >;\n      },\n    ) => Promise<void> | void;\n\n    /**\n    Callback that is called when each step (LLM call) is finished, including intermediate steps.\n    */\n    onStepFinish?: (event: StepResult<TOOLS>) => Promise<void> | void;\n\n    /**\n     * Internal. For test use only. May change without notice.\n     */\n    _internal?: {\n      now?: () => number;\n      generateId?: () => string;\n      currentDate?: () => Date;\n    };\n  }): Promise<StreamTextResult<TOOLS>> {\n  if (maxSteps < 1) {\n    throw new InvalidArgumentError({\n      parameter: 'maxSteps',\n      value: maxSteps,\n      message: 'maxSteps must be at least 1',\n    });\n  }\n\n  const baseTelemetryAttributes = getBaseTelemetryAttributes({\n    model,\n    telemetry,\n    headers,\n    settings: { ...settings, maxRetries },\n  });\n\n  const tracer = getTracer({ isEnabled: telemetry?.isEnabled ?? false });\n\n  return recordSpan({\n    name: 'ai.streamText',\n    attributes: selectTelemetryAttributes({\n      telemetry,\n      attributes: {\n        ...assembleOperationName({ operationId: 'ai.streamText', telemetry }),\n        ...baseTelemetryAttributes,\n        // specific settings that only make sense on the outer level:\n        'ai.prompt': {\n          input: () => JSON.stringify({ system, prompt, messages }),\n        },\n        'ai.settings.maxSteps': maxSteps,\n      },\n    }),\n    tracer,\n    endWhenDone: false,\n    fn: async rootSpan => {\n      const retry = retryWithExponentialBackoff({ maxRetries });\n\n      const startStep: StartStepFunction<TOOLS> = async ({\n        promptMessages,\n        promptType,\n      }: {\n        promptMessages: LanguageModelV1Prompt;\n        promptType: 'prompt' | 'messages';\n      }) => {\n        const {\n          result: { stream, warnings, rawResponse },\n          doStreamSpan,\n          startTimestampMs,\n        } = await retry(() =>\n          recordSpan({\n            name: 'ai.streamText.doStream',\n            attributes: selectTelemetryAttributes({\n              telemetry,\n              attributes: {\n                ...assembleOperationName({\n                  operationId: 'ai.streamText.doStream',\n                  telemetry,\n                }),\n                ...baseTelemetryAttributes,\n                'ai.prompt.format': {\n                  input: () => promptType,\n                },\n                'ai.prompt.messages': {\n                  input: () => JSON.stringify(promptMessages),\n                },\n\n                // standardized gen-ai llm span attributes:\n                'gen_ai.system': model.provider,\n                'gen_ai.request.model': model.modelId,\n                'gen_ai.request.frequency_penalty': settings.frequencyPenalty,\n                'gen_ai.request.max_tokens': settings.maxTokens,\n                'gen_ai.request.presence_penalty': settings.presencePenalty,\n                'gen_ai.request.stop_sequences': settings.stopSequences,\n                'gen_ai.request.temperature': settings.temperature,\n                'gen_ai.request.top_k': settings.topK,\n                'gen_ai.request.top_p': settings.topP,\n              },\n            }),\n            tracer,\n            endWhenDone: false,\n            fn: async doStreamSpan => ({\n              startTimestampMs: now(), // get before the call\n              doStreamSpan,\n              result: await model.doStream({\n                mode: {\n                  type: 'regular',\n                  ...prepareToolsAndToolChoice({ tools, toolChoice }),\n                },\n                ...prepareCallSettings(settings),\n                inputFormat: promptType,\n                prompt: promptMessages,\n                providerMetadata,\n                abortSignal,\n                headers,\n              }),\n            }),\n          }),\n        );\n\n        return {\n          result: {\n            stream: runToolsTransformation({\n              tools,\n              generatorStream: stream,\n              toolCallStreaming,\n              tracer,\n              telemetry,\n            }),\n            warnings,\n            rawResponse,\n          },\n          doStreamSpan,\n          startTimestampMs,\n        };\n      };\n\n      const promptMessages = await convertToLanguageModelPrompt({\n        prompt: validatePrompt({ system, prompt, messages }),\n        modelSupportsImageUrls: model.supportsImageUrls,\n      });\n\n      const {\n        result: { stream, warnings, rawResponse },\n        doStreamSpan,\n        startTimestampMs,\n      } = await startStep({\n        promptType: validatePrompt({ system, prompt, messages }).type,\n        promptMessages,\n      });\n\n      return new DefaultStreamTextResult({\n        stream,\n        warnings,\n        rawResponse,\n        onChunk,\n        onFinish,\n        onStepFinish,\n        rootSpan,\n        doStreamSpan,\n        telemetry,\n        startTimestampMs,\n        maxSteps,\n        continueSteps,\n        startStep,\n        promptMessages,\n        modelId: model.modelId,\n        now,\n        currentDate,\n        generateId,\n      });\n    },\n  });\n}\n\ntype StartStepFunction<TOOLS extends Record<string, CoreTool>> = (options: {\n  promptMessages: LanguageModelV1Prompt;\n  promptType: 'prompt' | 'messages';\n}) => Promise<{\n  result: {\n    stream: ReadableStream<SingleRequestTextStreamPart<TOOLS>>;\n    warnings?: CallWarning[] | undefined;\n    rawResponse?: {\n      headers?: Record<string, string>;\n    };\n  };\n  doStreamSpan: Span;\n  startTimestampMs: number;\n}>;\n\nclass DefaultStreamTextResult<TOOLS extends Record<string, CoreTool>>\n  implements StreamTextResult<TOOLS>\n{\n  private originalStream: ReadableStream<TextStreamPart<TOOLS>>;\n\n  // TODO needs to be changed to readonly async in v4 (and only return value from last step)\n  // (can't change before v4 because of backwards compatibility)\n  warnings: StreamTextResult<TOOLS>['warnings'];\n  rawResponse: StreamTextResult<TOOLS>['rawResponse'];\n\n  readonly usage: StreamTextResult<TOOLS>['usage'];\n  readonly finishReason: StreamTextResult<TOOLS>['finishReason'];\n  readonly experimental_providerMetadata: StreamTextResult<TOOLS>['experimental_providerMetadata'];\n  readonly text: StreamTextResult<TOOLS>['text'];\n  readonly toolCalls: StreamTextResult<TOOLS>['toolCalls'];\n  readonly toolResults: StreamTextResult<TOOLS>['toolResults'];\n  readonly response: StreamTextResult<TOOLS>['response'];\n  readonly steps: StreamTextResult<TOOLS>['steps'];\n  readonly responseMessages: StreamTextResult<TOOLS>['responseMessages'];\n\n  constructor({\n    stream,\n    warnings,\n    rawResponse,\n    onChunk,\n    onFinish,\n    onStepFinish,\n    rootSpan,\n    doStreamSpan,\n    telemetry,\n    startTimestampMs,\n    maxSteps,\n    continueSteps,\n    startStep,\n    promptMessages,\n    modelId,\n    now,\n    currentDate,\n    generateId,\n  }: {\n    stream: ReadableStream<SingleRequestTextStreamPart<TOOLS>>;\n    warnings: StreamTextResult<TOOLS>['warnings'];\n    rawResponse: StreamTextResult<TOOLS>['rawResponse'];\n    onChunk: Parameters<typeof streamText>[0]['onChunk'];\n    onFinish:\n      | ((\n          event: Omit<StepResult<TOOLS>, 'stepType' | 'isContinued'> & {\n            steps: StepResult<TOOLS>[];\n            responseMessages: Array<CoreAssistantMessage | CoreToolMessage>;\n          },\n        ) => Promise<void> | void)\n      | undefined;\n    onStepFinish:\n      | ((event: StepResult<TOOLS>) => Promise<void> | void)\n      | undefined;\n    rootSpan: Span;\n    doStreamSpan: Span;\n    telemetry: TelemetrySettings | undefined;\n    startTimestampMs: number;\n    maxSteps: number;\n    continueSteps: boolean;\n    startStep: StartStepFunction<TOOLS>;\n    promptMessages: LanguageModelV1Prompt;\n    modelId: string;\n    now: () => number;\n    currentDate: () => Date;\n    generateId: () => string;\n  }) {\n    this.warnings = warnings;\n    this.rawResponse = rawResponse;\n\n    // initialize usage promise\n    const { resolve: resolveUsage, promise: usagePromise } =\n      createResolvablePromise<LanguageModelUsage>();\n    this.usage = usagePromise;\n\n    // initialize finish reason promise\n    const { resolve: resolveFinishReason, promise: finishReasonPromise } =\n      createResolvablePromise<FinishReason>();\n    this.finishReason = finishReasonPromise;\n\n    // initialize text promise\n    const { resolve: resolveText, promise: textPromise } =\n      createResolvablePromise<string>();\n    this.text = textPromise;\n\n    // initialize toolCalls promise\n    const { resolve: resolveToolCalls, promise: toolCallsPromise } =\n      createResolvablePromise<ToToolCall<TOOLS>[]>();\n    this.toolCalls = toolCallsPromise;\n\n    // initialize toolResults promise\n    const { resolve: resolveToolResults, promise: toolResultsPromise } =\n      createResolvablePromise<ToToolResult<TOOLS>[]>();\n    this.toolResults = toolResultsPromise;\n\n    // initialize steps promise\n    const { resolve: resolveSteps, promise: stepsPromise } =\n      createResolvablePromise<StepResult<TOOLS>[]>();\n    this.steps = stepsPromise;\n\n    // initialize experimental_providerMetadata promise\n    const {\n      resolve: resolveProviderMetadata,\n      promise: providerMetadataPromise,\n    } = createResolvablePromise<ProviderMetadata | undefined>();\n    this.experimental_providerMetadata = providerMetadataPromise;\n\n    // initialize response promise\n    const { resolve: resolveResponse, promise: responsePromise } =\n      createResolvablePromise<Awaited<StreamTextResult<TOOLS>['response']>>();\n    this.response = responsePromise;\n\n    // initialize responseMessages promise\n    const {\n      resolve: resolveResponseMessages,\n      promise: responseMessagesPromise,\n    } =\n      createResolvablePromise<Array<CoreAssistantMessage | CoreToolMessage>>();\n    this.responseMessages = responseMessagesPromise;\n\n    // create a stitchable stream to send steps in a single response stream\n    const {\n      stream: stitchableStream,\n      addStream,\n      close: closeStitchableStream,\n    } = createStitchableStream<TextStreamPart<TOOLS>>();\n\n    this.originalStream = stitchableStream;\n\n    // collect step results\n    const stepResults: StepResult<TOOLS>[] = [];\n\n    const self = this;\n\n    // add the steps stream\n    function addStepStream({\n      stream,\n      startTimestamp,\n      doStreamSpan,\n      currentStep,\n      promptMessages,\n      usage = {\n        promptTokens: 0,\n        completionTokens: 0,\n        totalTokens: 0,\n      },\n      stepType,\n      previousStepText = '',\n    }: {\n      stream: ReadableStream<SingleRequestTextStreamPart<TOOLS>>;\n      startTimestamp: number;\n      doStreamSpan: Span;\n      currentStep: number;\n      promptMessages: LanguageModelV1Prompt;\n      usage: LanguageModelUsage | undefined;\n      stepType: 'initial' | 'continue' | 'tool-result';\n      previousStepText?: string;\n    }) {\n      const stepToolCalls: ToToolCall<TOOLS>[] = [];\n      const stepToolResults: ToToolResult<TOOLS>[] = [];\n      let stepFinishReason: FinishReason = 'unknown';\n      let stepUsage: LanguageModelUsage = {\n        promptTokens: 0,\n        completionTokens: 0,\n        totalTokens: 0,\n      };\n      let stepProviderMetadata: ProviderMetadata | undefined;\n      let stepFirstChunk = true;\n      let stepText = '';\n      let fullStepText = stepType === 'continue' ? previousStepText : '';\n      let stepLogProbs: LogProbs | undefined;\n      let stepResponse: { id: string; timestamp: Date; modelId: string } = {\n        id: generateId(),\n        timestamp: currentDate(),\n        modelId,\n      };\n\n      // chunk buffer when using continue:\n      let chunkBuffer = '';\n      let chunkTextPublished = false;\n\n      async function publishTextChunk({\n        controller,\n        chunk,\n      }: {\n        controller: TransformStreamDefaultController<TextStreamPart<TOOLS>>;\n        chunk: TextStreamPart<TOOLS> & { type: 'text-delta' };\n      }) {\n        controller.enqueue(chunk);\n\n        stepText += chunk.textDelta;\n        fullStepText += chunk.textDelta;\n        chunkTextPublished = true;\n\n        await onChunk?.({ chunk });\n      }\n\n      addStream(\n        stream.pipeThrough(\n          new TransformStream<\n            SingleRequestTextStreamPart<TOOLS>,\n            TextStreamPart<TOOLS>\n          >({\n            async transform(chunk, controller): Promise<void> {\n              // Telemetry for first chunk:\n              if (stepFirstChunk) {\n                const msToFirstChunk = now() - startTimestamp;\n\n                stepFirstChunk = false;\n\n                doStreamSpan.addEvent('ai.stream.firstChunk', {\n                  'ai.response.msToFirstChunk': msToFirstChunk,\n\n                  // deprecated:\n                  'ai.stream.msToFirstChunk': msToFirstChunk,\n                });\n\n                doStreamSpan.setAttributes({\n                  'ai.response.msToFirstChunk': msToFirstChunk,\n\n                  // deprecated:\n                  'ai.stream.msToFirstChunk': msToFirstChunk,\n                });\n              }\n\n              // Filter out empty text deltas\n              if (chunk.type === 'text-delta' && chunk.textDelta.length === 0) {\n                return;\n              }\n\n              const chunkType = chunk.type;\n              switch (chunkType) {\n                case 'text-delta': {\n                  if (continueSteps) {\n                    chunkBuffer += chunk.textDelta;\n\n                    const split = splitOnLastWhitespace(chunkBuffer);\n\n                    // publish the text until the last whitespace:\n                    if (split != null) {\n                      chunkBuffer = split.suffix;\n\n                      await publishTextChunk({\n                        controller,\n                        chunk: {\n                          type: 'text-delta',\n                          textDelta: split.prefix + split.whitespace,\n                        },\n                      });\n                    }\n                  } else {\n                    await publishTextChunk({ controller, chunk });\n                  }\n\n                  break;\n                }\n\n                case 'tool-call': {\n                  controller.enqueue(chunk);\n                  // store tool calls for onFinish callback and toolCalls promise:\n                  stepToolCalls.push(chunk);\n                  await onChunk?.({ chunk });\n                  break;\n                }\n\n                case 'tool-result': {\n                  controller.enqueue(chunk);\n                  // store tool results for onFinish callback and toolResults promise:\n                  stepToolResults.push(chunk);\n                  // as any needed, bc type inferences mixed up tool-result with tool-call\n                  await onChunk?.({ chunk: chunk as any });\n                  break;\n                }\n\n                case 'response-metadata': {\n                  stepResponse = {\n                    id: chunk.id ?? stepResponse.id,\n                    timestamp: chunk.timestamp ?? stepResponse.timestamp,\n                    modelId: chunk.modelId ?? stepResponse.modelId,\n                  };\n                  break;\n                }\n\n                case 'finish': {\n                  // Note: tool executions might not be finished yet when the finish event is emitted.\n                  // store usage and finish reason for promises and onFinish callback:\n                  stepUsage = chunk.usage;\n                  stepFinishReason = chunk.finishReason;\n                  stepProviderMetadata = chunk.experimental_providerMetadata;\n                  stepLogProbs = chunk.logprobs;\n\n                  // Telemetry for finish event timing\n                  // (since tool executions can take longer and distort calculations)\n                  const msToFinish = now() - startTimestamp;\n                  doStreamSpan.addEvent('ai.stream.finish');\n                  doStreamSpan.setAttributes({\n                    'ai.response.msToFinish': msToFinish,\n                    'ai.response.avgCompletionTokensPerSecond':\n                      (1000 * stepUsage.completionTokens) / msToFinish,\n                  });\n\n                  break;\n                }\n\n                case 'tool-call-streaming-start':\n                case 'tool-call-delta': {\n                  controller.enqueue(chunk);\n                  await onChunk?.({ chunk });\n                  break;\n                }\n\n                case 'error': {\n                  controller.enqueue(chunk);\n                  stepFinishReason = 'error';\n                  break;\n                }\n\n                default: {\n                  const exhaustiveCheck: never = chunkType;\n                  throw new Error(`Unknown chunk type: ${exhaustiveCheck}`);\n                }\n              }\n            },\n\n            // invoke onFinish callback and resolve toolResults promise when the stream is about to close:\n            async flush(controller) {\n              const stepToolCallsJson =\n                stepToolCalls.length > 0\n                  ? JSON.stringify(stepToolCalls)\n                  : undefined;\n\n              // determine the next step type\n              let nextStepType: 'done' | 'continue' | 'tool-result' = 'done';\n              if (currentStep + 1 < maxSteps) {\n                if (\n                  continueSteps &&\n                  stepFinishReason === 'length' &&\n                  // only use continue when there are no tool calls:\n                  stepToolCalls.length === 0\n                ) {\n                  nextStepType = 'continue';\n                } else if (\n                  // there are tool calls:\n                  stepToolCalls.length > 0 &&\n                  // all current tool calls have results:\n                  stepToolResults.length === stepToolCalls.length\n                ) {\n                  nextStepType = 'tool-result';\n                }\n              }\n\n              // when using continuation, publish buffer on final step or if there\n              // was no whitespace in the step:\n              if (\n                continueSteps &&\n                chunkBuffer.length > 0 &&\n                (nextStepType !== 'continue' || // when the next step is a regular step, publish the buffer\n                  (stepType === 'continue' && !chunkTextPublished)) // when the next step is a continue step, publish the buffer if no text was published in the step\n              ) {\n                await publishTextChunk({\n                  controller,\n                  chunk: {\n                    type: 'text-delta',\n                    textDelta: chunkBuffer,\n                  },\n                });\n                chunkBuffer = '';\n              }\n\n              // record telemetry information first to ensure best effort timing\n              try {\n                doStreamSpan.setAttributes(\n                  selectTelemetryAttributes({\n                    telemetry,\n                    attributes: {\n                      'ai.response.finishReason': stepFinishReason,\n                      'ai.response.text': { output: () => stepText },\n                      'ai.response.toolCalls': {\n                        output: () => stepToolCallsJson,\n                      },\n                      'ai.response.id': stepResponse.id,\n                      'ai.response.model': stepResponse.modelId,\n                      'ai.response.timestamp':\n                        stepResponse.timestamp.toISOString(),\n\n                      'ai.usage.promptTokens': stepUsage.promptTokens,\n                      'ai.usage.completionTokens': stepUsage.completionTokens,\n\n                      // deprecated\n                      'ai.finishReason': stepFinishReason,\n                      'ai.result.text': { output: () => stepText },\n                      'ai.result.toolCalls': {\n                        output: () => stepToolCallsJson,\n                      },\n\n                      // standardized gen-ai llm span attributes:\n                      'gen_ai.response.finish_reasons': [stepFinishReason],\n                      'gen_ai.response.id': stepResponse.id,\n                      'gen_ai.response.model': stepResponse.modelId,\n                      'gen_ai.usage.input_tokens': stepUsage.promptTokens,\n                      'gen_ai.usage.output_tokens': stepUsage.completionTokens,\n                    },\n                  }),\n                );\n              } catch (error) {\n                // ignore error setting telemetry attributes\n              } finally {\n                // finish doStreamSpan before other operations for correct timing:\n                doStreamSpan.end();\n              }\n\n              controller.enqueue({\n                type: 'step-finish',\n                finishReason: stepFinishReason,\n                usage: stepUsage,\n                experimental_providerMetadata: stepProviderMetadata,\n                logprobs: stepLogProbs,\n                response: stepResponse,\n                isContinued: nextStepType === 'continue',\n              });\n\n              const stepResult: StepResult<TOOLS> = {\n                stepType,\n                text: stepText,\n                toolCalls: stepToolCalls,\n                toolResults: stepToolResults,\n                finishReason: stepFinishReason,\n                usage: stepUsage,\n                warnings: self.warnings,\n                logprobs: stepLogProbs,\n                response: stepResponse,\n                rawResponse: self.rawResponse,\n                experimental_providerMetadata: stepProviderMetadata,\n                isContinued: nextStepType === 'continue',\n              };\n\n              stepResults.push(stepResult);\n\n              await onStepFinish?.(stepResult);\n\n              const combinedUsage = {\n                promptTokens: usage.promptTokens + stepUsage.promptTokens,\n                completionTokens:\n                  usage.completionTokens + stepUsage.completionTokens,\n                totalTokens: usage.totalTokens + stepUsage.totalTokens,\n              };\n\n              if (nextStepType !== 'done') {\n                // append to messages for the next step:\n                if (stepType === 'continue') {\n                  // continue step: update the last assistant message\n                  // continue is only possible when there are no tool calls,\n                  // so we can assume that there is a single last assistant message:\n                  const lastPromptMessage = promptMessages[\n                    promptMessages.length - 1\n                  ] as LanguageModelV1Message & {\n                    role: 'assistant';\n                  };\n\n                  lastPromptMessage.content.push({\n                    text: stepText,\n                    type: 'text',\n                  });\n                } else {\n                  // tool result step\n                  // add the assistant message with the tool calls\n                  // and the tool result messages:\n                  promptMessages.push(\n                    ...toResponseMessages({\n                      text: stepText,\n                      toolCalls: stepToolCalls,\n                      toolResults: stepToolResults,\n                    }).map(message =>\n                      convertToLanguageModelMessage(message, null),\n                    ),\n                  );\n                }\n\n                // create call and doStream span:\n                const {\n                  result,\n                  doStreamSpan,\n                  startTimestampMs: startTimestamp,\n                } = await startStep({\n                  promptType: 'messages',\n                  promptMessages,\n                });\n\n                // update warnings and rawResponse:\n                self.warnings = result.warnings;\n                self.rawResponse = result.rawResponse;\n\n                // needs to add to stitchable stream\n                addStepStream({\n                  stream: result.stream,\n                  startTimestamp,\n                  doStreamSpan,\n                  currentStep: currentStep + 1,\n                  promptMessages,\n                  usage: combinedUsage,\n                  stepType: nextStepType,\n                  previousStepText: fullStepText,\n                });\n\n                return;\n              }\n\n              try {\n                // enqueue the finish chunk:\n                controller.enqueue({\n                  type: 'finish',\n                  finishReason: stepFinishReason,\n                  usage: combinedUsage,\n                  experimental_providerMetadata: stepProviderMetadata,\n                  logprobs: stepLogProbs,\n                  response: stepResponse,\n                });\n\n                // close the stitchable stream\n                closeStitchableStream();\n\n                // Add response information to the root span:\n                rootSpan.setAttributes(\n                  selectTelemetryAttributes({\n                    telemetry,\n                    attributes: {\n                      'ai.response.finishReason': stepFinishReason,\n                      'ai.response.text': { output: () => fullStepText },\n                      'ai.response.toolCalls': {\n                        output: () => stepToolCallsJson,\n                      },\n\n                      'ai.usage.promptTokens': combinedUsage.promptTokens,\n                      'ai.usage.completionTokens':\n                        combinedUsage.completionTokens,\n\n                      // deprecated\n                      'ai.finishReason': stepFinishReason,\n                      'ai.result.text': { output: () => fullStepText },\n                      'ai.result.toolCalls': {\n                        output: () => stepToolCallsJson,\n                      },\n                    },\n                  }),\n                );\n\n                // Collect responseMessages from all steps:\n                const responseMessages = stepResults.reduce<\n                  Array<CoreAssistantMessage | CoreToolMessage>\n                >((responseMessages, step) => {\n                  if (step.stepType === 'continue') {\n                    // continue step: update the last assistant message\n                    // continue is only possible when there are no tool calls,\n                    // so we can assume that there is a single last assistant message:\n                    const lastResponseMessage =\n                      responseMessages.pop() as CoreAssistantMessage;\n\n                    if (typeof lastResponseMessage.content === 'string') {\n                      lastResponseMessage.content += step.text;\n                    } else {\n                      lastResponseMessage.content.push({\n                        text: step.text,\n                        type: 'text',\n                      });\n                    }\n\n                    return [...responseMessages, lastResponseMessage];\n                  }\n\n                  return [\n                    ...responseMessages,\n                    ...toResponseMessages({\n                      text: step.text,\n                      toolCalls: step.toolCalls,\n                      toolResults: step.toolResults,\n                    }),\n                  ];\n                }, []);\n\n                // resolve promises:\n                resolveUsage(combinedUsage);\n                resolveFinishReason(stepFinishReason!);\n                resolveText(fullStepText);\n                resolveToolCalls(stepToolCalls);\n                resolveProviderMetadata(stepProviderMetadata);\n                resolveToolResults(stepToolResults);\n                resolveResponse({\n                  ...stepResponse,\n                  headers: rawResponse?.headers,\n                });\n                resolveSteps(stepResults);\n                resolveResponseMessages(responseMessages);\n\n                // call onFinish callback:\n                await onFinish?.({\n                  finishReason: stepFinishReason,\n                  logprobs: stepLogProbs,\n                  usage: combinedUsage,\n                  text: fullStepText,\n                  toolCalls: stepToolCalls,\n                  // The tool results are inferred as a never[] type, because they are\n                  // optional and the execute method with an inferred result type is\n                  // optional as well. Therefore we need to cast the toolResults to any.\n                  // The type exposed to the users will be correctly inferred.\n                  toolResults: stepToolResults as any,\n                  rawResponse,\n                  response: {\n                    ...stepResponse,\n                    headers: rawResponse?.headers,\n                  },\n                  warnings,\n                  experimental_providerMetadata: stepProviderMetadata,\n                  steps: stepResults,\n                  responseMessages,\n                });\n              } catch (error) {\n                controller.error(error);\n              } finally {\n                rootSpan.end();\n              }\n            },\n          }),\n        ),\n      );\n    }\n\n    // add the initial stream to the stitchable stream\n    addStepStream({\n      stream,\n      startTimestamp: startTimestampMs,\n      doStreamSpan,\n      currentStep: 0,\n      promptMessages,\n      usage: undefined,\n      stepType: 'initial',\n    });\n  }\n\n  /**\nSplit out a new stream from the original stream.\nThe original stream is replaced to allow for further splitting,\nsince we do not know how many times the stream will be split.\n\nNote: this leads to buffering the stream content on the server.\nHowever, the LLM results are expected to be small enough to not cause issues.\n   */\n  private teeStream() {\n    const [stream1, stream2] = this.originalStream.tee();\n    this.originalStream = stream2;\n    return stream1;\n  }\n\n  get textStream(): AsyncIterableStream<string> {\n    return createAsyncIterableStream(this.teeStream(), {\n      transform(chunk, controller) {\n        if (chunk.type === 'text-delta') {\n          controller.enqueue(chunk.textDelta);\n        } else if (chunk.type === 'error') {\n          controller.error(chunk.error);\n        }\n      },\n    });\n  }\n\n  get fullStream(): AsyncIterableStream<TextStreamPart<TOOLS>> {\n    return createAsyncIterableStream(this.teeStream(), {\n      transform(chunk, controller) {\n        controller.enqueue(chunk);\n      },\n    });\n  }\n\n  toAIStream(callbacks: AIStreamCallbacksAndOptions = {}) {\n    return this.toDataStreamInternal({ callbacks });\n  }\n\n  private toDataStreamInternal({\n    callbacks = {},\n    getErrorMessage = () => '', // mask error messages for safety by default\n    sendUsage = true,\n  }: {\n    callbacks?: AIStreamCallbacksAndOptions;\n    getErrorMessage?: (error: unknown) => string;\n    sendUsage?: boolean;\n  } = {}) {\n    let aggregatedResponse = '';\n\n    const callbackTransformer = new TransformStream<\n      TextStreamPart<TOOLS>,\n      TextStreamPart<TOOLS>\n    >({\n      async start(): Promise<void> {\n        if (callbacks.onStart) await callbacks.onStart();\n      },\n\n      async transform(chunk, controller): Promise<void> {\n        controller.enqueue(chunk);\n\n        if (chunk.type === 'text-delta') {\n          const textDelta = chunk.textDelta;\n\n          aggregatedResponse += textDelta;\n\n          if (callbacks.onToken) await callbacks.onToken(textDelta);\n          if (callbacks.onText) await callbacks.onText(textDelta);\n        }\n      },\n\n      async flush(): Promise<void> {\n        if (callbacks.onCompletion)\n          await callbacks.onCompletion(aggregatedResponse);\n        if (callbacks.onFinal) await callbacks.onFinal(aggregatedResponse);\n      },\n    });\n\n    const streamPartsTransformer = new TransformStream<\n      TextStreamPart<TOOLS>,\n      string\n    >({\n      transform: async (chunk, controller) => {\n        const chunkType = chunk.type;\n        switch (chunkType) {\n          case 'text-delta': {\n            controller.enqueue(formatStreamPart('text', chunk.textDelta));\n            break;\n          }\n\n          case 'tool-call-streaming-start': {\n            controller.enqueue(\n              formatStreamPart('tool_call_streaming_start', {\n                toolCallId: chunk.toolCallId,\n                toolName: chunk.toolName,\n              }),\n            );\n            break;\n          }\n\n          case 'tool-call-delta': {\n            controller.enqueue(\n              formatStreamPart('tool_call_delta', {\n                toolCallId: chunk.toolCallId,\n                argsTextDelta: chunk.argsTextDelta,\n              }),\n            );\n            break;\n          }\n\n          case 'tool-call': {\n            controller.enqueue(\n              formatStreamPart('tool_call', {\n                toolCallId: chunk.toolCallId,\n                toolName: chunk.toolName,\n                args: chunk.args,\n              }),\n            );\n            break;\n          }\n\n          case 'tool-result': {\n            controller.enqueue(\n              formatStreamPart('tool_result', {\n                toolCallId: chunk.toolCallId,\n                result: chunk.result,\n              }),\n            );\n            break;\n          }\n\n          case 'error': {\n            controller.enqueue(\n              formatStreamPart('error', getErrorMessage(chunk.error)),\n            );\n            break;\n          }\n\n          case 'step-finish': {\n            controller.enqueue(\n              formatStreamPart('finish_step', {\n                finishReason: chunk.finishReason,\n                usage: sendUsage\n                  ? {\n                      promptTokens: chunk.usage.promptTokens,\n                      completionTokens: chunk.usage.completionTokens,\n                    }\n                  : undefined,\n                isContinued: chunk.isContinued,\n              }),\n            );\n            break;\n          }\n\n          case 'finish': {\n            controller.enqueue(\n              formatStreamPart('finish_message', {\n                finishReason: chunk.finishReason,\n                usage: sendUsage\n                  ? {\n                      promptTokens: chunk.usage.promptTokens,\n                      completionTokens: chunk.usage.completionTokens,\n                    }\n                  : undefined,\n              }),\n            );\n            break;\n          }\n\n          default: {\n            const exhaustiveCheck: never = chunkType;\n            throw new Error(`Unknown chunk type: ${exhaustiveCheck}`);\n          }\n        }\n      },\n    });\n\n    return this.fullStream\n      .pipeThrough(callbackTransformer)\n      .pipeThrough(streamPartsTransformer)\n      .pipeThrough(new TextEncoderStream());\n  }\n\n  pipeAIStreamToResponse(\n    response: ServerResponse,\n    init?: { headers?: Record<string, string>; status?: number },\n  ): void {\n    return this.pipeDataStreamToResponse(response, init);\n  }\n\n  pipeDataStreamToResponse(\n    response: ServerResponse,\n    options?:\n      | ResponseInit\n      | {\n          init?: ResponseInit;\n          data?: StreamData;\n          getErrorMessage?: (error: unknown) => string;\n          sendUsage?: boolean;\n        },\n  ) {\n    const init: ResponseInit | undefined =\n      options == null\n        ? undefined\n        : 'init' in options\n        ? options.init\n        : {\n            headers: 'headers' in options ? options.headers : undefined,\n            status: 'status' in options ? options.status : undefined,\n            statusText:\n              'statusText' in options ? options.statusText : undefined,\n          };\n\n    const data: StreamData | undefined =\n      options == null\n        ? undefined\n        : 'data' in options\n        ? options.data\n        : undefined;\n\n    const getErrorMessage: ((error: unknown) => string) | undefined =\n      options == null\n        ? undefined\n        : 'getErrorMessage' in options\n        ? options.getErrorMessage\n        : undefined;\n\n    const sendUsage: boolean | undefined =\n      options == null\n        ? undefined\n        : 'sendUsage' in options\n        ? options.sendUsage\n        : undefined;\n\n    writeToServerResponse({\n      response,\n      status: init?.status,\n      statusText: init?.statusText,\n      headers: prepareOutgoingHttpHeaders(init, {\n        contentType: 'text/plain; charset=utf-8',\n        dataStreamVersion: 'v1',\n      }),\n      stream: this.toDataStream({ data, getErrorMessage, sendUsage }),\n    });\n  }\n\n  pipeTextStreamToResponse(response: ServerResponse, init?: ResponseInit) {\n    writeToServerResponse({\n      response,\n      status: init?.status,\n      statusText: init?.statusText,\n      headers: prepareOutgoingHttpHeaders(init, {\n        contentType: 'text/plain; charset=utf-8',\n      }),\n      stream: this.textStream.pipeThrough(new TextEncoderStream()),\n    });\n  }\n\n  toAIStreamResponse(\n    options?: ResponseInit | { init?: ResponseInit; data?: StreamData },\n  ): Response {\n    return this.toDataStreamResponse(options);\n  }\n\n  toDataStream(options?: {\n    data?: StreamData;\n    getErrorMessage?: (error: unknown) => string;\n    sendUsage?: boolean;\n  }) {\n    const stream = this.toDataStreamInternal({\n      getErrorMessage: options?.getErrorMessage,\n      sendUsage: options?.sendUsage,\n    });\n\n    return options?.data ? mergeStreams(options?.data.stream, stream) : stream;\n  }\n\n  toDataStreamResponse(\n    options?:\n      | ResponseInit\n      | {\n          init?: ResponseInit;\n          data?: StreamData;\n          getErrorMessage?: (error: unknown) => string;\n          sendUsage?: boolean;\n        },\n  ): Response {\n    const init: ResponseInit | undefined =\n      options == null\n        ? undefined\n        : 'init' in options\n        ? options.init\n        : {\n            headers: 'headers' in options ? options.headers : undefined,\n            status: 'status' in options ? options.status : undefined,\n            statusText:\n              'statusText' in options ? options.statusText : undefined,\n          };\n\n    const data: StreamData | undefined =\n      options == null\n        ? undefined\n        : 'data' in options\n        ? options.data\n        : undefined;\n\n    const getErrorMessage: ((error: unknown) => string) | undefined =\n      options == null\n        ? undefined\n        : 'getErrorMessage' in options\n        ? options.getErrorMessage\n        : undefined;\n\n    const sendUsage: boolean | undefined =\n      options == null\n        ? undefined\n        : 'sendUsage' in options\n        ? options.sendUsage\n        : undefined;\n\n    return new Response(\n      this.toDataStream({ data, getErrorMessage, sendUsage }),\n      {\n        status: init?.status ?? 200,\n        statusText: init?.statusText,\n        headers: prepareResponseHeaders(init, {\n          contentType: 'text/plain; charset=utf-8',\n          dataStreamVersion: 'v1',\n        }),\n      },\n    );\n  }\n\n  toTextStreamResponse(init?: ResponseInit): Response {\n    return new Response(this.textStream.pipeThrough(new TextEncoderStream()), {\n      status: init?.status ?? 200,\n      headers: prepareResponseHeaders(init, {\n        contentType: 'text/plain; charset=utf-8',\n      }),\n    });\n  }\n}\n\n/**\n * @deprecated Use `streamText` instead.\n */\nexport const experimental_streamText = streamText;\n","/**\n * Creates a stitchable stream that can pipe one stream at a time.\n *\n * @template T - The type of values emitted by the streams.\n * @returns {Object} An object containing the stitchable stream and control methods.\n */\nexport function createStitchableStream<T>() {\n  let innerStreamReaders: ReadableStreamDefaultReader<T>[] = [];\n  let controller: ReadableStreamDefaultController<T> | null = null;\n  let isClosed = false;\n\n  const processPull = async () => {\n    // Case 1: Outer stream is closed and no more inner streams\n    if (isClosed && innerStreamReaders.length === 0) {\n      controller?.close();\n      return;\n    }\n\n    // Case 2: No inner streams available, but outer stream is open\n    if (innerStreamReaders.length === 0) {\n      return;\n    }\n\n    try {\n      const { value, done } = await innerStreamReaders[0].read();\n\n      if (done) {\n        // Case 3: Current inner stream is done\n        innerStreamReaders.shift(); // Remove the finished stream\n\n        // Continue pulling from the next stream if available\n        if (innerStreamReaders.length > 0) {\n          await processPull();\n        } else if (isClosed) {\n          controller?.close();\n        }\n      } else {\n        // Case 4: Current inner stream returns an item\n        controller?.enqueue(value);\n      }\n    } catch (error) {\n      // Case 5: Current inner stream throws an error\n      controller?.error(error);\n      innerStreamReaders.shift(); // Remove the errored stream\n\n      if (isClosed && innerStreamReaders.length === 0) {\n        controller?.close();\n      }\n    }\n  };\n\n  return {\n    stream: new ReadableStream<T>({\n      start(controllerParam) {\n        controller = controllerParam;\n      },\n      pull: processPull,\n      async cancel() {\n        for (const reader of innerStreamReaders) {\n          await reader.cancel();\n        }\n        innerStreamReaders = [];\n        isClosed = true;\n      },\n    }),\n    addStream: (innerStream: ReadableStream<T>) => {\n      if (isClosed) {\n        throw new Error('Cannot add inner stream: outer stream is closed');\n      }\n\n      innerStreamReaders.push(innerStream.getReader());\n    },\n    close: () => {\n      isClosed = true;\n\n      if (innerStreamReaders.length === 0) {\n        controller?.close();\n      }\n    },\n  };\n}\n","/**\n * Merges two readable streams into a single readable stream, emitting values\n * from each stream as they become available.\n *\n * The first stream is prioritized over the second stream. If both streams have\n * values available, the first stream's value is emitted first.\n *\n * @template VALUE1 - The type of values emitted by the first stream.\n * @template VALUE2 - The type of values emitted by the second stream.\n * @param {ReadableStream<VALUE1>} stream1 - The first readable stream.\n * @param {ReadableStream<VALUE2>} stream2 - The second readable stream.\n * @returns {ReadableStream<VALUE1 | VALUE2>} A new readable stream that emits values from both input streams.\n */\nexport function mergeStreams<VALUE1, VALUE2>(\n  stream1: ReadableStream<VALUE1>,\n  stream2: ReadableStream<VALUE2>,\n): ReadableStream<VALUE1 | VALUE2> {\n  const reader1 = stream1.getReader();\n  const reader2 = stream2.getReader();\n\n  let lastRead1: Promise<ReadableStreamReadResult<VALUE1>> | undefined =\n    undefined;\n  let lastRead2: Promise<ReadableStreamReadResult<VALUE2>> | undefined =\n    undefined;\n\n  let stream1Done = false;\n  let stream2Done = false;\n\n  // only use when stream 2 is done:\n  async function readStream1(\n    controller: ReadableStreamDefaultController<VALUE1 | VALUE2>,\n  ) {\n    try {\n      if (lastRead1 == null) {\n        lastRead1 = reader1.read();\n      }\n\n      const result = await lastRead1;\n      lastRead1 = undefined;\n\n      if (!result.done) {\n        controller.enqueue(result.value);\n      } else {\n        controller.close();\n      }\n    } catch (error) {\n      controller.error(error);\n    }\n  }\n\n  // only use when stream 1 is done:\n  async function readStream2(\n    controller: ReadableStreamDefaultController<VALUE1 | VALUE2>,\n  ) {\n    try {\n      if (lastRead2 == null) {\n        lastRead2 = reader2.read();\n      }\n\n      const result = await lastRead2;\n      lastRead2 = undefined;\n\n      if (!result.done) {\n        controller.enqueue(result.value);\n      } else {\n        controller.close();\n      }\n    } catch (error) {\n      controller.error(error);\n    }\n  }\n\n  return new ReadableStream<VALUE1 | VALUE2>({\n    async pull(controller) {\n      try {\n        // stream 1 is done, we can only read from stream 2:\n        if (stream1Done) {\n          await readStream2(controller);\n          return;\n        }\n\n        // stream 2 is done, we can only read from stream 1:\n        if (stream2Done) {\n          await readStream1(controller);\n          return;\n        }\n\n        // pull the next value from the stream that was read last:\n        if (lastRead1 == null) {\n          lastRead1 = reader1.read();\n        }\n        if (lastRead2 == null) {\n          lastRead2 = reader2.read();\n        }\n\n        // Note on Promise.race (prioritizing stream 1 over stream 2):\n        // If the iterable contains one or more non-promise values and/or an already settled promise,\n        // then Promise.race() will settle to the first of these values found in the iterable.\n        const { result, reader } = await Promise.race([\n          lastRead1.then(result => ({ result, reader: reader1 })),\n          lastRead2.then(result => ({ result, reader: reader2 })),\n        ]);\n\n        if (!result.done) {\n          controller.enqueue(result.value);\n        }\n\n        if (reader === reader1) {\n          lastRead1 = undefined;\n          if (result.done) {\n            // stream 1 is done, we can only read from stream 2:\n            await readStream2(controller);\n            stream1Done = true;\n          }\n        } else {\n          lastRead2 = undefined;\n          // stream 2 is done, we can only read from stream 1:\n          if (result.done) {\n            stream2Done = true;\n            await readStream1(controller);\n          }\n        }\n      } catch (error) {\n        controller.error(error);\n      }\n    },\n    cancel() {\n      reader1.cancel();\n      reader2.cancel();\n    },\n  });\n}\n","import { LanguageModelV1StreamPart } from '@ai-sdk/provider';\nimport { generateId } from '@ai-sdk/ui-utils';\nimport { Tracer } from '@opentelemetry/api';\nimport { NoSuchToolError } from '../../errors/no-such-tool-error';\nimport { assembleOperationName } from '../telemetry/assemble-operation-name';\nimport { recordSpan } from '../telemetry/record-span';\nimport { selectTelemetryAttributes } from '../telemetry/select-telemetry-attributes';\nimport { TelemetrySettings } from '../telemetry/telemetry-settings';\nimport { CoreTool } from '../tool';\nimport {\n  FinishReason,\n  LanguageModelUsage,\n  LogProbs,\n  ProviderMetadata,\n} from '../types';\nimport { calculateLanguageModelUsage } from '../types/usage';\nimport { parseToolCall } from './parse-tool-call';\nimport { ToToolCall } from './tool-call';\nimport { ToToolResult } from './tool-result';\n\nexport type SingleRequestTextStreamPart<\n  TOOLS extends Record<string, CoreTool>,\n> =\n  | {\n      type: 'text-delta';\n      textDelta: string;\n    }\n  | ({\n      type: 'tool-call';\n    } & ToToolCall<TOOLS>)\n  | {\n      type: 'tool-call-streaming-start';\n      toolCallId: string;\n      toolName: string;\n    }\n  | {\n      type: 'tool-call-delta';\n      toolCallId: string;\n      toolName: string;\n      argsTextDelta: string;\n    }\n  | ({\n      type: 'tool-result';\n    } & ToToolResult<TOOLS>)\n  | {\n      type: 'response-metadata';\n      id?: string;\n      timestamp?: Date;\n      modelId?: string;\n    }\n  | {\n      type: 'finish';\n      finishReason: FinishReason;\n      logprobs?: LogProbs;\n      usage: LanguageModelUsage;\n      experimental_providerMetadata?: ProviderMetadata;\n    }\n  | {\n      type: 'error';\n      error: unknown;\n    };\n\nexport function runToolsTransformation<TOOLS extends Record<string, CoreTool>>({\n  tools,\n  generatorStream,\n  toolCallStreaming,\n  tracer,\n  telemetry,\n}: {\n  tools: TOOLS | undefined;\n  generatorStream: ReadableStream<LanguageModelV1StreamPart>;\n  toolCallStreaming: boolean;\n  tracer: Tracer;\n  telemetry: TelemetrySettings | undefined;\n}): ReadableStream<SingleRequestTextStreamPart<TOOLS>> {\n  let canClose = false;\n  const outstandingToolCalls = new Set<string>();\n\n  // tool results stream\n  let toolResultsStreamController: ReadableStreamDefaultController<\n    SingleRequestTextStreamPart<TOOLS>\n  > | null = null;\n  const toolResultsStream = new ReadableStream<\n    SingleRequestTextStreamPart<TOOLS>\n  >({\n    start(controller) {\n      toolResultsStreamController = controller;\n    },\n  });\n\n  // keep track of active tool calls\n  const activeToolCalls: Record<string, boolean> = {};\n\n  // forward stream\n  const forwardStream = new TransformStream<\n    LanguageModelV1StreamPart,\n    SingleRequestTextStreamPart<TOOLS>\n  >({\n    transform(\n      chunk: LanguageModelV1StreamPart,\n      controller: TransformStreamDefaultController<\n        SingleRequestTextStreamPart<TOOLS>\n      >,\n    ) {\n      const chunkType = chunk.type;\n\n      switch (chunkType) {\n        // forward:\n        case 'text-delta':\n        case 'response-metadata':\n        case 'error': {\n          controller.enqueue(chunk);\n          break;\n        }\n\n        // forward with less information:\n        case 'tool-call-delta': {\n          if (toolCallStreaming) {\n            if (!activeToolCalls[chunk.toolCallId]) {\n              controller.enqueue({\n                type: 'tool-call-streaming-start',\n                toolCallId: chunk.toolCallId,\n                toolName: chunk.toolName,\n              });\n\n              activeToolCalls[chunk.toolCallId] = true;\n            }\n\n            controller.enqueue({\n              type: 'tool-call-delta',\n              toolCallId: chunk.toolCallId,\n              toolName: chunk.toolName,\n              argsTextDelta: chunk.argsTextDelta,\n            });\n          }\n          break;\n        }\n\n        // process tool call:\n        case 'tool-call': {\n          const toolName = chunk.toolName as keyof TOOLS & string;\n\n          if (tools == null) {\n            toolResultsStreamController!.enqueue({\n              type: 'error',\n              error: new NoSuchToolError({ toolName: chunk.toolName }),\n            });\n            break;\n          }\n\n          const tool = tools[toolName];\n\n          if (tool == null) {\n            toolResultsStreamController!.enqueue({\n              type: 'error',\n              error: new NoSuchToolError({\n                toolName: chunk.toolName,\n                availableTools: Object.keys(tools),\n              }),\n            });\n\n            break;\n          }\n\n          try {\n            const toolCall = parseToolCall({\n              toolCall: chunk,\n              tools,\n            });\n\n            controller.enqueue(toolCall);\n\n            if (tool.execute != null) {\n              const toolExecutionId = generateId(); // use our own id to guarantee uniqueness\n              outstandingToolCalls.add(toolExecutionId);\n\n              // Note: we don't await the tool execution here (by leaving out 'await' on recordSpan),\n              // because we want to process the next chunk as soon as possible.\n              // This is important for the case where the tool execution takes a long time.\n              recordSpan({\n                name: 'ai.toolCall',\n                attributes: selectTelemetryAttributes({\n                  telemetry,\n                  attributes: {\n                    ...assembleOperationName({\n                      operationId: 'ai.toolCall',\n                      telemetry,\n                    }),\n                    'ai.toolCall.name': toolCall.toolName,\n                    'ai.toolCall.id': toolCall.toolCallId,\n                    'ai.toolCall.args': {\n                      output: () => JSON.stringify(toolCall.args),\n                    },\n                  },\n                }),\n                tracer,\n                fn: async span =>\n                  tool.execute!(toolCall.args).then(\n                    (result: any) => {\n                      toolResultsStreamController!.enqueue({\n                        ...toolCall,\n                        type: 'tool-result',\n                        result,\n                      } as any);\n\n                      outstandingToolCalls.delete(toolExecutionId);\n\n                      // close the tool results controller if no more outstanding tool calls\n                      if (canClose && outstandingToolCalls.size === 0) {\n                        toolResultsStreamController!.close();\n                      }\n\n                      // record telemetry\n                      try {\n                        span.setAttributes(\n                          selectTelemetryAttributes({\n                            telemetry,\n                            attributes: {\n                              'ai.toolCall.result': {\n                                output: () => JSON.stringify(result),\n                              },\n                            },\n                          }),\n                        );\n                      } catch (ignored) {\n                        // JSON stringify might fail if the result is not serializable,\n                        // in which case we just ignore it. In the future we might want to\n                        // add an optional serialize method to the tool interface and warn\n                        // if the result is not serializable.\n                      }\n                    },\n                    (error: any) => {\n                      toolResultsStreamController!.enqueue({\n                        type: 'error',\n                        error,\n                      });\n\n                      outstandingToolCalls.delete(toolExecutionId);\n\n                      // close the tool results controller if no more outstanding tool calls\n                      if (canClose && outstandingToolCalls.size === 0) {\n                        toolResultsStreamController!.close();\n                      }\n                    },\n                  ),\n              });\n            }\n          } catch (error) {\n            toolResultsStreamController!.enqueue({\n              type: 'error',\n              error,\n            });\n          }\n\n          break;\n        }\n\n        // process finish:\n        case 'finish': {\n          controller.enqueue({\n            type: 'finish',\n            finishReason: chunk.finishReason,\n            logprobs: chunk.logprobs,\n            usage: calculateLanguageModelUsage(chunk.usage),\n            experimental_providerMetadata: chunk.providerMetadata,\n          });\n          break;\n        }\n\n        default: {\n          const _exhaustiveCheck: never = chunkType;\n          throw new Error(`Unhandled chunk type: ${_exhaustiveCheck}`);\n        }\n      }\n    },\n\n    flush() {\n      canClose = true;\n\n      if (outstandingToolCalls.size === 0) {\n        toolResultsStreamController!.close();\n      }\n    },\n  });\n\n  // combine the generator stream and the tool results stream\n  return new ReadableStream<SingleRequestTextStreamPart<TOOLS>>({\n    async start(controller) {\n      // need to wait for both pipes so there are no dangling promises that\n      // can cause uncaught promise rejections when the stream is aborted\n      return Promise.all([\n        generatorStream.pipeThrough(forwardStream).pipeTo(\n          new WritableStream({\n            write(chunk) {\n              controller.enqueue(chunk);\n            },\n            close() {\n              // the generator stream controller is automatically closed when it's consumed\n            },\n          }),\n        ),\n        toolResultsStream.pipeTo(\n          new WritableStream({\n            write(chunk) {\n              controller.enqueue(chunk);\n            },\n            close() {\n              controller.close();\n            },\n          }),\n        ),\n      ]);\n    },\n  });\n}\n","import { LanguageModelV1, LanguageModelV1CallOptions } from '@ai-sdk/provider';\nimport { Experimental_LanguageModelV1Middleware } from './language-model-v1-middleware';\n\n/**\n * Wraps a LanguageModelV1 instance with middleware functionality.\n * This function allows you to apply middleware to transform parameters,\n * wrap generate operations, and wrap stream operations of a language model.\n *\n * @param options - Configuration options for wrapping the language model.\n * @param options.model - The original LanguageModelV1 instance to be wrapped.\n * @param options.middleware - The middleware to be applied to the language model.\n * @param options.modelId - Optional custom model ID to override the original model's ID.\n * @param options.providerId - Optional custom provider ID to override the original model's provider.\n * @returns A new LanguageModelV1 instance with middleware applied.\n */\nexport const experimental_wrapLanguageModel = ({\n  model,\n  middleware: { transformParams, wrapGenerate, wrapStream },\n  modelId,\n  providerId,\n}: {\n  model: LanguageModelV1;\n  middleware: Experimental_LanguageModelV1Middleware;\n  modelId?: string;\n  providerId?: string;\n}): LanguageModelV1 => {\n  async function doTransform({\n    params,\n    type,\n  }: {\n    params: LanguageModelV1CallOptions;\n    type: 'generate' | 'stream';\n  }) {\n    return transformParams ? await transformParams({ params, type }) : params;\n  }\n\n  return {\n    specificationVersion: 'v1',\n\n    provider: providerId ?? model.provider,\n    modelId: modelId ?? model.modelId,\n\n    defaultObjectGenerationMode: model.defaultObjectGenerationMode,\n    supportsImageUrls: model.supportsImageUrls,\n    supportsStructuredOutputs: model.supportsStructuredOutputs,\n\n    async doGenerate(\n      params: LanguageModelV1CallOptions,\n    ): Promise<Awaited<ReturnType<LanguageModelV1['doGenerate']>>> {\n      const transformedParams = await doTransform({ params, type: 'generate' });\n      const doGenerate = async () => model.doGenerate(transformedParams);\n      return wrapGenerate\n        ? wrapGenerate({ doGenerate, params: transformedParams, model })\n        : doGenerate();\n    },\n\n    async doStream(\n      params: LanguageModelV1CallOptions,\n    ): Promise<Awaited<ReturnType<LanguageModelV1['doStream']>>> {\n      const transformedParams = await doTransform({ params, type: 'stream' });\n      const doStream = async () => model.doStream(transformedParams);\n      return wrapStream\n        ? wrapStream({ doStream, params: transformedParams, model })\n        : doStream();\n    },\n  };\n};\n","import { Attachment } from '@ai-sdk/ui-utils';\nimport { FilePart, ImagePart, TextPart } from './content-part';\nimport {\n  convertDataContentToUint8Array,\n  convertUint8ArrayToText,\n} from './data-content';\n\ntype ContentPart = TextPart | ImagePart | FilePart;\n\n/**\n * Converts a list of attachments to a list of content parts\n * for consumption by `ai/core` functions.\n * Currently only supports images and text attachments.\n */\nexport function attachmentsToParts(attachments: Attachment[]): ContentPart[] {\n  const parts: ContentPart[] = [];\n\n  for (const attachment of attachments) {\n    let url;\n\n    try {\n      url = new URL(attachment.url);\n    } catch (error) {\n      throw new Error(`Invalid URL: ${attachment.url}`);\n    }\n\n    switch (url.protocol) {\n      case 'http:':\n      case 'https:': {\n        if (attachment.contentType?.startsWith('image/')) {\n          parts.push({ type: 'image', image: url });\n        } else {\n          if (!attachment.contentType) {\n            throw new Error(\n              'If the attachment is not an image, it must specify a content type',\n            );\n          }\n\n          parts.push({\n            type: 'file',\n            data: url,\n            mimeType: attachment.contentType,\n          });\n        }\n        break;\n      }\n\n      case 'data:': {\n        let header;\n        let base64Content;\n        let mimeType;\n\n        try {\n          [header, base64Content] = attachment.url.split(',');\n          mimeType = header.split(';')[0].split(':')[1];\n        } catch (error) {\n          throw new Error(`Error processing data URL: ${attachment.url}`);\n        }\n\n        if (mimeType == null || base64Content == null) {\n          throw new Error(`Invalid data URL format: ${attachment.url}`);\n        }\n\n        if (attachment.contentType?.startsWith('image/')) {\n          parts.push({\n            type: 'image',\n            image: convertDataContentToUint8Array(base64Content),\n          });\n        } else if (attachment.contentType?.startsWith('text/')) {\n          parts.push({\n            type: 'text',\n            text: convertUint8ArrayToText(\n              convertDataContentToUint8Array(base64Content),\n            ),\n          });\n        } else {\n          if (!attachment.contentType) {\n            throw new Error(\n              'If the attachment is not an image or text, it must specify a content type',\n            );\n          }\n\n          parts.push({\n            type: 'file',\n            data: base64Content,\n            mimeType: attachment.contentType,\n          });\n        }\n\n        break;\n      }\n\n      default: {\n        throw new Error(`Unsupported URL protocol: ${url.protocol}`);\n      }\n    }\n  }\n\n  return parts;\n}\n","import { Attachment, ToolInvocation } from '@ai-sdk/ui-utils';\nimport { CoreMessage } from '../prompt';\nimport { attachmentsToParts } from './attachments-to-parts';\nimport { MessageConversionError } from './message-conversion-error';\n\n// Compatible with Message. Interface is limited to increase flexibility.\n// Only exposed internally.\nexport type ConvertibleMessage = {\n  role:\n    | 'system'\n    | 'user'\n    | 'assistant'\n    | 'function' // @deprecated\n    | 'data'\n    | 'tool'; // @deprecated\n\n  content: string;\n  toolInvocations?: ToolInvocation[];\n  experimental_attachments?: Attachment[];\n};\n\n/**\nConverts an array of messages from useChat into an array of CoreMessages that can be used\nwith the AI core functions (e.g. `streamText`).\n */\nexport function convertToCoreMessages(messages: Array<ConvertibleMessage>) {\n  const coreMessages: CoreMessage[] = [];\n\n  for (const message of messages) {\n    const { role, content, toolInvocations, experimental_attachments } =\n      message;\n\n    switch (role) {\n      case 'system': {\n        coreMessages.push({\n          role: 'system',\n          content,\n        });\n        break;\n      }\n\n      case 'user': {\n        coreMessages.push({\n          role: 'user',\n          content: experimental_attachments\n            ? [\n                { type: 'text', text: content },\n                ...attachmentsToParts(experimental_attachments),\n              ]\n            : content,\n        });\n        break;\n      }\n\n      case 'assistant': {\n        if (toolInvocations == null) {\n          coreMessages.push({ role: 'assistant', content });\n          break;\n        }\n\n        // assistant message with tool calls\n        coreMessages.push({\n          role: 'assistant',\n          content: [\n            { type: 'text', text: content },\n            ...toolInvocations.map(({ toolCallId, toolName, args }) => ({\n              type: 'tool-call' as const,\n              toolCallId,\n              toolName,\n              args,\n            })),\n          ],\n        });\n\n        // tool message with tool results\n        coreMessages.push({\n          role: 'tool',\n          content: toolInvocations.map(ToolInvocation => {\n            if (!('result' in ToolInvocation)) {\n              throw new MessageConversionError({\n                originalMessage: message,\n                message:\n                  'ToolInvocation must have a result: ' +\n                  JSON.stringify(ToolInvocation),\n              });\n            }\n\n            const { toolCallId, toolName, args, result } = ToolInvocation;\n\n            return {\n              type: 'tool-result' as const,\n              toolCallId,\n              toolName,\n              args,\n              result,\n            };\n          }),\n        });\n\n        break;\n      }\n\n      case 'function':\n      case 'data':\n      case 'tool': {\n        // ignore\n        break;\n      }\n\n      default: {\n        const _exhaustiveCheck: never = role;\n        throw new MessageConversionError({\n          originalMessage: message,\n          message: `Unsupported role: ${_exhaustiveCheck}`,\n        });\n      }\n    }\n  }\n\n  return coreMessages;\n}\n","import { EmbeddingModelV1, LanguageModelV1 } from '@ai-sdk/provider';\nimport { Provider } from '../types';\nimport { NoSuchModelError } from '@ai-sdk/provider';\n\n/**\n * Creates a custom provider with specified language models, text embedding models, and an optional fallback provider.\n *\n * @param {Object} options - The options for creating the custom provider.\n * @param {Record<string, LanguageModelV1>} [options.languageModels] - A record of language models, where keys are model IDs and values are LanguageModelV1 instances.\n * @param {Record<string, EmbeddingModelV1<string>>} [options.textEmbeddingModels] - A record of text embedding models, where keys are model IDs and values are EmbeddingModelV1<string> instances.\n * @param {Provider} [options.fallbackProvider] - An optional fallback provider to use when a requested model is not found in the custom provider.\n * @returns {Provider} A Provider object with languageModel and textEmbeddingModel methods.\n *\n * @throws {NoSuchModelError} Throws when a requested model is not found and no fallback provider is available.\n */\nexport function experimental_customProvider({\n  languageModels,\n  textEmbeddingModels,\n  fallbackProvider,\n}: {\n  languageModels?: Record<string, LanguageModelV1>;\n  textEmbeddingModels?: Record<string, EmbeddingModelV1<string>>;\n  fallbackProvider?: Provider;\n}): Provider {\n  return {\n    languageModel(modelId: string): LanguageModelV1 {\n      if (languageModels != null && modelId in languageModels) {\n        return languageModels[modelId];\n      }\n\n      if (fallbackProvider) {\n        return fallbackProvider.languageModel(modelId);\n      }\n\n      throw new NoSuchModelError({ modelId, modelType: 'languageModel' });\n    },\n\n    textEmbeddingModel(modelId: string): EmbeddingModelV1<string> {\n      if (textEmbeddingModels != null && modelId in textEmbeddingModels) {\n        return textEmbeddingModels[modelId];\n      }\n\n      if (fallbackProvider) {\n        return fallbackProvider.textEmbeddingModel(modelId);\n      }\n\n      throw new NoSuchModelError({ modelId, modelType: 'textEmbeddingModel' });\n    },\n  };\n}\n","import { AISDKError, NoSuchModelError } from '@ai-sdk/provider';\n\nconst name = 'AI_NoSuchProviderError';\nconst marker = `vercel.ai.error.${name}`;\nconst symbol = Symbol.for(marker);\n\nexport class NoSuchProviderError extends NoSuchModelError {\n  private readonly [symbol] = true; // used in isInstance\n\n  readonly providerId: string;\n  readonly availableProviders: string[];\n\n  constructor({\n    modelId,\n    modelType,\n    providerId,\n    availableProviders,\n    message = `No such provider: ${providerId} (available providers: ${availableProviders.join()})`,\n  }: {\n    modelId: string;\n    modelType: 'languageModel' | 'textEmbeddingModel';\n    providerId: string;\n    availableProviders: string[];\n    message?: string;\n  }) {\n    super({ errorName: name, modelId, modelType, message });\n\n    this.providerId = providerId;\n    this.availableProviders = availableProviders;\n  }\n\n  static isInstance(error: unknown): error is NoSuchProviderError {\n    return AISDKError.hasMarker(error, marker);\n  }\n\n  /**\n   * @deprecated use `isInstance` instead\n   */\n  static isNoSuchProviderError(error: unknown): error is NoSuchProviderError {\n    return (\n      error instanceof Error &&\n      error.name === name &&\n      typeof (error as NoSuchProviderError).providerId === 'string' &&\n      Array.isArray((error as NoSuchProviderError).availableProviders)\n    );\n  }\n\n  /**\n   * @deprecated Do not use this method. It will be removed in the next major version.\n   */\n  toJSON() {\n    return {\n      name: this.name,\n      message: this.message,\n      stack: this.stack,\n\n      modelId: this.modelId,\n      modelType: this.modelType,\n\n      providerId: this.providerId,\n      availableProviders: this.availableProviders,\n    };\n  }\n}\n","import { NoSuchModelError } from '@ai-sdk/provider';\nimport { EmbeddingModel, LanguageModel, Provider } from '../types';\nimport { NoSuchProviderError } from './no-such-provider-error';\nimport { experimental_Provider } from './provider';\n\n/**\nRegistry for managing models. It enables getting a model with a string id.\n\n@deprecated Use `experimental_Provider` instead.\n */\nexport type experimental_ProviderRegistry = Provider;\n\n/**\n * @deprecated Use `experimental_ProviderRegistry` instead.\n */\nexport type experimental_ModelRegistry = experimental_ProviderRegistry;\n\n/**\n * Creates a registry for the given providers.\n */\nexport function experimental_createProviderRegistry(\n  providers: Record<string, experimental_Provider | Provider>,\n): Provider {\n  const registry = new DefaultProviderRegistry();\n\n  for (const [id, provider] of Object.entries(providers)) {\n    registry.registerProvider({ id, provider });\n  }\n\n  return registry;\n}\n\n/**\n * @deprecated Use `experimental_createProviderRegistry` instead.\n */\nexport const experimental_createModelRegistry =\n  experimental_createProviderRegistry;\n\nclass DefaultProviderRegistry implements Provider {\n  private providers: Record<string, experimental_Provider | Provider> = {};\n\n  registerProvider({\n    id,\n    provider,\n  }: {\n    id: string;\n    provider: experimental_Provider | Provider;\n  }): void {\n    this.providers[id] = provider;\n  }\n\n  private getProvider(id: string): experimental_Provider | Provider {\n    const provider = this.providers[id];\n\n    if (provider == null) {\n      throw new NoSuchProviderError({\n        modelId: id,\n        modelType: 'languageModel',\n        providerId: id,\n        availableProviders: Object.keys(this.providers),\n      });\n    }\n\n    return provider;\n  }\n\n  private splitId(\n    id: string,\n    modelType: 'languageModel' | 'textEmbeddingModel',\n  ): [string, string] {\n    const index = id.indexOf(':');\n\n    if (index === -1) {\n      throw new NoSuchModelError({\n        modelId: id,\n        modelType,\n        message:\n          `Invalid ${modelType} id for registry: ${id} ` +\n          `(must be in the format \"providerId:modelId\")`,\n      });\n    }\n\n    return [id.slice(0, index), id.slice(index + 1)];\n  }\n\n  languageModel(id: string): LanguageModel {\n    const [providerId, modelId] = this.splitId(id, 'languageModel');\n    const model = this.getProvider(providerId).languageModel?.(modelId);\n\n    if (model == null) {\n      throw new NoSuchModelError({ modelId: id, modelType: 'languageModel' });\n    }\n\n    return model;\n  }\n\n  textEmbeddingModel(id: string): EmbeddingModel<string> {\n    const [providerId, modelId] = this.splitId(id, 'textEmbeddingModel');\n    const provider = this.getProvider(providerId);\n\n    const model =\n      provider.textEmbeddingModel?.(modelId) ??\n      ('textEmbedding' in provider\n        ? provider.textEmbedding?.(modelId)\n        : undefined);\n\n    if (model == null) {\n      throw new NoSuchModelError({\n        modelId: id,\n        modelType: 'textEmbeddingModel',\n      });\n    }\n\n    return model;\n  }\n\n  /**\n   * @deprecated Use `textEmbeddingModel` instead.\n   */\n  textEmbedding(id: string): EmbeddingModel<string> {\n    return this.textEmbeddingModel(id);\n  }\n}\n","import { Schema } from '@ai-sdk/ui-utils';\nimport { z } from 'zod';\n\ntype Parameters = z.ZodTypeAny | Schema<any>;\n\nexport type inferParameters<PARAMETERS extends Parameters> =\n  PARAMETERS extends Schema<any>\n    ? PARAMETERS['_type']\n    : PARAMETERS extends z.ZodTypeAny\n    ? z.infer<PARAMETERS>\n    : never;\n\n/**\nA tool contains the description and the schema of the input that the tool expects.\nThis enables the language model to generate the input.\n\nThe tool can also contain an optional execute function for the actual execution function of the tool.\n */\nexport interface CoreTool<PARAMETERS extends Parameters = any, RESULT = any> {\n  /**\nAn optional description of what the tool does. Will be used by the language model to decide whether to use the tool.\n   */\n  description?: string;\n\n  /**\nThe schema of the input that the tool expects. The language model will use this to generate the input.\nIt is also used to validate the output of the language model.\nUse descriptions to make the input understandable for the language model.\n   */\n  parameters: PARAMETERS;\n\n  /**\nAn async function that is called with the arguments from the tool call and produces a result.\nIf not provided, the tool will not be executed automatically.\n   */\n  execute?: (args: inferParameters<PARAMETERS>) => PromiseLike<RESULT>;\n}\n\n/**\nHelper function for inferring the execute args of a tool.\n */\n// Note: special type inference is needed for the execute function args to make sure they are inferred correctly.\nexport function tool<PARAMETERS extends Parameters, RESULT>(\n  tool: CoreTool<PARAMETERS, RESULT> & {\n    execute: (args: inferParameters<PARAMETERS>) => PromiseLike<RESULT>;\n  },\n): CoreTool<PARAMETERS, RESULT> & {\n  execute: (args: inferParameters<PARAMETERS>) => PromiseLike<RESULT>;\n};\nexport function tool<PARAMETERS extends Parameters, RESULT>(\n  tool: CoreTool<PARAMETERS, RESULT> & {\n    execute?: undefined;\n  },\n): CoreTool<PARAMETERS, RESULT> & {\n  execute: undefined;\n};\nexport function tool<PARAMETERS extends Parameters, RESULT = any>(\n  tool: CoreTool<PARAMETERS, RESULT>,\n): CoreTool<PARAMETERS, RESULT> {\n  return tool;\n}\n\n/**\n * @deprecated Use `CoreTool` instead.\n */\nexport type ExperimentalTool = CoreTool;\n","/**\n * Calculates the cosine similarity between two vectors. This is a useful metric for\n * comparing the similarity of two vectors such as embeddings.\n *\n * @param vector1 - The first vector.\n * @param vector2 - The second vector.\n *\n * @returns The cosine similarity between vector1 and vector2.\n * @throws {Error} If the vectors do not have the same length.\n */\nexport function cosineSimilarity(vector1: number[], vector2: number[]) {\n  if (vector1.length !== vector2.length) {\n    throw new Error(\n      `Vectors must have the same length (vector1: ${vector1.length} elements, vector2: ${vector2.length} elements)`,\n    );\n  }\n\n  return (\n    dotProduct(vector1, vector2) / (magnitude(vector1) * magnitude(vector2))\n  );\n}\n\n/**\n * Calculates the dot product of two vectors.\n * @param vector1 - The first vector.\n * @param vector2 - The second vector.\n * @returns The dot product of vector1 and vector2.\n */\nfunction dotProduct(vector1: number[], vector2: number[]) {\n  return vector1.reduce(\n    (accumulator: number, value: number, index: number) =>\n      accumulator + value * vector2[index]!,\n    0,\n  );\n}\n\n/**\n * Calculates the magnitude of a vector.\n * @param vector - The vector.\n * @returns The magnitude of the vector.\n */\nfunction magnitude(vector: number[]) {\n  return Math.sqrt(dotProduct(vector, vector));\n}\n","import {\n  createParser,\n  type EventSourceParser,\n  type ParsedEvent,\n  type ReconnectInterval,\n} from 'eventsource-parser';\nimport { OpenAIStreamCallbacks } from './openai-stream';\n\nexport interface FunctionCallPayload {\n  name: string;\n  arguments: Record<string, unknown>;\n}\nexport interface ToolCallPayload {\n  tools: {\n    id: string;\n    type: 'function';\n    func: {\n      name: string;\n      arguments: Record<string, unknown>;\n    };\n  }[];\n}\n\n/**\n * Configuration options and helper callback methods for AIStream stream lifecycle events.\n * @interface\n */\nexport interface AIStreamCallbacksAndOptions {\n  /** `onStart`: Called once when the stream is initialized. */\n  onStart?: () => Promise<void> | void;\n  /** `onCompletion`: Called for each tokenized message. */\n  onCompletion?: (completion: string) => Promise<void> | void;\n  /** `onFinal`: Called once when the stream is closed with the final completion message. */\n  onFinal?: (completion: string) => Promise<void> | void;\n  /** `onToken`: Called for each tokenized message. */\n  onToken?: (token: string) => Promise<void> | void;\n  /** `onText`: Called for each text chunk. */\n  onText?: (text: string) => Promise<void> | void;\n  /**\n   * @deprecated This flag is no longer used and only retained for backwards compatibility.\n   * You can remove it from your code.\n   */\n  experimental_streamData?: boolean;\n}\n\n/**\n * Options for the AIStreamParser.\n * @interface\n * @property {string} event - The event (type) from the server side event stream.\n */\nexport interface AIStreamParserOptions {\n  event?: string;\n}\n\n/**\n * Custom parser for AIStream data.\n * @interface\n * @param {string} data - The data to be parsed.\n * @param {AIStreamParserOptions} options - The options for the parser.\n * @returns {string | void} The parsed data or void.\n */\nexport interface AIStreamParser {\n  (data: string, options: AIStreamParserOptions):\n    | string\n    | void\n    | { isText: false; content: string };\n}\n\n/**\n * Creates a TransformStream that parses events from an EventSource stream using a custom parser.\n * @param {AIStreamParser} customParser - Function to handle event data.\n * @returns {TransformStream<Uint8Array, string>} TransformStream parsing events.\n */\nexport function createEventStreamTransformer(\n  customParser?: AIStreamParser,\n): TransformStream<Uint8Array, string | { isText: false; content: string }> {\n  const textDecoder = new TextDecoder();\n  let eventSourceParser: EventSourceParser;\n\n  return new TransformStream({\n    async start(controller): Promise<void> {\n      eventSourceParser = createParser(\n        (event: ParsedEvent | ReconnectInterval) => {\n          if (\n            ('data' in event &&\n              event.type === 'event' &&\n              event.data === '[DONE]') ||\n            // Replicate doesn't send [DONE] but does send a 'done' event\n            // @see https://replicate.com/docs/streaming\n            (event as any).event === 'done'\n          ) {\n            controller.terminate();\n            return;\n          }\n\n          if ('data' in event) {\n            const parsedMessage = customParser\n              ? customParser(event.data, {\n                  event: event.event,\n                })\n              : event.data;\n            if (parsedMessage) controller.enqueue(parsedMessage);\n          }\n        },\n      );\n    },\n\n    transform(chunk) {\n      eventSourceParser.feed(textDecoder.decode(chunk));\n    },\n  });\n}\n\n/**\n * Creates a transform stream that encodes input messages and invokes optional callback functions.\n * The transform stream uses the provided callbacks to execute custom logic at different stages of the stream's lifecycle.\n * - `onStart`: Called once when the stream is initialized.\n * - `onToken`: Called for each tokenized message.\n * - `onCompletion`: Called every time an AIStream completion message is received. This can occur multiple times when using e.g. OpenAI functions\n * - `onFinal`: Called once when the stream is closed with the final completion message.\n *\n * This function is useful when you want to process a stream of messages and perform specific actions during the stream's lifecycle.\n *\n * @param {AIStreamCallbacksAndOptions} [callbacks] - An object containing the callback functions.\n * @return {TransformStream<string, Uint8Array>} A transform stream that encodes input messages as Uint8Array and allows the execution of custom logic through callbacks.\n *\n * @example\n * const callbacks = {\n *   onStart: async () => console.log('Stream started'),\n *   onToken: async (token) => console.log(`Token: ${token}`),\n *   onCompletion: async (completion) => console.log(`Completion: ${completion}`)\n *   onFinal: async () => data.close()\n * };\n * const transformer = createCallbacksTransformer(callbacks);\n */\nexport function createCallbacksTransformer(\n  cb: AIStreamCallbacksAndOptions | OpenAIStreamCallbacks | undefined,\n): TransformStream<string | { isText: false; content: string }, Uint8Array> {\n  const textEncoder = new TextEncoder();\n  let aggregatedResponse = '';\n  const callbacks = cb || {};\n\n  return new TransformStream({\n    async start(): Promise<void> {\n      if (callbacks.onStart) await callbacks.onStart();\n    },\n\n    async transform(message, controller): Promise<void> {\n      const content = typeof message === 'string' ? message : message.content;\n\n      controller.enqueue(textEncoder.encode(content));\n\n      aggregatedResponse += content;\n\n      if (callbacks.onToken) await callbacks.onToken(content);\n      if (callbacks.onText && typeof message === 'string') {\n        await callbacks.onText(message);\n      }\n    },\n\n    async flush(): Promise<void> {\n      const isOpenAICallbacks = isOfTypeOpenAIStreamCallbacks(callbacks);\n      // If it's OpenAICallbacks, it has an experimental_onFunctionCall which means that the createFunctionCallTransformer\n      // will handle calling onComplete.\n      if (callbacks.onCompletion) {\n        await callbacks.onCompletion(aggregatedResponse);\n      }\n\n      if (callbacks.onFinal && !isOpenAICallbacks) {\n        await callbacks.onFinal(aggregatedResponse);\n      }\n    },\n  });\n}\n\nfunction isOfTypeOpenAIStreamCallbacks(\n  callbacks: AIStreamCallbacksAndOptions | OpenAIStreamCallbacks,\n): callbacks is OpenAIStreamCallbacks {\n  return 'experimental_onFunctionCall' in callbacks;\n}\n/**\n * Returns a stateful function that, when invoked, trims leading whitespace\n * from the input text. The trimming only occurs on the first invocation, ensuring that\n * subsequent calls do not alter the input text. This is particularly useful in scenarios\n * where a text stream is being processed and only the initial whitespace should be removed.\n *\n * @return {function(string): string} A function that takes a string as input and returns a string\n * with leading whitespace removed if it is the first invocation; otherwise, it returns the input unchanged.\n *\n * @example\n * const trimStart = trimStartOfStreamHelper();\n * const output1 = trimStart(\"   text\"); // \"text\"\n * const output2 = trimStart(\"   text\"); // \"   text\"\n *\n */\nexport function trimStartOfStreamHelper(): (text: string) => string {\n  let isStreamStart = true;\n\n  return (text: string): string => {\n    if (isStreamStart) {\n      text = text.trimStart();\n      if (text) isStreamStart = false;\n    }\n    return text;\n  };\n}\n\n/**\n * Returns a ReadableStream created from the response, parsed and handled with custom logic.\n * The stream goes through two transformation stages, first parsing the events and then\n * invoking the provided callbacks.\n *\n * For 2xx HTTP responses:\n * - The function continues with standard stream processing.\n *\n * For non-2xx HTTP responses:\n * - If the response body is defined, it asynchronously extracts and decodes the response body.\n * - It then creates a custom ReadableStream to propagate a detailed error message.\n *\n * @param {Response} response - The response.\n * @param {AIStreamParser} customParser - The custom parser function.\n * @param {AIStreamCallbacksAndOptions} callbacks - The callbacks.\n * @return {ReadableStream} The AIStream.\n * @throws Will throw an error if the response is not OK.\n */\nexport function AIStream(\n  response: Response,\n  customParser?: AIStreamParser,\n  callbacks?: AIStreamCallbacksAndOptions,\n): ReadableStream<Uint8Array> {\n  if (!response.ok) {\n    if (response.body) {\n      const reader = response.body.getReader();\n      return new ReadableStream({\n        async start(controller) {\n          const { done, value } = await reader.read();\n          if (!done) {\n            const errorText = new TextDecoder().decode(value);\n            controller.error(new Error(`Response error: ${errorText}`));\n          }\n        },\n      });\n    } else {\n      return new ReadableStream({\n        start(controller) {\n          controller.error(new Error('Response error: No response body'));\n        },\n      });\n    }\n  }\n\n  const responseBodyStream = response.body || createEmptyReadableStream();\n\n  return responseBodyStream\n    .pipeThrough(createEventStreamTransformer(customParser))\n    .pipeThrough(createCallbacksTransformer(callbacks));\n}\n\n// outputs lines like\n// 0: chunk\n// 0: more chunk\n// 1: a fct call\n// z: added data from Data\n\n/**\n * Creates an empty ReadableStream that immediately closes upon creation.\n * This function is used as a fallback for creating a ReadableStream when the response body is null or undefined,\n * ensuring that the subsequent pipeline processing doesn't fail due to a lack of a stream.\n *\n * @returns {ReadableStream} An empty and closed ReadableStream instance.\n */\nfunction createEmptyReadableStream(): ReadableStream {\n  return new ReadableStream({\n    start(controller) {\n      controller.close();\n    },\n  });\n}\n\n/**\n * Implements ReadableStream.from(asyncIterable), which isn't documented in MDN and isn't implemented in node.\n * https://github.com/whatwg/streams/commit/8d7a0bf26eb2cc23e884ddbaac7c1da4b91cf2bc\n */\nexport function readableFromAsyncIterable<T>(iterable: AsyncIterable<T>) {\n  let it = iterable[Symbol.asyncIterator]();\n  return new ReadableStream<T>({\n    async pull(controller) {\n      const { done, value } = await it.next();\n      if (done) controller.close();\n      else controller.enqueue(value);\n    },\n\n    async cancel(reason) {\n      await it.return?.(reason);\n    },\n  });\n}\n","import { JSONValue, formatStreamPart } from '@ai-sdk/ui-utils';\nimport { HANGING_STREAM_WARNING_TIME_MS } from '../util/constants';\n\n/**\n * A stream wrapper to send custom JSON-encoded data back to the client.\n */\nexport class StreamData {\n  private encoder = new TextEncoder();\n\n  private controller: ReadableStreamController<Uint8Array> | null = null;\n  public stream: ReadableStream<Uint8Array>;\n\n  private isClosed: boolean = false;\n  private warningTimeout: NodeJS.Timeout | null = null;\n\n  constructor() {\n    const self = this;\n\n    this.stream = new ReadableStream({\n      start: async controller => {\n        self.controller = controller;\n\n        // Set a timeout to show a warning if the stream is not closed within 3 seconds\n        if (process.env.NODE_ENV === 'development') {\n          self.warningTimeout = setTimeout(() => {\n            console.warn(\n              'The data stream is hanging. Did you forget to close it with `data.close()`?',\n            );\n          }, HANGING_STREAM_WARNING_TIME_MS);\n        }\n      },\n      pull: controller => {\n        // No-op: we don't need to do anything special on pull\n      },\n      cancel: reason => {\n        this.isClosed = true;\n      },\n    });\n  }\n\n  async close(): Promise<void> {\n    if (this.isClosed) {\n      throw new Error('Data Stream has already been closed.');\n    }\n\n    if (!this.controller) {\n      throw new Error('Stream controller is not initialized.');\n    }\n\n    this.controller.close();\n    this.isClosed = true;\n\n    // Clear the warning timeout if the stream is closed\n    if (this.warningTimeout) {\n      clearTimeout(this.warningTimeout);\n    }\n  }\n\n  append(value: JSONValue): void {\n    if (this.isClosed) {\n      throw new Error('Data Stream has already been closed.');\n    }\n\n    if (!this.controller) {\n      throw new Error('Stream controller is not initialized.');\n    }\n\n    this.controller.enqueue(\n      this.encoder.encode(formatStreamPart('data', [value])),\n    );\n  }\n\n  appendMessageAnnotation(value: JSONValue): void {\n    if (this.isClosed) {\n      throw new Error('Data Stream has already been closed.');\n    }\n\n    if (!this.controller) {\n      throw new Error('Stream controller is not initialized.');\n    }\n\n    this.controller.enqueue(\n      this.encoder.encode(formatStreamPart('message_annotations', [value])),\n    );\n  }\n}\n\n/**\n * A TransformStream for LLMs that do not have their own transform stream handlers managing encoding (e.g. OpenAIStream has one for function call handling).\n * This assumes every chunk is a 'text' chunk.\n */\nexport function createStreamDataTransformer() {\n  const encoder = new TextEncoder();\n  const decoder = new TextDecoder();\n  return new TransformStream({\n    transform: async (chunk, controller) => {\n      const message = decoder.decode(chunk);\n      controller.enqueue(encoder.encode(formatStreamPart('text', message)));\n    },\n  });\n}\n\n/**\n@deprecated Use `StreamData` instead.\n */\nexport class experimental_StreamData extends StreamData {}\n","/**\n * Warning time for notifying developers that a stream is hanging in dev mode\n * using a console.warn.\n */\nexport const HANGING_STREAM_WARNING_TIME_MS = 15 * 1000;\n","import {\n  AIStream,\n  readableFromAsyncIterable,\n  type AIStreamCallbacksAndOptions,\n  createCallbacksTransformer,\n} from './ai-stream';\nimport { createStreamDataTransformer } from './stream-data';\n\n// from anthropic sdk (Completion)\ninterface CompletionChunk {\n  /**\n   * Unique object identifier.\n   *\n   * The format and length of IDs may change over time.\n   */\n  id: string;\n\n  /**\n   * The resulting completion up to and excluding the stop sequences.\n   */\n  completion: string;\n\n  /**\n   * The model that handled the request.\n   */\n  model: string;\n\n  /**\n   * The reason that we stopped.\n   *\n   * This may be one the following values:\n   *\n   * - `\"stop_sequence\"`: we reached a stop sequence — either provided by you via the\n   *   `stop_sequences` parameter, or a stop sequence built into the model\n   * - `\"max_tokens\"`: we exceeded `max_tokens_to_sample` or the model's maximum\n   */\n  stop_reason: string | null;\n\n  /**\n   * Object type.\n   *\n   * For Text Completions, this is always `\"completion\"`.\n   */\n  type: 'completion';\n}\n\ninterface StreamError {\n  error: {\n    type: string;\n    message: string;\n  };\n}\n\ninterface StreamPing {}\n\ntype StreamData = CompletionChunk | StreamError | StreamPing;\n\ninterface Message {\n  id: string;\n  content: Array<ContentBlock>;\n  model: string;\n  role: 'assistant';\n  stop_reason: 'end_turn' | 'max_tokens' | 'stop_sequence' | null;\n  stop_sequence: string | null;\n  type: 'message';\n}\n\ninterface ContentBlock {\n  text: string;\n  type: 'text';\n}\n\ninterface TextDelta {\n  text: string;\n  type: 'text_delta';\n}\n\ninterface ContentBlockDeltaEvent {\n  delta: TextDelta;\n  index: number;\n  type: 'content_block_delta';\n}\n\ninterface ContentBlockStartEvent {\n  content_block: ContentBlock;\n  index: number;\n  type: 'content_block_start';\n}\n\ninterface ContentBlockStopEvent {\n  index: number;\n  type: 'content_block_stop';\n}\n\ninterface MessageDeltaEventDelta {\n  stop_reason: 'end_turn' | 'max_tokens' | 'stop_sequence' | null;\n  stop_sequence: string | null;\n}\n\ninterface MessageDeltaEvent {\n  delta: MessageDeltaEventDelta;\n  type: 'message_delta';\n}\n\ntype MessageStreamEvent =\n  | MessageStartEvent\n  | MessageDeltaEvent\n  | MessageStopEvent\n  | ContentBlockStartEvent\n  | ContentBlockDeltaEvent\n  | ContentBlockStopEvent;\n\ninterface MessageStartEvent {\n  message: Message;\n  type: 'message_start';\n}\n\ninterface MessageStopEvent {\n  type: 'message_stop';\n}\n\nfunction parseAnthropicStream(): (data: string) => string | void {\n  let previous = '';\n\n  return data => {\n    const json = JSON.parse(data as string) as StreamData;\n\n    // error event\n    if ('error' in json) {\n      throw new Error(`${json.error.type}: ${json.error.message}`);\n    }\n\n    // ping event\n    if (!('completion' in json)) {\n      return;\n    }\n\n    // On API versions older than 2023-06-01,\n    // Anthropic's `completion` field is cumulative unlike OpenAI's\n    // deltas. In order to compute the delta, we must slice out the text\n    // we previously received.\n    const text = json.completion;\n    if (\n      !previous ||\n      (text.length > previous.length && text.startsWith(previous))\n    ) {\n      const delta = text.slice(previous.length);\n      previous = text;\n\n      return delta;\n    }\n\n    return text;\n  };\n}\n\nasync function* streamable(\n  stream: AsyncIterable<CompletionChunk> | AsyncIterable<MessageStreamEvent>,\n) {\n  for await (const chunk of stream) {\n    if ('completion' in chunk) {\n      // completion stream\n      const text = chunk.completion;\n      if (text) yield text;\n    } else if ('delta' in chunk) {\n      // messge stream\n      const { delta } = chunk;\n      if ('text' in delta) {\n        const text = delta.text;\n        if (text) yield text;\n      }\n    }\n  }\n}\n\n/**\n * Accepts either a fetch Response from the Anthropic `POST /v1/complete` endpoint,\n * or the return value of `await client.completions.create({ stream: true })`\n * from the `@anthropic-ai/sdk` package.\n *\n * @deprecated Use the [Anthropic provider](https://sdk.vercel.ai/providers/ai-sdk-providers/anthropic) instead.\n */\nexport function AnthropicStream(\n  res:\n    | Response\n    | AsyncIterable<CompletionChunk>\n    | AsyncIterable<MessageStreamEvent>,\n  cb?: AIStreamCallbacksAndOptions,\n): ReadableStream {\n  if (Symbol.asyncIterator in res) {\n    return readableFromAsyncIterable(streamable(res))\n      .pipeThrough(createCallbacksTransformer(cb))\n      .pipeThrough(createStreamDataTransformer());\n  } else {\n    return AIStream(res, parseAnthropicStream(), cb).pipeThrough(\n      createStreamDataTransformer(),\n    );\n  }\n}\n","import {\n  AssistantMessage,\n  DataMessage,\n  formatStreamPart,\n} from '@ai-sdk/ui-utils';\nimport type { AssistantStream } from 'openai/lib/AssistantStream';\nimport type { Run } from 'openai/resources/beta/threads/runs/runs';\n\n/**\nYou can pass the thread and the latest message into the `AssistantResponse`. This establishes the context for the response.\n */\ntype AssistantResponseSettings = {\n  /**\nThe thread ID that the response is associated with.\n   */\n  threadId: string;\n\n  /**\nThe ID of the latest message that the response is associated with.\n */\n  messageId: string;\n};\n\n/**\nThe process parameter is a callback in which you can run the assistant on threads, and send messages and data messages to the client.\n */\ntype AssistantResponseCallback = (options: {\n  /**\n@deprecated use variable from outer scope instead.\n   */\n  threadId: string;\n\n  /**\n@deprecated use variable from outer scope instead.\n   */\n  messageId: string;\n\n  /**\nForwards an assistant message (non-streaming) to the client.\n   */\n  sendMessage: (message: AssistantMessage) => void;\n\n  /**\nSend a data message to the client. You can use this to provide information for rendering custom UIs while the assistant is processing the thread.\n */\n  sendDataMessage: (message: DataMessage) => void;\n\n  /**\nForwards the assistant response stream to the client. Returns the `Run` object after it completes, or when it requires an action.\n   */\n  forwardStream: (stream: AssistantStream) => Promise<Run | undefined>;\n}) => Promise<void>;\n\n/**\nThe `AssistantResponse` allows you to send a stream of assistant update to `useAssistant`.\nIt is designed to facilitate streaming assistant responses to the `useAssistant` hook.\nIt receives an assistant thread and a current message, and can send messages and data messages to the client.\n */\nexport function AssistantResponse(\n  { threadId, messageId }: AssistantResponseSettings,\n  process: AssistantResponseCallback,\n): Response {\n  const stream = new ReadableStream({\n    async start(controller) {\n      const textEncoder = new TextEncoder();\n\n      const sendMessage = (message: AssistantMessage) => {\n        controller.enqueue(\n          textEncoder.encode(formatStreamPart('assistant_message', message)),\n        );\n      };\n\n      const sendDataMessage = (message: DataMessage) => {\n        controller.enqueue(\n          textEncoder.encode(formatStreamPart('data_message', message)),\n        );\n      };\n\n      const sendError = (errorMessage: string) => {\n        controller.enqueue(\n          textEncoder.encode(formatStreamPart('error', errorMessage)),\n        );\n      };\n\n      const forwardStream = async (stream: AssistantStream) => {\n        let result: Run | undefined = undefined;\n\n        for await (const value of stream) {\n          switch (value.event) {\n            case 'thread.message.created': {\n              controller.enqueue(\n                textEncoder.encode(\n                  formatStreamPart('assistant_message', {\n                    id: value.data.id,\n                    role: 'assistant',\n                    content: [{ type: 'text', text: { value: '' } }],\n                  }),\n                ),\n              );\n              break;\n            }\n\n            case 'thread.message.delta': {\n              const content = value.data.delta.content?.[0];\n\n              if (content?.type === 'text' && content.text?.value != null) {\n                controller.enqueue(\n                  textEncoder.encode(\n                    formatStreamPart('text', content.text.value),\n                  ),\n                );\n              }\n\n              break;\n            }\n\n            case 'thread.run.completed':\n            case 'thread.run.requires_action': {\n              result = value.data;\n              break;\n            }\n          }\n        }\n\n        return result;\n      };\n\n      // send the threadId and messageId as the first message:\n      controller.enqueue(\n        textEncoder.encode(\n          formatStreamPart('assistant_control_data', {\n            threadId,\n            messageId,\n          }),\n        ),\n      );\n\n      try {\n        await process({\n          threadId,\n          messageId,\n          sendMessage,\n          sendDataMessage,\n          forwardStream,\n        });\n      } catch (error) {\n        sendError((error as any).message ?? `${error}`);\n      } finally {\n        controller.close();\n      }\n    },\n    pull(controller) {},\n    cancel() {},\n  });\n\n  return new Response(stream, {\n    status: 200,\n    headers: {\n      'Content-Type': 'text/plain; charset=utf-8',\n    },\n  });\n}\n\n/**\n@deprecated Use `AssistantResponse` instead.\n */\nexport const experimental_AssistantResponse = AssistantResponse;\n","import {\n  AIStreamCallbacksAndOptions,\n  createCallbacksTransformer,\n  readableFromAsyncIterable,\n} from './ai-stream';\nimport { createStreamDataTransformer } from './stream-data';\n\ninterface AWSBedrockResponse {\n  body?: AsyncIterable<{\n    chunk?: { bytes?: Uint8Array };\n  }>;\n}\n\nasync function* asDeltaIterable(\n  response: AWSBedrockResponse,\n  extractTextDeltaFromChunk: (chunk: any) => string,\n) {\n  const decoder = new TextDecoder();\n  for await (const chunk of response.body ?? []) {\n    const bytes = chunk.chunk?.bytes;\n\n    if (bytes != null) {\n      const chunkText = decoder.decode(bytes);\n      const chunkJSON = JSON.parse(chunkText);\n      const delta = extractTextDeltaFromChunk(chunkJSON);\n\n      if (delta != null) {\n        yield delta;\n      }\n    }\n  }\n}\n\nexport function AWSBedrockAnthropicMessagesStream(\n  response: AWSBedrockResponse,\n  callbacks?: AIStreamCallbacksAndOptions,\n): ReadableStream {\n  return AWSBedrockStream(response, callbacks, chunk => chunk.delta?.text);\n}\n\nexport function AWSBedrockAnthropicStream(\n  response: AWSBedrockResponse,\n  callbacks?: AIStreamCallbacksAndOptions,\n): ReadableStream {\n  return AWSBedrockStream(response, callbacks, chunk => chunk.completion);\n}\n\nexport function AWSBedrockCohereStream(\n  response: AWSBedrockResponse,\n  callbacks?: AIStreamCallbacksAndOptions,\n): ReadableStream {\n  return AWSBedrockStream(response, callbacks, chunk => chunk?.text);\n}\n\nexport function AWSBedrockLlama2Stream(\n  response: AWSBedrockResponse,\n  callbacks?: AIStreamCallbacksAndOptions,\n): ReadableStream {\n  return AWSBedrockStream(response, callbacks, chunk => chunk.generation);\n}\n\nexport function AWSBedrockStream(\n  response: AWSBedrockResponse,\n  callbacks: AIStreamCallbacksAndOptions | undefined,\n  extractTextDeltaFromChunk: (chunk: any) => string,\n) {\n  return readableFromAsyncIterable(\n    asDeltaIterable(response, extractTextDeltaFromChunk),\n  )\n    .pipeThrough(createCallbacksTransformer(callbacks))\n    .pipeThrough(createStreamDataTransformer());\n}\n","import {\n  type AIStreamCallbacksAndOptions,\n  createCallbacksTransformer,\n  readableFromAsyncIterable,\n} from './ai-stream';\nimport { createStreamDataTransformer } from './stream-data';\n\nconst utf8Decoder = new TextDecoder('utf-8');\n\n// Full types\n// @see: https://github.com/cohere-ai/cohere-typescript/blob/c2eceb4a845098240ba0bc44e3787ccf75e268e8/src/api/types/StreamedChatResponse.ts\ninterface StreamChunk {\n  text?: string;\n  eventType:\n    | 'stream-start'\n    | 'search-queries-generation'\n    | 'search-results'\n    | 'text-generation'\n    | 'citation-generation'\n    | 'stream-end';\n}\n\nasync function processLines(\n  lines: string[],\n  controller: ReadableStreamDefaultController<string>,\n) {\n  for (const line of lines) {\n    const { text, is_finished } = JSON.parse(line);\n\n    // closing the reader is handed in readAndProcessLines\n    if (!is_finished) {\n      controller.enqueue(text);\n    }\n  }\n}\n\nasync function readAndProcessLines(\n  reader: ReadableStreamDefaultReader<Uint8Array>,\n  controller: ReadableStreamDefaultController<string>,\n) {\n  let segment = '';\n\n  while (true) {\n    const { value: chunk, done } = await reader.read();\n    if (done) {\n      break;\n    }\n\n    segment += utf8Decoder.decode(chunk, { stream: true });\n\n    const linesArray = segment.split(/\\r\\n|\\n|\\r/g);\n    segment = linesArray.pop() || '';\n\n    await processLines(linesArray, controller);\n  }\n\n  if (segment) {\n    const linesArray = [segment];\n    await processLines(linesArray, controller);\n  }\n\n  controller.close();\n}\n\nfunction createParser(res: Response) {\n  const reader = res.body?.getReader();\n\n  return new ReadableStream<string>({\n    async start(controller): Promise<void> {\n      if (!reader) {\n        controller.close();\n        return;\n      }\n\n      await readAndProcessLines(reader, controller);\n    },\n  });\n}\n\nasync function* streamable(stream: AsyncIterable<StreamChunk>) {\n  for await (const chunk of stream) {\n    if (chunk.eventType === 'text-generation') {\n      const text = chunk.text;\n      if (text) yield text;\n    }\n  }\n}\n\nexport function CohereStream(\n  reader: Response | AsyncIterable<StreamChunk>,\n  callbacks?: AIStreamCallbacksAndOptions,\n): ReadableStream {\n  if (Symbol.asyncIterator in reader) {\n    return readableFromAsyncIterable(streamable(reader))\n      .pipeThrough(createCallbacksTransformer(callbacks))\n      .pipeThrough(createStreamDataTransformer());\n  } else {\n    return createParser(reader)\n      .pipeThrough(createCallbacksTransformer(callbacks))\n      .pipeThrough(createStreamDataTransformer());\n  }\n}\n","import {\n  createCallbacksTransformer,\n  readableFromAsyncIterable,\n  type AIStreamCallbacksAndOptions,\n} from './ai-stream';\nimport { createStreamDataTransformer } from './stream-data';\n\ninterface GenerateContentResponse {\n  candidates?: GenerateContentCandidate[];\n}\n\ninterface GenerateContentCandidate {\n  index: number;\n  content: Content;\n}\n\ninterface Content {\n  role: string;\n  parts: Part[];\n}\n\ntype Part = TextPart | InlineDataPart;\n\ninterface InlineDataPart {\n  text?: never;\n}\n\ninterface TextPart {\n  text: string;\n  inlineData?: never;\n}\n\nasync function* streamable(response: {\n  stream: AsyncIterable<GenerateContentResponse>;\n}) {\n  for await (const chunk of response.stream) {\n    const parts = chunk.candidates?.[0]?.content?.parts;\n\n    if (parts === undefined) {\n      continue;\n    }\n\n    const firstPart = parts[0];\n\n    if (typeof firstPart.text === 'string') {\n      yield firstPart.text;\n    }\n  }\n}\n\n/**\n * @deprecated Use the [Google Generative AI provider](https://sdk.vercel.ai/providers/ai-sdk-providers/google-generative-ai) instead.\n */\nexport function GoogleGenerativeAIStream(\n  response: {\n    stream: AsyncIterable<GenerateContentResponse>;\n  },\n  cb?: AIStreamCallbacksAndOptions,\n): ReadableStream {\n  return readableFromAsyncIterable(streamable(response))\n    .pipeThrough(createCallbacksTransformer(cb))\n    .pipeThrough(createStreamDataTransformer());\n}\n","import {\n  type AIStreamCallbacksAndOptions,\n  createCallbacksTransformer,\n  trimStartOfStreamHelper,\n} from './ai-stream';\nimport { createStreamDataTransformer } from './stream-data';\n\nfunction createParser(res: AsyncGenerator<any>) {\n  const trimStartOfStream = trimStartOfStreamHelper();\n  return new ReadableStream<string>({\n    async pull(controller): Promise<void> {\n      const { value, done } = await res.next();\n\n      if (done) {\n        controller.close();\n        return;\n      }\n\n      const text = trimStartOfStream(value.token?.text ?? '');\n      if (!text) return;\n\n      // some HF models return generated_text instead of a real ending token\n      if (value.generated_text != null && value.generated_text.length > 0) {\n        return;\n      }\n\n      // <|endoftext|> is for https://huggingface.co/OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5\n      // <|end|> is for https://huggingface.co/HuggingFaceH4/starchat-beta\n      // </s> is also often last token in the stream depending on the model\n      if (text === '</s>' || text === '<|endoftext|>' || text === '<|end|>') {\n        return;\n      }\n\n      controller.enqueue(text);\n    },\n  });\n}\n\nexport function HuggingFaceStream(\n  res: AsyncGenerator<any>,\n  callbacks?: AIStreamCallbacksAndOptions,\n): ReadableStream {\n  return createParser(res)\n    .pipeThrough(createCallbacksTransformer(callbacks))\n    .pipeThrough(createStreamDataTransformer());\n}\n","// packages/ai/streams/inkeep-stream.ts\nimport {\n  AIStream,\n  type AIStreamCallbacksAndOptions,\n  AIStreamParser,\n} from './ai-stream';\nimport { createStreamDataTransformer } from './stream-data';\n\nexport type InkeepOnFinalMetadata = {\n  chat_session_id: string;\n  records_cited: any;\n};\n\nexport type InkeepChatResultCallbacks = {\n  onFinal?: (\n    completion: string,\n    metadata?: InkeepOnFinalMetadata,\n  ) => Promise<void> | void;\n  onRecordsCited?: (\n    records_cited: InkeepOnFinalMetadata['records_cited'],\n  ) => void;\n};\n\nexport type InkeepAIStreamCallbacksAndOptions = AIStreamCallbacksAndOptions &\n  InkeepChatResultCallbacks;\n\nexport function InkeepStream(\n  res: Response,\n  callbacks?: InkeepAIStreamCallbacksAndOptions,\n): ReadableStream {\n  if (!res.body) {\n    throw new Error('Response body is null');\n  }\n\n  let chat_session_id = '';\n  let records_cited: any;\n\n  const inkeepEventParser: AIStreamParser = (data: string, options) => {\n    const { event } = options;\n\n    if (event === 'records_cited') {\n      records_cited = JSON.parse(data) as any;\n      callbacks?.onRecordsCited?.(records_cited);\n    }\n\n    if (event === 'message_chunk') {\n      const inkeepMessageChunk = JSON.parse(data);\n      chat_session_id = inkeepMessageChunk.chat_session_id ?? chat_session_id;\n      return inkeepMessageChunk.content_chunk;\n    }\n    return;\n  };\n\n  let { onRecordsCited, ...passThroughCallbacks } = callbacks || {};\n\n  // extend onFinal callback with Inkeep specific metadata\n  passThroughCallbacks = {\n    ...passThroughCallbacks,\n    onFinal: completion => {\n      const inkeepOnFinalMetadata: InkeepOnFinalMetadata = {\n        chat_session_id,\n        records_cited,\n      };\n      callbacks?.onFinal?.(completion, inkeepOnFinalMetadata);\n    },\n  };\n\n  return AIStream(res, inkeepEventParser, passThroughCallbacks).pipeThrough(\n    createStreamDataTransformer(),\n  );\n}\n","import { mergeStreams } from '../core/util/merge-streams';\nimport { prepareResponseHeaders } from '../core/util/prepare-response-headers';\nimport {\n  AIStreamCallbacksAndOptions,\n  createCallbacksTransformer,\n} from './ai-stream';\nimport { createStreamDataTransformer, StreamData } from './stream-data';\n\ntype LangChainImageDetail = 'auto' | 'low' | 'high';\n\ntype LangChainMessageContentText = {\n  type: 'text';\n  text: string;\n};\n\ntype LangChainMessageContentImageUrl = {\n  type: 'image_url';\n  image_url:\n    | string\n    | {\n        url: string;\n        detail?: LangChainImageDetail;\n      };\n};\n\ntype LangChainMessageContentComplex =\n  | LangChainMessageContentText\n  | LangChainMessageContentImageUrl\n  | (Record<string, any> & {\n      type?: 'text' | 'image_url' | string;\n    })\n  | (Record<string, any> & {\n      type?: never;\n    });\n\ntype LangChainMessageContent = string | LangChainMessageContentComplex[];\n\ntype LangChainAIMessageChunk = {\n  content: LangChainMessageContent;\n};\n\n// LC stream event v2\ntype LangChainStreamEvent = {\n  event: string;\n  data: any;\n};\n\n/**\nConverts LangChain output streams to AIStream.\n\nThe following streams are supported:\n- `LangChainAIMessageChunk` streams (LangChain `model.stream` output)\n- `string` streams (LangChain `StringOutputParser` output)\n\n@deprecated Use `toDataStream` instead.\n */\nexport function toAIStream(\n  stream:\n    | ReadableStream<LangChainStreamEvent>\n    | ReadableStream<LangChainAIMessageChunk>\n    | ReadableStream<string>,\n  callbacks?: AIStreamCallbacksAndOptions,\n) {\n  return toDataStream(stream, callbacks);\n}\n\n/**\nConverts LangChain output streams to AIStream.\n\nThe following streams are supported:\n- `LangChainAIMessageChunk` streams (LangChain `model.stream` output)\n- `string` streams (LangChain `StringOutputParser` output)\n */\nexport function toDataStream(\n  stream:\n    | ReadableStream<LangChainStreamEvent>\n    | ReadableStream<LangChainAIMessageChunk>\n    | ReadableStream<string>,\n  callbacks?: AIStreamCallbacksAndOptions,\n) {\n  return stream\n    .pipeThrough(\n      new TransformStream<\n        LangChainStreamEvent | LangChainAIMessageChunk | string\n      >({\n        transform: async (value, controller) => {\n          // text stream:\n          if (typeof value === 'string') {\n            controller.enqueue(value);\n            return;\n          }\n\n          // LC stream events v2:\n          if ('event' in value) {\n            // chunk is AIMessage Chunk for on_chat_model_stream event:\n            if (value.event === 'on_chat_model_stream') {\n              forwardAIMessageChunk(\n                value.data?.chunk as LangChainAIMessageChunk,\n                controller,\n              );\n            }\n            return;\n          }\n\n          // AI Message chunk stream:\n          forwardAIMessageChunk(value, controller);\n        },\n      }),\n    )\n    .pipeThrough(createCallbacksTransformer(callbacks))\n    .pipeThrough(createStreamDataTransformer());\n}\n\nexport function toDataStreamResponse(\n  stream:\n    | ReadableStream<LangChainStreamEvent>\n    | ReadableStream<LangChainAIMessageChunk>\n    | ReadableStream<string>,\n  options?: {\n    init?: ResponseInit;\n    data?: StreamData;\n    callbacks?: AIStreamCallbacksAndOptions;\n  },\n) {\n  const dataStream = toDataStream(stream, options?.callbacks);\n  const data = options?.data;\n  const init = options?.init;\n\n  const responseStream = data\n    ? mergeStreams(data.stream, dataStream)\n    : dataStream;\n\n  return new Response(responseStream, {\n    status: init?.status ?? 200,\n    statusText: init?.statusText,\n    headers: prepareResponseHeaders(init, {\n      contentType: 'text/plain; charset=utf-8',\n      dataStreamVersion: 'v1',\n    }),\n  });\n}\n\nfunction forwardAIMessageChunk(\n  chunk: LangChainAIMessageChunk,\n  controller: TransformStreamDefaultController<any>,\n) {\n  if (typeof chunk.content === 'string') {\n    controller.enqueue(chunk.content);\n  } else {\n    const content: LangChainMessageContentComplex[] = chunk.content;\n    for (const item of content) {\n      if (item.type === 'text') {\n        controller.enqueue(item.text);\n      }\n    }\n  }\n}\n","import { mergeStreams } from '../core/util/merge-streams';\nimport { prepareResponseHeaders } from '../core/util/prepare-response-headers';\nimport { createStreamDataTransformer, StreamData } from './stream-data';\nimport {\n  AIStreamCallbacksAndOptions,\n  createCallbacksTransformer,\n  trimStartOfStreamHelper,\n} from './ai-stream';\n\ntype EngineResponse = {\n  delta: string;\n};\n\nexport function toDataStream(\n  stream: AsyncIterable<EngineResponse>,\n  callbacks?: AIStreamCallbacksAndOptions,\n) {\n  return toReadableStream(stream)\n    .pipeThrough(createCallbacksTransformer(callbacks))\n    .pipeThrough(createStreamDataTransformer());\n}\n\nexport function toDataStreamResponse(\n  stream: AsyncIterable<EngineResponse>,\n  options: {\n    init?: ResponseInit;\n    data?: StreamData;\n    callbacks?: AIStreamCallbacksAndOptions;\n  } = {},\n) {\n  const { init, data, callbacks } = options;\n  const dataStream = toDataStream(stream, callbacks);\n  const responseStream = data\n    ? mergeStreams(data.stream, dataStream)\n    : dataStream;\n\n  return new Response(responseStream, {\n    status: init?.status ?? 200,\n    statusText: init?.statusText,\n    headers: prepareResponseHeaders(init, {\n      contentType: 'text/plain; charset=utf-8',\n      dataStreamVersion: 'v1',\n    }),\n  });\n}\n\nfunction toReadableStream(res: AsyncIterable<EngineResponse>) {\n  const it = res[Symbol.asyncIterator]();\n  const trimStartOfStream = trimStartOfStreamHelper();\n\n  return new ReadableStream<string>({\n    async pull(controller): Promise<void> {\n      const { value, done } = await it.next();\n      if (done) {\n        controller.close();\n        return;\n      }\n      const text = trimStartOfStream(value.delta ?? '');\n      if (text) {\n        controller.enqueue(text);\n      }\n    },\n  });\n}\n","import {\n  type AIStreamCallbacksAndOptions,\n  createCallbacksTransformer,\n} from './ai-stream';\nimport { createStreamDataTransformer } from './stream-data';\n\n/**\n * @deprecated Use [LangChainAdapter](https://sdk.vercel.ai/providers/adapters/langchain) instead.\n */\nexport function LangChainStream(callbacks?: AIStreamCallbacksAndOptions) {\n  const stream = new TransformStream();\n  const writer = stream.writable.getWriter();\n\n  const runs = new Set();\n\n  const handleError = async (e: Error, runId: string) => {\n    runs.delete(runId);\n    await writer.ready;\n    await writer.abort(e);\n  };\n\n  const handleStart = async (runId: string) => {\n    runs.add(runId);\n  };\n\n  const handleEnd = async (runId: string) => {\n    runs.delete(runId);\n\n    if (runs.size === 0) {\n      await writer.ready;\n      await writer.close();\n    }\n  };\n\n  return {\n    stream: stream.readable\n      .pipeThrough(createCallbacksTransformer(callbacks))\n      .pipeThrough(createStreamDataTransformer()),\n    writer,\n    handlers: {\n      handleLLMNewToken: async (token: string) => {\n        await writer.ready;\n        await writer.write(token);\n      },\n      handleLLMStart: async (_llm: any, _prompts: string[], runId: string) => {\n        handleStart(runId);\n      },\n      handleLLMEnd: async (_output: any, runId: string) => {\n        await handleEnd(runId);\n      },\n      handleLLMError: async (e: Error, runId: string) => {\n        await handleError(e, runId);\n      },\n      handleChainStart: async (_chain: any, _inputs: any, runId: string) => {\n        handleStart(runId);\n      },\n      handleChainEnd: async (_outputs: any, runId: string) => {\n        await handleEnd(runId);\n      },\n      handleChainError: async (e: Error, runId: string) => {\n        await handleError(e, runId);\n      },\n      handleToolStart: async (_tool: any, _input: string, runId: string) => {\n        handleStart(runId);\n      },\n      handleToolEnd: async (_output: string, runId: string) => {\n        await handleEnd(runId);\n      },\n      handleToolError: async (e: Error, runId: string) => {\n        await handleError(e, runId);\n      },\n    },\n  };\n}\n","import {\n  createCallbacksTransformer,\n  readableFromAsyncIterable,\n  type AIStreamCallbacksAndOptions,\n} from './ai-stream';\nimport { createStreamDataTransformer } from './stream-data';\n\ninterface ChatCompletionResponseChunk {\n  id: string;\n  object: 'chat.completion.chunk';\n  created: number;\n  model: string;\n  choices: ChatCompletionResponseChunkChoice[];\n}\n\ninterface ChatCompletionResponseChunkChoice {\n  index: number;\n  delta: {\n    role?: string;\n    content?: string;\n    tool_calls?: ToolCalls[];\n  };\n  finish_reason: string;\n}\n\ninterface FunctionCall {\n  name: string;\n  arguments: string;\n}\n\ninterface ToolCalls {\n  id: 'null';\n  type: 'function';\n  function: FunctionCall;\n}\n\nasync function* streamable(stream: AsyncIterable<ChatCompletionResponseChunk>) {\n  for await (const chunk of stream) {\n    const content = chunk.choices[0]?.delta?.content;\n\n    if (content === undefined || content === '') {\n      continue;\n    }\n\n    yield content;\n  }\n}\n\n/*\n * @deprecated Use the [Mistral provider](https://sdk.vercel.ai/providers/ai-sdk-providers/mistral) instead.\n */\nexport function MistralStream(\n  response: AsyncGenerator<ChatCompletionResponseChunk, void, unknown>,\n  callbacks?: AIStreamCallbacksAndOptions,\n): ReadableStream {\n  const stream = readableFromAsyncIterable(streamable(response));\n  return stream\n    .pipeThrough(createCallbacksTransformer(callbacks))\n    .pipeThrough(createStreamDataTransformer());\n}\n","import {\n  CreateMessage,\n  FunctionCall,\n  JSONValue,\n  ToolCall,\n  createChunkDecoder,\n  formatStreamPart,\n} from '@ai-sdk/ui-utils';\nimport {\n  AIStream,\n  FunctionCallPayload,\n  ToolCallPayload,\n  createCallbacksTransformer,\n  readableFromAsyncIterable,\n  trimStartOfStreamHelper,\n  type AIStreamCallbacksAndOptions,\n} from './ai-stream';\nimport { AzureChatCompletions } from './azure-openai-types';\nimport { createStreamDataTransformer } from './stream-data';\n\nexport type OpenAIStreamCallbacks = AIStreamCallbacksAndOptions & {\n  /**\n   * @example\n   * ```js\n   * const response = await openai.chat.completions.create({\n   *   model: 'gpt-3.5-turbo-0613',\n   *   stream: true,\n   *   messages,\n   *   functions,\n   * })\n   *\n   * const stream = OpenAIStream(response, {\n   *   experimental_onFunctionCall: async (functionCallPayload, createFunctionCallMessages) => {\n   *     // ... run your custom logic here\n   *     const result = await myFunction(functionCallPayload)\n   *\n   *     // Ask for another completion, or return a string to send to the client as an assistant message.\n   *     return await openai.chat.completions.create({\n   *       model: 'gpt-3.5-turbo-0613',\n   *       stream: true,\n   *       // Append the relevant \"assistant\" and \"function\" call messages\n   *       messages: [...messages, ...createFunctionCallMessages(result)],\n   *       functions,\n   *     })\n   *   }\n   * })\n   * ```\n   */\n  experimental_onFunctionCall?: (\n    functionCallPayload: FunctionCallPayload,\n    createFunctionCallMessages: (\n      functionCallResult: JSONValue,\n    ) => CreateMessage[],\n  ) => Promise<\n    Response | undefined | void | string | AsyncIterableOpenAIStreamReturnTypes\n  >;\n  /**\n   * @example\n   * ```js\n   * const response = await openai.chat.completions.create({\n   *   model: 'gpt-3.5-turbo-1106', // or gpt-4-1106-preview\n   *   stream: true,\n   *   messages,\n   *   tools,\n   *   tool_choice: \"auto\", // auto is default, but we'll be explicit\n   * })\n   *\n   * const stream = OpenAIStream(response, {\n   *   experimental_onToolCall: async (toolCallPayload, appendToolCallMessages) => {\n   *    let messages: CreateMessage[] = []\n   *    //   There might be multiple tool calls, so we need to iterate through them\n   *    for (const tool of toolCallPayload.tools) {\n   *     // ... run your custom logic here\n   *     const result = await myFunction(tool.function)\n   *    // Append the relevant \"assistant\" and \"tool\" call messages\n   *     appendToolCallMessage({tool_call_id:tool.id, function_name:tool.function.name, tool_call_result:result})\n   *    }\n   *     // Ask for another completion, or return a string to send to the client as an assistant message.\n   *     return await openai.chat.completions.create({\n   *       model: 'gpt-3.5-turbo-1106', // or gpt-4-1106-preview\n   *       stream: true,\n   *       // Append the results messages, calling appendToolCallMessage without\n   *       // any arguments will jsut return the accumulated messages\n   *       messages: [...messages, ...appendToolCallMessage()],\n   *       tools,\n   *        tool_choice: \"auto\", // auto is default, but we'll be explicit\n   *     })\n   *   }\n   * })\n   * ```\n   */\n  experimental_onToolCall?: (\n    toolCallPayload: ToolCallPayload,\n    appendToolCallMessage: (result?: {\n      tool_call_id: string;\n      function_name: string;\n      tool_call_result: JSONValue;\n    }) => CreateMessage[],\n  ) => Promise<\n    Response | undefined | void | string | AsyncIterableOpenAIStreamReturnTypes\n  >;\n};\n\n// https://github.com/openai/openai-node/blob/07b3504e1c40fd929f4aae1651b83afc19e3baf8/src/resources/chat/completions.ts#L28-L40\ninterface ChatCompletionChunk {\n  id: string;\n  choices: Array<ChatCompletionChunkChoice>;\n  created: number;\n  model: string;\n  object: string;\n}\n\n// https://github.com/openai/openai-node/blob/07b3504e1c40fd929f4aae1651b83afc19e3baf8/src/resources/chat/completions.ts#L43-L49\n// Updated for https://github.com/openai/openai-node/commit/f10c757d831d90407ba47b4659d9cd34b1a35b1d\n// Updated to https://github.com/openai/openai-node/commit/84b43280089eacdf18f171723591856811beddce\ninterface ChatCompletionChunkChoice {\n  delta: ChoiceDelta;\n  finish_reason:\n    | 'stop'\n    | 'length'\n    | 'tool_calls'\n    | 'content_filter'\n    | 'function_call'\n    | null;\n  index: number;\n}\n\n// https://github.com/openai/openai-node/blob/07b3504e1c40fd929f4aae1651b83afc19e3baf8/src/resources/chat/completions.ts#L123-L139\n// Updated to https://github.com/openai/openai-node/commit/84b43280089eacdf18f171723591856811beddce\ninterface ChoiceDelta {\n  /**\n   * The contents of the chunk message.\n   */\n  content?: string | null;\n\n  /**\n   * The name and arguments of a function that should be called, as generated by the\n   * model.\n   */\n  function_call?: FunctionCall;\n\n  /**\n   * The role of the author of this message.\n   */\n  role?: 'system' | 'user' | 'assistant' | 'tool';\n\n  tool_calls?: Array<DeltaToolCall>;\n}\n\n// From https://github.com/openai/openai-node/blob/master/src/resources/chat/completions.ts\n// Updated to https://github.com/openai/openai-node/commit/84b43280089eacdf18f171723591856811beddce\ninterface DeltaToolCall {\n  index: number;\n\n  /**\n   * The ID of the tool call.\n   */\n  id?: string;\n\n  /**\n   * The function that the model called.\n   */\n  function?: ToolCallFunction;\n\n  /**\n   * The type of the tool. Currently, only `function` is supported.\n   */\n  type?: 'function';\n}\n\n// From https://github.com/openai/openai-node/blob/master/src/resources/chat/completions.ts\n// Updated to https://github.com/openai/openai-node/commit/84b43280089eacdf18f171723591856811beddce\ninterface ToolCallFunction {\n  /**\n   * The arguments to call the function with, as generated by the model in JSON\n   * format. Note that the model does not always generate valid JSON, and may\n   * hallucinate parameters not defined by your function schema. Validate the\n   * arguments in your code before calling your function.\n   */\n  arguments?: string;\n\n  /**\n   * The name of the function to call.\n   */\n  name?: string;\n}\n\n/**\n * https://github.com/openai/openai-node/blob/3ec43ee790a2eb6a0ccdd5f25faa23251b0f9b8e/src/resources/completions.ts#L28C1-L64C1\n * Completions API. Streamed and non-streamed responses are the same.\n */\ninterface Completion {\n  /**\n   * A unique identifier for the completion.\n   */\n  id: string;\n\n  /**\n   * The list of completion choices the model generated for the input prompt.\n   */\n  choices: Array<CompletionChoice>;\n\n  /**\n   * The Unix timestamp of when the completion was created.\n   */\n  created: number;\n\n  /**\n   * The model used for completion.\n   */\n  model: string;\n\n  /**\n   * The object type, which is always \"text_completion\"\n   */\n  object: string;\n\n  /**\n   * Usage statistics for the completion request.\n   */\n  usage?: CompletionUsage;\n}\n\ninterface CompletionChoice {\n  /**\n   * The reason the model stopped generating tokens. This will be `stop` if the model\n   * hit a natural stop point or a provided stop sequence, or `length` if the maximum\n   * number of tokens specified in the request was reached.\n   */\n  finish_reason: 'stop' | 'length' | 'content_filter';\n\n  index: number;\n\n  // edited: Removed CompletionChoice.logProbs and replaced with any\n  logprobs: any | null;\n\n  text: string;\n}\n\nexport interface CompletionUsage {\n  /**\n   * Usage statistics for the completion request.\n   */\n\n  /**\n   * Number of tokens in the generated completion.\n   */\n  completion_tokens: number;\n\n  /**\n   * Number of tokens in the prompt.\n   */\n  prompt_tokens: number;\n\n  /**\n   * Total number of tokens used in the request (prompt + completion).\n   */\n  total_tokens: number;\n}\n\n/**\n * Creates a parser function for processing the OpenAI stream data.\n * The parser extracts and trims text content from the JSON data. This parser\n * can handle data for chat or completion models.\n *\n * @return {(data: string) => string | void| { isText: false; content: string }}\n * A parser function that takes a JSON string as input and returns the extracted text content,\n * a complex object with isText: false for function/tool calls, or nothing.\n */\nfunction parseOpenAIStream(): (\n  data: string,\n) => string | void | { isText: false; content: string } {\n  const extract = chunkToText();\n  return data => extract(JSON.parse(data) as OpenAIStreamReturnTypes);\n}\n\n/**\n * Reads chunks from OpenAI's new Streamable interface, which is essentially\n * the same as the old Response body interface with an included SSE parser\n * doing the parsing for us.\n */\nasync function* streamable(stream: AsyncIterableOpenAIStreamReturnTypes) {\n  const extract = chunkToText();\n\n  for await (let chunk of stream) {\n    // convert chunk if it is an Azure chat completion. Azure does not expose all\n    // properties in the interfaces, and also uses camelCase instead of snake_case\n    if ('promptFilterResults' in chunk) {\n      chunk = {\n        id: chunk.id,\n        created: chunk.created.getDate(),\n        object: (chunk as any).object, // not exposed by Azure API\n        model: (chunk as any).model, // not exposed by Azure API\n        choices: chunk.choices.map(choice => ({\n          delta: {\n            content: choice.delta?.content,\n            function_call: choice.delta?.functionCall,\n            role: choice.delta?.role as any,\n            tool_calls: choice.delta?.toolCalls?.length\n              ? choice.delta?.toolCalls?.map((toolCall, index) => ({\n                  index,\n                  id: toolCall.id,\n                  function: toolCall.function,\n                  type: toolCall.type,\n                }))\n              : undefined,\n          },\n          finish_reason: choice.finishReason as any,\n          index: choice.index,\n        })),\n      } satisfies ChatCompletionChunk;\n    }\n\n    const text = extract(chunk);\n\n    if (text) yield text;\n  }\n}\n\nfunction chunkToText(): (\n  chunk: OpenAIStreamReturnTypes,\n) => string | { isText: false; content: string } | void {\n  const trimStartOfStream = trimStartOfStreamHelper();\n  let isFunctionStreamingIn: boolean;\n  return json => {\n    if (isChatCompletionChunk(json)) {\n      const delta = json.choices[0]?.delta;\n      if (delta.function_call?.name) {\n        isFunctionStreamingIn = true;\n        return {\n          isText: false,\n          content: `{\"function_call\": {\"name\": \"${delta.function_call.name}\", \"arguments\": \"`,\n        };\n      } else if (delta.tool_calls?.[0]?.function?.name) {\n        isFunctionStreamingIn = true;\n        const toolCall = delta.tool_calls[0];\n        if (toolCall.index === 0) {\n          return {\n            isText: false,\n            content: `{\"tool_calls\":[ {\"id\": \"${toolCall.id}\", \"type\": \"function\", \"function\": {\"name\": \"${toolCall.function?.name}\", \"arguments\": \"`,\n          };\n        } else {\n          return {\n            isText: false,\n            content: `\"}}, {\"id\": \"${toolCall.id}\", \"type\": \"function\", \"function\": {\"name\": \"${toolCall.function?.name}\", \"arguments\": \"`,\n          };\n        }\n      } else if (delta.function_call?.arguments) {\n        return {\n          isText: false,\n          content: cleanupArguments(delta.function_call?.arguments),\n        };\n      } else if (delta.tool_calls?.[0]?.function?.arguments) {\n        return {\n          isText: false,\n          content: cleanupArguments(delta.tool_calls?.[0]?.function?.arguments),\n        };\n      } else if (\n        isFunctionStreamingIn &&\n        (json.choices[0]?.finish_reason === 'function_call' ||\n          json.choices[0]?.finish_reason === 'stop')\n      ) {\n        isFunctionStreamingIn = false; // Reset the flag\n        return {\n          isText: false,\n          content: '\"}}',\n        };\n      } else if (\n        isFunctionStreamingIn &&\n        json.choices[0]?.finish_reason === 'tool_calls'\n      ) {\n        isFunctionStreamingIn = false; // Reset the flag\n        return {\n          isText: false,\n          content: '\"}}]}',\n        };\n      }\n    }\n\n    const text = trimStartOfStream(\n      isChatCompletionChunk(json) && json.choices[0].delta.content\n        ? json.choices[0].delta.content\n        : isCompletion(json)\n        ? json.choices[0].text\n        : '',\n    );\n\n    return text;\n  };\n\n  function cleanupArguments(argumentChunk: string) {\n    let escapedPartialJson = argumentChunk\n      .replace(/\\\\/g, '\\\\\\\\') // Replace backslashes first to prevent double escaping\n      .replace(/\\//g, '\\\\/') // Escape slashes\n      .replace(/\"/g, '\\\\\"') // Escape double quotes\n      .replace(/\\n/g, '\\\\n') // Escape new lines\n      .replace(/\\r/g, '\\\\r') // Escape carriage returns\n      .replace(/\\t/g, '\\\\t') // Escape tabs\n      .replace(/\\f/g, '\\\\f'); // Escape form feeds\n\n    return `${escapedPartialJson}`;\n  }\n}\n\nconst __internal__OpenAIFnMessagesSymbol = Symbol(\n  'internal_openai_fn_messages',\n);\n\ntype AsyncIterableOpenAIStreamReturnTypes =\n  | AsyncIterable<ChatCompletionChunk>\n  | AsyncIterable<Completion>\n  | AsyncIterable<AzureChatCompletions>;\n\ntype ExtractType<T> = T extends AsyncIterable<infer U> ? U : never;\n\ntype OpenAIStreamReturnTypes =\n  ExtractType<AsyncIterableOpenAIStreamReturnTypes>;\n\nfunction isChatCompletionChunk(\n  data: OpenAIStreamReturnTypes,\n): data is ChatCompletionChunk {\n  return (\n    'choices' in data &&\n    data.choices &&\n    data.choices[0] &&\n    'delta' in data.choices[0]\n  );\n}\n\nfunction isCompletion(data: OpenAIStreamReturnTypes): data is Completion {\n  return (\n    'choices' in data &&\n    data.choices &&\n    data.choices[0] &&\n    'text' in data.choices[0]\n  );\n}\n\n/**\n * @deprecated Use the [OpenAI provider](https://sdk.vercel.ai/providers/ai-sdk-providers/openai) instead.\n */\nexport function OpenAIStream(\n  res: Response | AsyncIterableOpenAIStreamReturnTypes,\n  callbacks?: OpenAIStreamCallbacks,\n): ReadableStream {\n  // Annotate the internal `messages` property for recursive function calls\n  const cb:\n    | undefined\n    | (OpenAIStreamCallbacks & {\n        [__internal__OpenAIFnMessagesSymbol]?: CreateMessage[];\n      }) = callbacks;\n\n  let stream: ReadableStream<Uint8Array>;\n  if (Symbol.asyncIterator in res) {\n    stream = readableFromAsyncIterable(streamable(res)).pipeThrough(\n      createCallbacksTransformer(\n        cb?.experimental_onFunctionCall || cb?.experimental_onToolCall\n          ? {\n              ...cb,\n              onFinal: undefined,\n            }\n          : {\n              ...cb,\n            },\n      ),\n    );\n  } else {\n    stream = AIStream(\n      res,\n      parseOpenAIStream(),\n      cb?.experimental_onFunctionCall || cb?.experimental_onToolCall\n        ? {\n            ...cb,\n            onFinal: undefined,\n          }\n        : {\n            ...cb,\n          },\n    );\n  }\n\n  if (cb && (cb.experimental_onFunctionCall || cb.experimental_onToolCall)) {\n    const functionCallTransformer = createFunctionCallTransformer(cb);\n    return stream.pipeThrough(functionCallTransformer);\n  } else {\n    return stream.pipeThrough(createStreamDataTransformer());\n  }\n}\n\nfunction createFunctionCallTransformer(\n  callbacks: OpenAIStreamCallbacks & {\n    [__internal__OpenAIFnMessagesSymbol]?: CreateMessage[];\n  },\n): TransformStream<Uint8Array, Uint8Array> {\n  const textEncoder = new TextEncoder();\n  let isFirstChunk = true;\n  let aggregatedResponse = '';\n  let aggregatedFinalCompletionResponse = '';\n  let isFunctionStreamingIn = false;\n\n  let functionCallMessages: CreateMessage[] =\n    callbacks[__internal__OpenAIFnMessagesSymbol] || [];\n\n  const decode = createChunkDecoder();\n\n  return new TransformStream({\n    async transform(chunk, controller): Promise<void> {\n      const message = decode(chunk);\n      aggregatedFinalCompletionResponse += message;\n\n      const shouldHandleAsFunction =\n        isFirstChunk &&\n        (message.startsWith('{\"function_call\":') ||\n          message.startsWith('{\"tool_calls\":'));\n\n      if (shouldHandleAsFunction) {\n        isFunctionStreamingIn = true;\n        aggregatedResponse += message;\n        isFirstChunk = false;\n        return;\n      }\n\n      // Stream as normal\n      if (!isFunctionStreamingIn) {\n        controller.enqueue(\n          textEncoder.encode(formatStreamPart('text', message)),\n        );\n        return;\n      } else {\n        aggregatedResponse += message;\n      }\n    },\n    async flush(controller): Promise<void> {\n      try {\n        if (\n          !isFirstChunk &&\n          isFunctionStreamingIn &&\n          (callbacks.experimental_onFunctionCall ||\n            callbacks.experimental_onToolCall)\n        ) {\n          isFunctionStreamingIn = false;\n          const payload = JSON.parse(aggregatedResponse);\n          // Append the function call message to the list\n          let newFunctionCallMessages: CreateMessage[] = [\n            ...functionCallMessages,\n          ];\n\n          let functionResponse:\n            | Response\n            | undefined\n            | void\n            | string\n            | AsyncIterableOpenAIStreamReturnTypes\n            | undefined = undefined;\n          // This callbacks.experimental_onFunctionCall check should not be necessary but TS complains\n          if (callbacks.experimental_onFunctionCall) {\n            // If the user is using the experimental_onFunctionCall callback, they should not be using tools\n            // if payload.function_call is not defined by time we get here we must have gotten a tool response\n            // and the user had defined experimental_onToolCall\n            if (payload.function_call === undefined) {\n              console.warn(\n                'experimental_onFunctionCall should not be defined when using tools',\n              );\n            }\n\n            const argumentsPayload = JSON.parse(\n              payload.function_call.arguments,\n            );\n\n            functionResponse = await callbacks.experimental_onFunctionCall(\n              {\n                name: payload.function_call.name,\n                arguments: argumentsPayload,\n              },\n              result => {\n                // Append the function call request and result messages to the list\n                newFunctionCallMessages = [\n                  ...functionCallMessages,\n                  {\n                    role: 'assistant',\n                    content: '',\n                    function_call: payload.function_call,\n                  },\n                  {\n                    role: 'function',\n                    name: payload.function_call.name,\n                    content: JSON.stringify(result),\n                  },\n                ];\n                // Return it to the user\n                return newFunctionCallMessages;\n              },\n            );\n          }\n          if (callbacks.experimental_onToolCall) {\n            const toolCalls: ToolCallPayload = {\n              tools: [],\n            };\n            for (const tool of payload.tool_calls) {\n              toolCalls.tools.push({\n                id: tool.id,\n                type: 'function',\n                func: {\n                  name: tool.function.name,\n                  arguments: JSON.parse(tool.function.arguments),\n                },\n              });\n            }\n            let responseIndex = 0;\n            try {\n              functionResponse = await callbacks.experimental_onToolCall(\n                toolCalls,\n                result => {\n                  if (result) {\n                    const { tool_call_id, function_name, tool_call_result } =\n                      result;\n                    // Append the function call request and result messages to the list\n                    newFunctionCallMessages = [\n                      ...newFunctionCallMessages,\n                      // Only append the assistant message if it's the first response\n                      ...(responseIndex === 0\n                        ? [\n                            {\n                              role: 'assistant' as const,\n                              content: '',\n                              tool_calls: payload.tool_calls.map(\n                                (tc: ToolCall) => ({\n                                  id: tc.id,\n                                  type: 'function',\n                                  function: {\n                                    name: tc.function.name,\n                                    // we send the arguments an object to the user, but as the API expects a string, we need to stringify it\n                                    arguments: JSON.stringify(\n                                      tc.function.arguments,\n                                    ),\n                                  },\n                                }),\n                              ),\n                            },\n                          ]\n                        : []),\n                      // Append the function call result message\n                      {\n                        role: 'tool',\n                        tool_call_id,\n                        name: function_name,\n                        content: JSON.stringify(tool_call_result),\n                      },\n                    ];\n                    responseIndex++;\n                  }\n                  // Return it to the user\n                  return newFunctionCallMessages;\n                },\n              );\n            } catch (e) {\n              console.error('Error calling experimental_onToolCall:', e);\n            }\n          }\n\n          if (!functionResponse) {\n            // The user didn't do anything with the function call on the server and wants\n            // to either do nothing or run it on the client\n            // so we just return the function call as a message\n            controller.enqueue(\n              textEncoder.encode(\n                formatStreamPart(\n                  payload.function_call ? 'function_call' : 'tool_calls',\n                  // parse to prevent double-encoding:\n                  JSON.parse(aggregatedResponse),\n                ),\n              ),\n            );\n            return;\n          } else if (typeof functionResponse === 'string') {\n            // The user returned a string, so we just return it as a message\n            controller.enqueue(\n              textEncoder.encode(formatStreamPart('text', functionResponse)),\n            );\n            aggregatedFinalCompletionResponse = functionResponse;\n            return;\n          }\n\n          // Recursively:\n\n          // We don't want to trigger onStart or onComplete recursively\n          // so we remove them from the callbacks\n          // see https://github.com/vercel/ai/issues/351\n          const filteredCallbacks: OpenAIStreamCallbacks = {\n            ...callbacks,\n            onStart: undefined,\n          };\n          // We only want onFinal to be called the _last_ time\n          callbacks.onFinal = undefined;\n\n          const openAIStream = OpenAIStream(functionResponse, {\n            ...filteredCallbacks,\n            [__internal__OpenAIFnMessagesSymbol]: newFunctionCallMessages,\n          } as AIStreamCallbacksAndOptions);\n\n          const reader = openAIStream.getReader();\n\n          while (true) {\n            const { done, value } = await reader.read();\n            if (done) {\n              break;\n            }\n            controller.enqueue(value);\n          }\n        }\n      } finally {\n        if (callbacks.onFinal && aggregatedFinalCompletionResponse) {\n          await callbacks.onFinal(aggregatedFinalCompletionResponse);\n        }\n      }\n    },\n  });\n}\n","import { AIStream, type AIStreamCallbacksAndOptions } from './ai-stream';\nimport { createStreamDataTransformer } from './stream-data';\n\n// from replicate SDK\ninterface Prediction {\n  id: string;\n  status: 'starting' | 'processing' | 'succeeded' | 'failed' | 'canceled';\n  version: string;\n  input: object;\n  output?: any;\n  source: 'api' | 'web';\n  error?: any;\n  logs?: string;\n  metrics?: {\n    predict_time?: number;\n  };\n  webhook?: string;\n  webhook_events_filter?: ('start' | 'output' | 'logs' | 'completed')[];\n  created_at: string;\n  updated_at?: string;\n  completed_at?: string;\n  urls: {\n    get: string;\n    cancel: string;\n    stream?: string;\n  };\n}\n\n/**\n * Stream predictions from Replicate.\n * Only certain models are supported and you must pass `stream: true` to\n * replicate.predictions.create().\n * @see https://github.com/replicate/replicate-javascript#streaming\n *\n * @example\n * const response = await replicate.predictions.create({\n *  stream: true,\n *  input: {\n *    prompt: messages.join('\\n')\n *  },\n *  version: '2c1608e18606fad2812020dc541930f2d0495ce32eee50074220b87300bc16e1'\n * })\n *\n * const stream = await ReplicateStream(response)\n * return new StreamingTextResponse(stream)\n *\n */\nexport async function ReplicateStream(\n  res: Prediction,\n  cb?: AIStreamCallbacksAndOptions,\n  options?: {\n    headers?: Record<string, string>;\n  },\n): Promise<ReadableStream> {\n  const url = res.urls?.stream;\n\n  if (!url) {\n    if (res.error) throw new Error(res.error);\n    else throw new Error('Missing stream URL in Replicate response');\n  }\n\n  const eventStream = await fetch(url, {\n    method: 'GET',\n    headers: {\n      Accept: 'text/event-stream',\n      ...options?.headers,\n    },\n  });\n\n  return AIStream(eventStream, undefined, cb).pipeThrough(\n    createStreamDataTransformer(),\n  );\n}\n","import type { ServerResponse } from 'node:http';\nimport { StreamData } from './stream-data';\nimport { mergeStreams } from '../core/util/merge-streams';\n\n/**\n * A utility function to stream a ReadableStream to a Node.js response-like object.\n *\n * @deprecated Use `pipeDataStreamToResponse` (part of `StreamTextResult`) instead.\n */\nexport function streamToResponse(\n  res: ReadableStream,\n  response: ServerResponse,\n  init?: { headers?: Record<string, string>; status?: number },\n  data?: StreamData,\n) {\n  response.writeHead(init?.status ?? 200, {\n    'Content-Type': 'text/plain; charset=utf-8',\n    ...init?.headers,\n  });\n\n  let processedStream = res;\n\n  if (data) {\n    processedStream = mergeStreams(data.stream, res);\n  }\n\n  const reader = processedStream.getReader();\n  function read() {\n    reader.read().then(({ done, value }: { done: boolean; value?: any }) => {\n      if (done) {\n        response.end();\n        return;\n      }\n      response.write(value);\n      read();\n    });\n  }\n  read();\n}\n","import { mergeStreams } from '../core/util/merge-streams';\nimport { prepareResponseHeaders } from '../core/util/prepare-response-headers';\nimport { StreamData } from './stream-data';\n\n/**\n * A utility class for streaming text responses.\n *\n * @deprecated Use `streamText.toDataStreamResponse()` (if you did send StreamData)\n * or a regular `Response` instead (if you did not send any StreamData):\n *\n * ```ts\n * return new Response(stream, {\n *   status: 200,\n *   contentType: 'text/plain; charset=utf-8',\n * })\n * ```\n */\nexport class StreamingTextResponse extends Response {\n  constructor(res: ReadableStream, init?: ResponseInit, data?: StreamData) {\n    let processedStream = res;\n\n    if (data) {\n      processedStream = mergeStreams(data.stream, res);\n    }\n\n    super(processedStream as any, {\n      ...init,\n      status: 200,\n      headers: prepareResponseHeaders(init, {\n        contentType: 'text/plain; charset=utf-8',\n      }),\n    });\n  }\n}\n"],"names":["_a","name","attributes","embedding","usage","embeddings","marker","symbol","AISDKError","getErrorMessage","convertUint8ArrayToBase64","z","safeValidateTypes","generateId","span","result","originalGenerateId","createIdGenerator","now","doStreamSpan","error","tool","asSchema","safeParseJSON","_b","_c","_d","_e","promptMessages","stream","warnings","rawResponse","startTimestampMs","startTimestamp","responseMessages","NoSuchModelError","StreamData","formatStreamPart","process","createParser","streamable","toDataStream","toDataStreamResponse"],"mappings":";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;AGAA,eAAsB,MAAM,SAAA;IAC1B,OAAO,cAAc,KAAA,IACjB,QAAQ,OAAA,KACR,IAAI,QAAQ,CAAA,UAAW,WAAW,SAAS;AACjD;;ACFA,IAAM,OAAO;AACb,IAAM,SAAS,CAAA,gBAAA,EAAmB,KAAI,CAAA;AACtC,IAAM,SAAS,OAAO,GAAA,CAAI;AAJ1B,IAAA;AAWO,IAAM,aAAN,cAAyB,iLAAA,CAAA,aAAA;IAQ9B,YAAY,EACV,OAAA,EACA,MAAA,EACA,MAAA,EACF,CAIG;QACD,KAAA,CAAM;YAAE;YAAM;QAAQ;QAhBxB,IAAA,CAAkB,GAAA,GAAU;QAkB1B,IAAA,CAAK,MAAA,GAAS;QACd,IAAA,CAAK,MAAA,GAAS;QAGd,IAAA,CAAK,SAAA,GAAY,MAAA,CAAO,OAAO,MAAA,GAAS,EAAC;IAC3C;IAEA,OAAO,WAAW,KAAA,EAAqC;QACrD,OAAO,iLAAA,CAAA,aAAA,CAAW,SAAA,CAAU,OAAO;IACrC;IAAA;;GAAA,GAKA,OAAO,aAAa,KAAA,EAAqC;QACvD,OACE,iBAAiB,SACjB,MAAM,IAAA,KAAS,QACf,OAAQ,MAAqB,MAAA,KAAW,YACxC,MAAM,OAAA,CAAS,MAAqB,MAAM;IAE9C;IAAA;;GAAA,GAKA,SAAS;QACP,OAAO;YACL,MAAM,IAAA,CAAK,IAAA;YACX,SAAS,IAAA,CAAK,OAAA;YACd,QAAQ,IAAA,CAAK,MAAA;YACb,WAAW,IAAA,CAAK,SAAA;YAChB,QAAQ,IAAA,CAAK,MAAA;QACf;IACF;AACF;AArDoB,KAAA;;AFCb,IAAM,8BACX,CAAC,EACC,aAAa,CAAA,EACb,mBAAmB,GAAA,EACnB,gBAAgB,CAAA,EAClB,GAAI,CAAC,CAAA,GACL,OAAe,IACb,6BAA6B,GAAG;YAC9B;YACA,WAAW;YACX;QACF;AAEJ,eAAe,6BACb,CAAA,EACA,EACE,UAAA,EACA,SAAA,EACA,aAAA,EACF,EACA,SAAoB,EAAC;IAErB,IAAI;QACF,OAAO,MAAM;IACf,EAAA,OAAS,OAAO;QACd,IAAI,CAAA,GAAA,0LAAA,CAAA,eAAA,EAAa,QAAQ;YACvB,MAAM;QACR;QAEA,IAAI,eAAe,GAAG;YACpB,MAAM;QACR;QAEA,MAAM,eAAe,CAAA,GAAA,0LAAA,CAAA,kBAAA,EAAgB;QACrC,MAAM,YAAY;eAAI;YAAQ;SAAK;QACnC,MAAM,YAAY,UAAU,MAAA;QAE5B,IAAI,YAAY,YAAY;YAC1B,MAAM,IAAI,WAAW;gBACnB,SAAS,CAAA,aAAA,EAAgB,UAAS,uBAAA,EAA0B,aAAY,CAAA;gBACxE,QAAQ;gBACR,QAAQ;YACV;QACF;QAEA,IACE,iBAAiB,SACjB,iLAAA,CAAA,eAAA,CAAa,cAAA,CAAe,UAC5B,MAAM,WAAA,KAAgB,QACtB,aAAa,YACb;YACA,MAAM,MAAM;YACZ,OAAO,6BACL,GACA;gBAAE;gBAAY,WAAW,gBAAgB;gBAAW;YAAc,GAClE;QAEJ;QAEA,IAAI,cAAc,GAAG;YACnB,MAAM;QACR;QAEA,MAAM,IAAI,WAAW;YACnB,SAAS,CAAA,aAAA,EAAgB,UAAS,qCAAA,EAAwC,aAAY,CAAA,CAAA;YACtF,QAAQ;YACR,QAAQ;QACV;IACF;AACF;;AGhFO,SAAS,sBAAsB,EACpC,WAAA,EACA,SAAA,EACF;IAIE,OAAO;QAAA,4CAAA;QAEL,kBAAkB,CAAA,EAAG,YAAW,EAAA,CAC9B,aAAA,OAAA,KAAA,IAAA,UAAW,UAAA,KAAc,OAAO,CAAA,CAAA,EAAI,UAAU,UAAU,CAAA,CAAA,GAAK,GAC/D,CAAA;QACA,iBAAiB,aAAA,OAAA,KAAA,IAAA,UAAW,UAAA;QAAA,kCAAA;QAG5B,kBAAkB;QAClB,2BAA2B,aAAA,OAAA,KAAA,IAAA,UAAW,UAAA;IACxC;AACF;;AChBO,SAAS,2BAA2B,EACzC,KAAA,EACA,QAAA,EACA,SAAA,EACA,OAAA,EACF;IATA,IAAAA;IAeE,OAAO;QACL,qBAAqB,MAAM,QAAA;QAC3B,eAAe,MAAM,OAAA;QAAA,YAAA;QAGrB,GAAG,OAAO,OAAA,CAAQ,UAAU,MAAA,CAAO,CAAC,YAAY,CAAC,KAAK,MAAK;YACzD,UAAA,CAAW,CAAA,YAAA,EAAe,IAAG,CAAE,CAAA,GAAI;YACnC,OAAO;QACT,GAAG,CAAC,EAAe;QAAA,8BAAA;QAGnB,GAAG,OAAO,OAAA,CAAA,CAAQA,OAAA,aAAA,OAAA,KAAA,IAAA,UAAW,QAAA,KAAX,OAAAA,OAAuB,CAAC,GAAG,MAAA,CAC3C,CAAC,YAAY,CAAC,KAAK,MAAK;YACtB,UAAA,CAAW,CAAA,sBAAA,EAAyB,IAAG,CAAE,CAAA,GAAI;YAC7C,OAAO;QACT,GACA,CAAC,EACH;QAAA,kBAAA;QAGA,GAAG,OAAO,OAAA,CAAQ,WAAA,OAAA,UAAW,CAAC,GAAG,MAAA,CAAO,CAAC,YAAY,CAAC,KAAK,MAAK;YAC9D,IAAI,UAAU,KAAA,GAAW;gBACvB,UAAA,CAAW,CAAA,mBAAA,EAAsB,IAAG,CAAE,CAAA,GAAI;YAC5C;YACA,OAAO;QACT,GAAG,CAAC,EAAe;IACrB;AACF;;;AErCO,IAAM,aAAqB;IAChC;QACE,OAAO;IACT;IAEA,iBACEC,MAAAA,EACA,IAAA,EACA,IAAA,EACA,IAAA;QAEA,IAAI,OAAO,SAAS,YAAY;YAC9B,OAAO,KAAK;QACd;QACA,IAAI,OAAO,SAAS,YAAY;YAC9B,OAAO,KAAK;QACd;QACA,IAAI,OAAO,SAAS,YAAY;YAC9B,OAAO,KAAK;QACd;IACF;AACF;AAEA,IAAM,WAAiB;IACrB;QACE,OAAO;IACT;IACA;QACE,OAAO,IAAA;IACT;IACA;QACE,OAAO,IAAA;IACT;IACA;QACE,OAAO,IAAA;IACT;IACA;QACE,OAAO,IAAA;IACT;IACA;QACE,OAAO,IAAA;IACT;IACA;QACE,OAAO,IAAA;IACT;IACA;QACE,OAAO,IAAA;IACT;IACA;QACE,OAAO,IAAA;IACT;IACA;QACE,OAAO;IACT;IACA;QACE,OAAO,IAAA;IACT;AACF;AAEA,IAAM,kBAA+B;IACnC,SAAS;IACT,QAAQ;IACR,YAAY;AACd;;AD9DA,IAAI,aAAiC,KAAA;AAM9B,SAAS,UAAU,EAAE,SAAA,EAAU;IACpC,IAAI,CAAC,WAAW;QACd,OAAO;IACT;IAEA,IAAI,YAAY;QACd,OAAO;IACT;IAEA,OAAO,2DAAA,CAAA,QAAA,CAAM,SAAA,CAAU;AACzB;;AEpBO,SAAS,WAAc,EAC5B,MAAAA,MAAAA,EACA,MAAA,EACA,UAAA,EACA,EAAA,EACA,cAAc,IAAA,EAChB;IAOE,OAAO,OAAO,eAAA,CAAgBA,QAAM;QAAE;IAAW,GAAG,OAAM;QACxD,IAAI;YACF,MAAM,SAAS,MAAM,GAAG;YAExB,IAAI,aAAa;gBACf,KAAK,GAAA;YACP;YAEA,OAAO;QACT,EAAA,OAAS,OAAO;YACd,IAAI;gBACF,IAAI,iBAAiB,OAAO;oBAC1B,KAAK,eAAA,CAAgB;wBACnB,MAAM,MAAM,IAAA;wBACZ,SAAS,MAAM,OAAA;wBACf,OAAO,MAAM,KAAA;oBACf;oBACA,KAAK,SAAA,CAAU;wBACb,MAAM,2DAAA,CAAA,iBAAA,CAAe,KAAA;wBACrB,SAAS,MAAM,OAAA;oBACjB;gBACF,OAAO;oBACL,KAAK,SAAA,CAAU;wBAAE,MAAM,2DAAA,CAAA,iBAAA,CAAe,KAAA;oBAAM;gBAC9C;YACF,SAAE;gBAEA,KAAK,GAAA;YACP;YAEA,MAAM;QACR;IACF;AACF;;AC5CO,SAAS,0BAA0B,EACxC,SAAA,EACA,UAAA,EACF;IAUE,OAAO,OAAO,OAAA,CAAQ,YAAY,MAAA,CAAO,CAACC,aAAY,CAAC,KAAK,MAAK;QAC/D,IAAI,UAAU,KAAA,GAAW;YACvB,OAAOA;QACT;QAGA,IACE,OAAO,UAAU,YACjB,WAAW,SACX,OAAO,MAAM,KAAA,KAAU,YACvB;YAEA,IAAA,CAAI,aAAA,OAAA,KAAA,IAAA,UAAW,YAAA,MAAiB,OAAO;gBACrC,OAAOA;YACT;YAEA,MAAM,SAAS,MAAM,KAAA;YAErB,OAAO,WAAW,KAAA,IACdA,cACA;gBAAE,GAAGA,WAAAA;gBAAY,CAAC,IAAG,EAAG;YAAO;QACrC;QAGA,IACE,OAAO,UAAU,YACjB,YAAY,SACZ,OAAO,MAAM,MAAA,KAAW,YACxB;YAEA,IAAA,CAAI,aAAA,OAAA,KAAA,IAAA,UAAW,aAAA,MAAkB,OAAO;gBACtC,OAAOA;YACT;YAEA,MAAM,SAAS,MAAM,MAAA;YAErB,OAAO,WAAW,KAAA,IACdA,cACA;gBAAE,GAAGA,WAAAA;gBAAY,CAAC,IAAG,EAAG;YAAO;QACrC;QAGA,OAAO;YAAE,GAAGA,WAAAA;YAAY,CAAC,IAAG,EAAG;QAAM;IACvC,GAAG,CAAC;AACN;;ACtCA,eAAsB,MAAa,EACjC,KAAA,EACA,KAAA,EACA,UAAA,EACA,WAAA,EACA,OAAA,EACA,wBAAwB,SAAA,EAC1B;IA7BA,IAAAF;IA+DE,MAAM,0BAA0B,2BAA2B;QACzD;QACA;QACA;QACA,UAAU;YAAE;QAAW;IACzB;IAEA,MAAM,SAAS,UAAU;QAAE,WAAA,CAAWA,OAAA,aAAA,OAAA,KAAA,IAAA,UAAW,SAAA,KAAX,OAAAA,OAAwB;IAAM;IAEpE,OAAO,WAAW;QAChB,MAAM;QACN,YAAY,0BAA0B;YACpC;YACA,YAAY;gBACV,GAAG,sBAAsB;oBAAE,aAAa;oBAAY;gBAAU,EAAC;gBAC/D,GAAG,uBAAA;gBACH,YAAY;oBAAE,OAAO,IAAM,KAAK,SAAA,CAAU;gBAAO;YACnD;QACF;QACA;QACA,IAAI,OAAM;YACR,MAAM,QAAQ,4BAA4B;gBAAE;YAAW;YAEvD,MAAM,EAAE,SAAA,EAAW,KAAA,EAAO,WAAA,EAAY,GAAI,MAAM,MAAM,IAAA,2DAAA;gBAEpD,WAAW;oBACT,MAAM;oBACN,YAAY,0BAA0B;wBACpC;wBACA,YAAY;4BACV,GAAG,sBAAsB;gCACvB,aAAa;gCACb;4BACF,EAAC;4BACD,GAAG,uBAAA;4BAAA,6DAAA;4BAEH,aAAa;gCAAE,OAAO,IAAM;wCAAC,KAAK,SAAA,CAAU;qCAAM;4BAAE;wBACtD;oBACF;oBACA;oBACA,IAAI,OAAM;wBAvGpB,IAAAA;wBAwGY,MAAM,gBAAgB,MAAM,MAAM,OAAA,CAAQ;4BACxC,QAAQ;gCAAC;6BAAK;4BACd;4BACA;wBACF;wBAEA,MAAMG,aAAY,cAAc,UAAA,CAAW,EAAC;wBAC5C,MAAMC,SAAAA,CAAQJ,OAAA,cAAc,KAAA,KAAd,OAAAA,OAAuB;4BAAE,QAAQ;wBAAI;wBAEnD,YAAY,aAAA,CACV,0BAA0B;4BACxB;4BACA,YAAY;gCACV,iBAAiB;oCACf,QAAQ,IACN,cAAc,UAAA,CAAW,GAAA,CAAI,CAAAG,aAC3B,KAAK,SAAA,CAAUA;gCAErB;gCACA,mBAAmBC,OAAM,MAAA;4BAC3B;wBACF;wBAGF,OAAO;4BACL,WAAAD;4BACA,OAAAC;4BACA,aAAa,cAAc,WAAA;wBAC7B;oBACF;gBACF;YAGF,KAAK,aAAA,CACH,0BAA0B;gBACxB;gBACA,YAAY;oBACV,gBAAgB;wBAAE,QAAQ,IAAM,KAAK,SAAA,CAAU;oBAAW;oBAC1D,mBAAmB,MAAM,MAAA;gBAC3B;YACF;YAGF,OAAO,IAAI,mBAAmB;gBAAE;gBAAO;gBAAW;gBAAO;YAAY;QACvE;IACF;AACF;AAEA,IAAM,qBAAN;IAME,YAAY,OAAA,CAKT;QACD,IAAA,CAAK,KAAA,GAAQ,QAAQ,KAAA;QACrB,IAAA,CAAK,SAAA,GAAY,QAAQ,SAAA;QACzB,IAAA,CAAK,KAAA,GAAQ,QAAQ,KAAA;QACrB,IAAA,CAAK,WAAA,GAAc,QAAQ,WAAA;IAC7B;AACF;;ACjKO,SAAS,WAAc,KAAA,EAAY,SAAA;IACxC,IAAI,aAAa,GAAG;QAClB,MAAM,IAAI,MAAM;IAClB;IAEA,MAAM,SAAS,EAAC;IAChB,IAAA,IAAS,IAAI,GAAG,IAAI,MAAM,MAAA,EAAQ,KAAK,UAAW;QAChD,OAAO,IAAA,CAAK,MAAM,KAAA,CAAM,GAAG,IAAI;IACjC;IAEA,OAAO;AACT;;ACQA,eAAsB,UAAiB,EACrC,KAAA,EACA,MAAA,EACA,UAAA,EACA,WAAA,EACA,OAAA,EACA,wBAAwB,SAAA,EAC1B;IAlCA,IAAAJ;IAoEE,MAAM,0BAA0B,2BAA2B;QACzD;QACA;QACA;QACA,UAAU;YAAE;QAAW;IACzB;IAEA,MAAM,SAAS,UAAU;QAAE,WAAA,CAAWA,OAAA,aAAA,OAAA,KAAA,IAAA,UAAW,SAAA,KAAX,OAAAA,OAAwB;IAAM;IAEpE,OAAO,WAAW;QAChB,MAAM;QACN,YAAY,0BAA0B;YACpC;YACA,YAAY;gBACV,GAAG,sBAAsB;oBAAE,aAAa;oBAAgB;gBAAU,EAAC;gBACnE,GAAG,uBAAA;gBAAA,6DAAA;gBAEH,aAAa;oBACX,OAAO,IAAM,OAAO,GAAA,CAAI,CAAA,QAAS,KAAK,SAAA,CAAU;gBAClD;YACF;QACF;QACA;QACA,IAAI,OAAM;YACR,MAAM,QAAQ,4BAA4B;gBAAE;YAAW;YACvD,MAAM,uBAAuB,MAAM,oBAAA;YAInC,IAAI,wBAAwB,MAAM;gBAChC,MAAM,EAAE,YAAAK,WAAAA,EAAY,KAAA,EAAM,GAAI,MAAM,MAAM;oBAExC,OAAO,WAAW;wBAChB,MAAM;wBACN,YAAY,0BAA0B;4BACpC;4BACA,YAAY;gCACV,GAAG,sBAAsB;oCACvB,aAAa;oCACb;gCACF,EAAC;gCACD,GAAG,uBAAA;gCAAA,6DAAA;gCAEH,aAAa;oCACX,OAAO,IAAM,OAAO,GAAA,CAAI,CAAA,QAAS,KAAK,SAAA,CAAU;gCAClD;4BACF;wBACF;wBACA;wBACA,IAAI,OAAM;4BArHtB,IAAAL;4BAsHc,MAAM,gBAAgB,MAAM,MAAM,OAAA,CAAQ;gCACxC;gCACA;gCACA;4BACF;4BAEA,MAAMK,cAAa,cAAc,UAAA;4BACjC,MAAMD,SAAAA,CAAQJ,OAAA,cAAc,KAAA,KAAd,OAAAA,OAAuB;gCAAE,QAAQ;4BAAI;4BAEnD,YAAY,aAAA,CACV,0BAA0B;gCACxB;gCACA,YAAY;oCACV,iBAAiB;wCACf,QAAQ,IACNK,YAAW,GAAA,CAAI,CAAA,YAAa,KAAK,SAAA,CAAU;oCAC/C;oCACA,mBAAmBD,OAAM,MAAA;gCAC3B;4BACF;4BAGF,OAAO;gCAAE,YAAAC;gCAAY,OAAAD;4BAAM;wBAC7B;oBACF;gBACF;gBAEA,KAAK,aAAA,CACH,0BAA0B;oBACxB;oBACA,YAAY;wBACV,iBAAiB;4BACf,QAAQ,IACNC,YAAW,GAAA,CAAI,CAAA,YAAa,KAAK,SAAA,CAAU;wBAC/C;wBACA,mBAAmB,MAAM,MAAA;oBAC3B;gBACF;gBAGF,OAAO,IAAI,uBAAuB;oBAAE;oBAAQ,YAAAA;oBAAY;gBAAM;YAChE;YAGA,MAAM,cAAc,WAAW,QAAQ;YAGvC,MAAM,aAA+B,EAAC;YACtC,IAAI,SAAS;YAEb,KAAA,MAAW,SAAS,YAAa;gBAC/B,MAAM,EAAE,YAAY,kBAAA,EAAoB,KAAA,EAAM,GAAI,MAAM,MAAM;oBAE5D,OAAO,WAAW;wBAChB,MAAM;wBACN,YAAY,0BAA0B;4BACpC;4BACA,YAAY;gCACV,GAAG,sBAAsB;oCACvB,aAAa;oCACb;gCACF,EAAC;gCACD,GAAG,uBAAA;gCAAA,6DAAA;gCAEH,aAAa;oCACX,OAAO,IAAM,MAAM,GAAA,CAAI,CAAA,QAAS,KAAK,SAAA,CAAU;gCACjD;4BACF;wBACF;wBACA;wBACA,IAAI,OAAM;4BA5LtB,IAAAL;4BA6Lc,MAAM,gBAAgB,MAAM,MAAM,OAAA,CAAQ;gCACxC,QAAQ;gCACR;gCACA;4BACF;4BAEA,MAAMK,cAAa,cAAc,UAAA;4BACjC,MAAMD,SAAAA,CAAQJ,OAAA,cAAc,KAAA,KAAd,OAAAA,OAAuB;gCAAE,QAAQ;4BAAI;4BAEnD,YAAY,aAAA,CACV,0BAA0B;gCACxB;gCACA,YAAY;oCACV,iBAAiB;wCACf,QAAQ,IACNK,YAAW,GAAA,CAAI,CAAA,YAAa,KAAK,SAAA,CAAU;oCAC/C;oCACA,mBAAmBD,OAAM,MAAA;gCAC3B;4BACF;4BAGF,OAAO;gCAAE,YAAAC;gCAAY,OAAAD;4BAAM;wBAC7B;oBACF;gBACF;gBAEA,WAAW,IAAA,IAAQ;gBACnB,UAAU,MAAM,MAAA;YAClB;YAEA,KAAK,aAAA,CACH,0BAA0B;gBACxB;gBACA,YAAY;oBACV,iBAAiB;wBACf,QAAQ,IACN,WAAW,GAAA,CAAI,CAAA,YAAa,KAAK,SAAA,CAAU;oBAC/C;oBACA,mBAAmB;gBACrB;YACF;YAGF,OAAO,IAAI,uBAAuB;gBAChC;gBACA;gBACA,OAAO;oBAAE;gBAAO;YAClB;QACF;IACF;AACF;AAEA,IAAM,yBAAN;IAKE,YAAY,OAAA,CAIT;QACD,IAAA,CAAK,MAAA,GAAS,QAAQ,MAAA;QACtB,IAAA,CAAK,UAAA,GAAa,QAAQ,UAAA;QAC1B,IAAA,CAAK,KAAA,GAAQ,QAAQ,KAAA;IACvB;AACF;;;;AG9PA,IAAMH,QAAO;AACb,IAAMK,UAAS,CAAA,gBAAA,EAAmBL,MAAI,CAAA;AACtC,IAAMM,UAAS,OAAO,GAAA,CAAID;AAJ1B,IAAAN;AAMO,IAAM,gBAAN,cAA4BQ,iLAAAA,CAAAA,aAAAA;IAOjC,YAAY,EACV,GAAA,EACA,UAAA,EACA,UAAA,EACA,KAAA,EACA,UAAU,SAAS,OACf,CAAA,mBAAA,EAAsB,IAAG,EAAA,EAAK,WAAU,CAAA,EAAI,WAAU,CAAA,GACtD,CAAA,mBAAA,EAAsB,IAAG,EAAA,EAAK,MAAK,CAAA,EACzC,CAMG;QACD,KAAA,CAAM;YAAE,MAAAP;YAAM;YAAS;QAAM;QArB/B,IAAA,CAAkBD,IAAAA,GAAU;QAuB1B,IAAA,CAAK,GAAA,GAAM;QACX,IAAA,CAAK,UAAA,GAAa;QAClB,IAAA,CAAK,UAAA,GAAa;IACpB;IAEA,OAAO,WAAW,KAAA,EAAwC;QACxD,OAAOQ,iLAAAA,CAAAA,aAAAA,CAAW,SAAA,CAAU,OAAOF;IACrC;IAAA;;GAAA,GAKA,OAAO,gBAAgB,KAAA,EAAwC;QAC7D,OACE,iBAAiB,SACjB,MAAM,IAAA,KAASL,SACf,OAAQ,MAAwB,GAAA,KAAQ,YAAA,CACtC,MAAwB,UAAA,IAAc,QACtC,OAAQ,MAAwB,UAAA,KAAe,QAAA,KAAA,CAC/C,MAAwB,UAAA,IAAc,QACtC,OAAQ,MAAwB,UAAA,KAAe,QAAA;IAErD;IAAA;;GAAA,GAKA,SAAS;QACP,OAAO;YACL,MAAM,IAAA,CAAK,IAAA;YACX,SAAS,IAAA,CAAK,OAAA;YACd,KAAK,IAAA,CAAK,GAAA;YACV,YAAY,IAAA,CAAK,UAAA;YACjB,YAAY,IAAA,CAAK,UAAA;YACjB,OAAO,IAAA,CAAK,KAAA;QACd;IACF;AACF;AA5DoBD,MAAAO;;ACLpB,eAAsB,SAAS,EAC7B,GAAA,EACA,sBAAsB,KAAA,EACxB;IALA,IAAAP;IAYE,MAAM,UAAU,IAAI,QAAA;IACpB,IAAI;QACF,MAAM,WAAW,MAAM,oBAAoB;QAE3C,IAAI,CAAC,SAAS,EAAA,EAAI;YAChB,MAAM,IAAI,cAAc;gBACtB,KAAK;gBACL,YAAY,SAAS,MAAA;gBACrB,YAAY,SAAS,UAAA;YACvB;QACF;QAEA,OAAO;YACL,MAAM,IAAI,WAAW,MAAM,SAAS,WAAA;YACpC,UAAA,CAAUA,OAAA,SAAS,OAAA,CAAQ,GAAA,CAAI,eAAc,KAAnC,OAAAA,OAAwC,KAAA;QACpD;IACF,EAAA,OAAS,OAAO;QACd,IAAI,cAAc,UAAA,CAAW,QAAQ;YACnC,MAAM;QACR;QAEA,MAAM,IAAI,cAAc;YAAE,KAAK;YAAS,OAAO;QAAM;IACvD;AACF;;ACnCA,IAAM,qBAAqB;IACzB;QAAE,UAAU;QAAsB,OAAO;YAAC;YAAM;YAAM;SAAI;IAAE;IAC5D;QAAE,UAAU;QAAsB,OAAO;YAAC;YAAM;YAAM;YAAM;SAAI;IAAE;IAClE;QAAE,UAAU;QAAuB,OAAO;YAAC;YAAM;SAAI;IAAE;IACvD;QAAE,UAAU;QAAuB,OAAO;YAAC;YAAM;YAAM;YAAM;SAAI;IAAE;CACrE;AAEO,SAAS,oBACd,KAAA;IAEA,KAAA,MAAW,EAAE,KAAA,EAAO,QAAA,EAAS,IAAK,mBAAoB;QACpD,IACE,MAAM,MAAA,IAAU,MAAM,MAAA,IACtB,MAAM,KAAA,CAAM,CAAC,MAAM,QAAU,KAAA,CAAM,MAAK,KAAM,OAC9C;YACA,OAAO;QACT;IACF;IAEA,OAAO,KAAA;AACT;;;AElBA,IAAMC,QAAO;AACb,IAAMK,UAAS,CAAA,gBAAA,EAAmBL,MAAI,CAAA;AACtC,IAAMM,UAAS,OAAO,GAAA,CAAID;AAJ1B,IAAAN;AAMO,IAAM,0BAAN,cAAsCQ,iLAAAA,CAAAA,aAAAA;IAK3C,YAAY,EACV,OAAA,EACA,KAAA,EACA,UAAU,CAAA,4FAAA,EAA+F,OAAO,QAAO,CAAA,CAAA,EACzH,CAIG;QACD,KAAA,CAAM;YAAE,MAAAP;YAAM;YAAS;QAAM;QAb/B,IAAA,CAAkBD,IAAAA,GAAU;QAe1B,IAAA,CAAK,OAAA,GAAU;IACjB;IAEA,OAAO,WAAW,KAAA,EAAkD;QAClE,OAAOQ,iLAAAA,CAAAA,aAAAA,CAAW,SAAA,CAAU,OAAOF;IACrC;IAAA;;GAAA,GAKA,OAAO,0BACL,KAAA,EACkC;QAClC,OACE,iBAAiB,SACjB,MAAM,IAAA,KAASL,SACd,MAAkC,OAAA,IAAW;IAElD;IAAA;;GAAA,GAKA,SAAS;QACP,OAAO;YACL,MAAM,IAAA,CAAK,IAAA;YACX,SAAS,IAAA,CAAK,OAAA;YACd,OAAO,IAAA,CAAK,KAAA;YACZ,OAAO,IAAA,CAAK,KAAA;YACZ,SAAS,IAAA,CAAK,OAAA;QAChB;IACF;AACF;AA/CoBD,MAAAO;;ADQb,IAAM,oBAA4C,sIAAA,CAAA,IAAA,CAAE,KAAA,CAAM;IAC/D,sIAAA,CAAA,IAAA,CAAE,MAAA;IACF,sIAAA,CAAA,IAAA,CAAE,UAAA,CAAW;IACb,sIAAA,CAAA,IAAA,CAAE,UAAA,CAAW;IACb,sIAAA,CAAA,IAAA,CAAE,MAAA,CAAA,yEAAA;IAEA,CAAC;QArBL,IAAAP,MAAA;QAsBM,OAAA,CAAA,KAAA,CAAAA,OAAA,WAAW,MAAA,KAAX,OAAA,KAAA,IAAAA,KAAmB,QAAA,CAAS,MAAA,KAA5B,OAAA,KAAsC;IAAA,GACxC;QAAE,SAAS;IAAmB;CAEjC;AAQM,SAAS,iCAAiC,OAAA;IAC/C,IAAI,OAAO,YAAY,UAAU;QAC/B,OAAO;IACT;IAEA,IAAI,mBAAmB,aAAa;QAClC,OAAO,CAAA,GAAA,0LAAA,CAAA,4BAAA,EAA0B,IAAI,WAAW;IAClD;IAEA,OAAO,CAAA,GAAA,0LAAA,CAAA,4BAAA,EAA0B;AACnC;AAQO,SAAS,+BACd,OAAA;IAEA,IAAI,mBAAmB,YAAY;QACjC,OAAO;IACT;IAEA,IAAI,OAAO,YAAY,UAAU;QAC/B,IAAI;YACF,OAAO,CAAA,GAAA,0LAAA,CAAA,4BAAA,EAA0B;QACnC,EAAA,OAAS,OAAO;YACd,MAAM,IAAI,wBAAwB;gBAChC,SACE;gBACF;gBACA,OAAO;YACT;QACF;IACF;IAEA,IAAI,mBAAmB,aAAa;QAClC,OAAO,IAAI,WAAW;IACxB;IAEA,MAAM,IAAI,wBAAwB;QAAE;IAAQ;AAC9C;AAQO,SAAS,wBAAwB,UAAA;IACtC,IAAI;QACF,OAAO,IAAI,cAAc,MAAA,CAAO;IAClC,EAAA,OAAS,OAAO;QACd,MAAM,IAAI,MAAM;IAClB;AACF;;AExFA,IAAMC,QAAO;AACb,IAAMK,UAAS,CAAA,gBAAA,EAAmBL,MAAI,CAAA;AACtC,IAAMM,UAAS,OAAO,GAAA,CAAID;AAJ1B,IAAAN;AAMO,IAAM,0BAAN,cAAsCQ,iLAAAA,CAAAA,aAAAA;IAK3C,YAAY,EACV,IAAA,EACA,UAAU,CAAA,uBAAA,EAA0B,KAAI,yDAAA,CAAA,EAC1C,CAGG;QACD,KAAA,CAAM;YAAE,MAAAP;YAAM;QAAQ;QAXxB,IAAA,CAAkBD,IAAAA,GAAU;QAa1B,IAAA,CAAK,IAAA,GAAO;IACd;IAEA,OAAO,WAAW,KAAA,EAAkD;QAClE,OAAOQ,iLAAAA,CAAAA,aAAAA,CAAW,SAAA,CAAU,OAAOF;IACrC;IAAA;;GAAA,GAKA,OAAO,0BACL,KAAA,EACkC;QAClC,OACE,iBAAiB,SACjB,MAAM,IAAA,KAASL,SACf,OAAQ,MAAkC,IAAA,KAAS;IAEvD;IAAA;;GAAA,GAKA,SAAS;QACP,OAAO;YACL,MAAM,IAAA,CAAK,IAAA;YACX,SAAS,IAAA,CAAK,OAAA;YACd,OAAO,IAAA,CAAK,KAAA;YAEZ,MAAM,IAAA,CAAK,IAAA;QACb;IACF;AACF;AA7CoBD,MAAAO;;ACPb,SAAS,aAAa,OAAA;IAI3B,IAAI;QACF,MAAM,CAAC,QAAQ,cAAa,GAAI,QAAQ,KAAA,CAAM;QAC9C,OAAO;YACL,UAAU,OAAO,KAAA,CAAM,IAAG,CAAE,EAAC,CAAE,KAAA,CAAM,IAAG,CAAE,EAAC;YAC3C;QACF;IACF,EAAA,OAAS,OAAO;QACd,OAAO;YACL,UAAU,KAAA;YACV,eAAe,KAAA;QACjB;IACF;AACF;;APOA,eAAsB,6BAA6B,EACjD,MAAA,EACA,yBAAyB,IAAA,EACzB,yBAAyB,QAAA,EAC3B;IAKE,MAAM,wBAA+C,EAAC;IAEtD,IAAI,OAAO,MAAA,IAAU,MAAM;QACzB,sBAAsB,IAAA,CAAK;YAAE,MAAM;YAAU,SAAS,OAAO,MAAA;QAAO;IACtE;IAEA,MAAM,mBACJ,0BAA0B,OAAO,QAAA,IAAY,OACzC,OACA,MAAM,eAAe,OAAO,QAAA,EAAU;IAE5C,MAAM,aAAa,OAAO,IAAA;IAC1B,OAAQ;QACN,KAAK;YAAU;gBACb,sBAAsB,IAAA,CAAK;oBACzB,MAAM;oBACN,SAAS;wBAAC;4BAAE,MAAM;4BAAQ,MAAM,OAAO,MAAA;wBAAO;qBAAC;gBACjD;gBACA;YACF;QAEA,KAAK;YAAY;gBACf,sBAAsB,IAAA,IACjB,OAAO,QAAA,CAAS,GAAA,CACjB,CAAC,UACC,8BAA8B,SAAS;gBAG7C;YACF;QAEA;YAAS;gBACP,MAAM,mBAA0B;gBAChC,MAAM,IAAI,MAAM,CAAA,yBAAA,EAA4B,iBAAgB,CAAE;YAChE;IACF;IAEA,OAAO;AACT;AASO,SAAS,8BACd,OAAA,EACA,gBAAA;IAKA,MAAM,OAAO,QAAQ,IAAA;IACrB,OAAQ;QACN,KAAK;YAAU;gBACb,OAAO;oBACL,MAAM;oBACN,SAAS,QAAQ,OAAA;oBACjB,kBAAkB,QAAQ,6BAAA;gBAC5B;YACF;QAEA,KAAK;YAAQ;gBACX,IAAI,OAAO,QAAQ,OAAA,KAAY,UAAU;oBACvC,OAAO;wBACL,MAAM;wBACN,SAAS;4BAAC;gCAAE,MAAM;gCAAQ,MAAM,QAAQ,OAAA;4BAAQ;yBAAC;wBACjD,kBAAkB,QAAQ,6BAAA;oBAC5B;gBACF;gBAEA,OAAO;oBACL,MAAM;oBACN,SAAS,QAAQ,OAAA,CACd,GAAA,CACC,CACE;wBA9Gd,IAAAP,MAAA,IAAA,IAAA,IAAA;wBAmHc,OAAQ,KAAK,IAAA;4BACX,KAAK;gCAAQ;oCACX,OAAO;wCACL,MAAM;wCACN,MAAM,KAAK,IAAA;wCACX,kBAAkB,KAAK,6BAAA;oCACzB;gCACF;4BAEA,KAAK;gCAAS;oCACZ,IAAI,KAAK,KAAA,YAAiB,KAAK;wCAC7B,IAAI,oBAAoB,MAAM;4CAC5B,OAAO;gDACL,MAAM;gDACN,OAAO,KAAK,KAAA;gDACZ,UAAU,KAAK,QAAA;gDACf,kBAAkB,KAAK,6BAAA;4CACzB;wCACF,OAAO;4CACL,MAAM,kBACJ,gBAAA,CAAiB,KAAK,KAAA,CAAM,QAAA,GAAU;4CACxC,OAAO;gDACL,MAAM;gDACN,OAAO,gBAAgB,IAAA;gDACvB,UAAA,CAAUA,OAAA,KAAK,QAAA,KAAL,OAAAA,OAAiB,gBAAgB,QAAA;gDAC3C,kBAAkB,KAAK,6BAAA;4CACzB;wCACF;oCACF;oCAGA,IAAI,OAAO,KAAK,KAAA,KAAU,UAAU;wCAClC,IAAI;4CACF,MAAM,MAAM,IAAI,IAAI,KAAK,KAAK;4CAE9B,OAAQ,IAAI,QAAA;gDACV,KAAK;gDACL,KAAK;oDAAU;wDACb,IAAI,oBAAoB,MAAM;4DAC5B,OAAO;gEACL,MAAM;gEACN,OAAO;gEACP,UAAU,KAAK,QAAA;gEACf,kBACE,KAAK,6BAAA;4DACT;wDACF,OAAO;4DACL,MAAM,kBACJ,gBAAA,CAAiB,IAAI,QAAA,GAAU;4DACjC,OAAO;gEACL,MAAM;gEACN,OAAO,gBAAgB,IAAA;gEACvB,UAAA,CACE,KAAA,KAAK,QAAA,KAAL,OAAA,KAAiB,gBAAgB,QAAA;gEACnC,kBACE,KAAK,6BAAA;4DACT;wDACF;oDACF;gDACA,KAAK;oDAAS;wDACZ,IAAI;4DACF,MAAM,EAAE,QAAA,EAAU,aAAA,EAAc,GAAI,aAClC,KAAK,KAAA;4DAGP,IAAI,YAAY,QAAQ,iBAAiB,MAAM;gEAC7C,MAAM,IAAI,MAAM;4DAClB;4DAEA,OAAO;gEACL,MAAM;gEACN,OACE,+BAA+B;gEACjC;gEACA,kBACE,KAAK,6BAAA;4DACT;wDACF,EAAA,OAAS,OAAO;4DACd,MAAM,IAAI,MACR,CAAA,2BAAA,EAA8BS,CAAAA,GAAAA,0LAAAA,CAAAA,kBAAAA,EAC5B,SACD,CAAA;wDAEL;oDACF;4CACF;wCACF,EAAA,OAAS,UAAU,CAEnB;oCACF;oCAEA,MAAM,aAAa,+BAA+B,KAAK,KAAK;oCAE5D,OAAO;wCACL,MAAM;wCACN,OAAO;wCACP,UAAA,CAAU,KAAA,KAAK,QAAA,KAAL,OAAA,KAAiB,oBAAoB;wCAC/C,kBAAkB,KAAK,6BAAA;oCACzB;gCACF;4BAEA,KAAK;gCAAQ;oCACX,IAAI,KAAK,IAAA,YAAgB,KAAK;wCAC5B,IAAI,oBAAoB,MAAM;4CAC5B,OAAO;gDACL,MAAM;gDACN,MAAM,KAAK,IAAA;gDACX,UAAU,KAAK,QAAA;gDACf,kBAAkB,KAAK,6BAAA;4CACzB;wCACF,OAAO;4CACL,MAAM,kBACJ,gBAAA,CAAiB,KAAK,IAAA,CAAK,QAAA,GAAU;4CACvC,OAAO;gDACL,MAAM;gDACN,MAAMC,CAAAA,GAAAA,0LAAAA,CAAAA,4BAAAA,EAA0B,gBAAgB,IAAI;gDACpD,UAAA,CAAU,KAAA,KAAK,QAAA,KAAL,OAAA,KAAiB,gBAAgB,QAAA;gDAC3C,kBAAkB,KAAK,6BAAA;4CACzB;wCACF;oCACF;oCAGA,IAAI,OAAO,KAAK,IAAA,KAAS,UAAU;wCACjC,IAAI;4CACF,MAAM,MAAM,IAAI,IAAI,KAAK,IAAI;4CAE7B,OAAQ,IAAI,QAAA;gDACV,KAAK;gDACL,KAAK;oDAAU;wDACb,IAAI,oBAAoB,MAAM;4DAC5B,OAAO;gEACL,MAAM;gEACN,MAAM;gEACN,UAAU,KAAK,QAAA;gEACf,kBACE,KAAK,6BAAA;4DACT;wDACF,OAAO;4DACL,MAAM,kBACJ,gBAAA,CAAiB,IAAI,QAAA,GAAU;4DACjC,OAAO;gEACL,MAAM;gEACN,MAAMA,CAAAA,GAAAA,0LAAAA,CAAAA,4BAAAA,EACJ,gBAAgB,IAAA;gEAElB,UAAA,CACE,KAAA,KAAK,QAAA,KAAL,OAAA,KAAiB,gBAAgB,QAAA;gEACnC,kBACE,KAAK,6BAAA;4DACT;wDACF;oDACF;gDACA,KAAK;oDAAS;wDACZ,IAAI;4DACF,MAAM,EAAE,QAAA,EAAU,aAAA,EAAc,GAAI,aAClC,KAAK,IAAA;4DAGP,IAAI,YAAY,QAAQ,iBAAiB,MAAM;gEAC7C,MAAM,IAAI,MAAM;4DAClB;4DAEA,OAAO;gEACL,MAAM;gEACN,MAAM,iCACJ;gEAEF;gEACA,kBACE,KAAK,6BAAA;4DACT;wDACF,EAAA,OAAS,OAAO;4DACd,MAAM,IAAI,MACR,CAAA,2BAAA,EAA8BD,CAAAA,GAAAA,0LAAAA,CAAAA,kBAAAA,EAC5B,SACD,CAAA;wDAEL;oDACF;4CACF;wCACF,EAAA,OAAS,UAAU,CAEnB;oCACF;oCAEA,MAAM,cAAc,iCAClB,KAAK,IAAA;oCAGP,OAAO;wCACL,MAAM;wCACN,MAAM;wCACN,UAAU,KAAK,QAAA;wCACf,kBAAkB,KAAK,6BAAA;oCACzB;gCACF;wBACF;oBACF,GAGD,MAAA,CAAO,CAAA,OAAQ,KAAK,IAAA,KAAS,UAAU,KAAK,IAAA,KAAS;oBACxD,kBAAkB,QAAQ,6BAAA;gBAC5B;YACF;QAEA,KAAK;YAAa;gBAChB,IAAI,OAAO,QAAQ,OAAA,KAAY,UAAU;oBACvC,OAAO;wBACL,MAAM;wBACN,SAAS;4BAAC;gCAAE,MAAM;gCAAQ,MAAM,QAAQ,OAAA;4BAAQ;yBAAC;wBACjD,kBAAkB,QAAQ,6BAAA;oBAC5B;gBACF;gBAEA,OAAO;oBACL,MAAM;oBACN,SAAS,QAAQ,OAAA,CACd,MAAA,CAAA,2BAAA;oBAEC,CAAA,OAAQ,KAAK,IAAA,KAAS,UAAU,KAAK,IAAA,KAAS,IAE/C,GAAA,CAAI,CAAA;wBACH,MAAM,EAAE,6BAAA,EAA+B,GAAG,MAAK,GAAI;wBACnD,OAAO;4BACL,GAAG,IAAA;4BACH,kBAAkB;wBACpB;oBACF;oBACF,kBAAkB,QAAQ,6BAAA;gBAC5B;YACF;QAEA,KAAK;YAAQ;gBACX,OAAO;oBACL,MAAM;oBACN,SAAS,QAAQ,OAAA,CAAQ,GAAA,CAAI,CAAA,OAAA,CAAS;4BACpC,MAAM;4BACN,YAAY,KAAK,UAAA;4BACjB,UAAU,KAAK,QAAA;4BACf,QAAQ,KAAK,MAAA;4BACb,kBAAkB,KAAK,6BAAA;wBACzB,CAAA;oBACA,kBAAkB,QAAQ,6BAAA;gBAC5B;YACF;QAEA;YAAS;gBACP,MAAM,mBAA0B;gBAChC,MAAM,IAAI,wBAAwB;oBAAE,MAAM;gBAAiB;YAC7D;IACF;AACF;AAKA,eAAe,eACb,QAAA,EACA,sBAAA;IAEA,MAAM,OAAO,SACV,MAAA,CAAO,CAAA,UAAW,QAAQ,IAAA,KAAS,QACnC,GAAA,CAAI,CAAA,UAAW,QAAQ,OAAO,EAC9B,MAAA,CAAO,CAAC,UACP,MAAM,OAAA,CAAQ,UAEf,IAAA,GACA,MAAA,CACC,CAAC,OACC,KAAK,IAAA,KAAS,WAAW,KAAK,IAAA,KAAS,QAE1C,GAAA,CAAI,CAAA,OAAS,KAAK,IAAA,KAAS,UAAU,KAAK,KAAA,GAAQ,KAAK,IAAK,EAC5D,GAAA,CAAI,CAAA,OAAA,uBAAA;QAEH,OAAO,SAAS,YAAA,CACf,KAAK,UAAA,CAAW,YAAY,KAAK,UAAA,CAAW,SAAQ,IACjD,IAAI,IAAI,QACR,MAEL,MAAA,CAAO,CAAC,QAAwB,iBAAiB;IAGpD,MAAM,mBAAmB,MAAM,QAAQ,GAAA,CACrC,KAAK,GAAA,CAAI,OAAM,MAAA,CAAQ;YACrB;YACA,MAAM,MAAM,uBAAuB;gBAAE;YAAI;QAC3C,CAAA;IAGF,OAAO,OAAO,WAAA,CACZ,iBAAiB,GAAA,CAAI,CAAC,EAAE,GAAA,EAAK,IAAA,EAAK,GAAM;YAAC,IAAI,QAAA;YAAY;SAAK;AAElE;;AQtZA,IAAMR,QAAO;AACb,IAAMK,UAAS,CAAA,gBAAA,EAAmBL,MAAI,CAAA;AACtC,IAAMM,UAAS,OAAO,GAAA,CAAID;AAJ1B,IAAAN;AAMO,IAAM,uBAAN,cAAmCQ,iLAAAA,CAAAA,aAAAA;IAMxC,YAAY,EACV,SAAA,EACA,KAAA,EACA,OAAA,EACF,CAIG;QACD,KAAA,CAAM;YACJ,MAAAP;YACA,SAAS,CAAA,+BAAA,EAAkC,UAAS,EAAA,EAAK,QAAO,CAAA;QAClE;QAjBF,IAAA,CAAkBD,IAAAA,GAAU;QAmB1B,IAAA,CAAK,SAAA,GAAY;QACjB,IAAA,CAAK,KAAA,GAAQ;IACf;IAEA,OAAO,WAAW,KAAA,EAA+C;QAC/D,OAAOQ,iLAAAA,CAAAA,aAAAA,CAAW,SAAA,CAAU,OAAOF;IACrC;IAAA;;GAAA,GAKA,OAAO,uBAAuB,KAAA,EAA+C;QAC3E,OACE,iBAAiB,SACjB,MAAM,IAAA,KAASL,SACf,OAAQ,MAA+B,SAAA,KAAc,YACrD,OAAQ,MAA+B,KAAA,KAAU;IAErD;IAEA,SAAS;QACP,OAAO;YACL,MAAM,IAAA,CAAK,IAAA;YACX,SAAS,IAAA,CAAK,OAAA;YACd,OAAO,IAAA,CAAK,KAAA;YAEZ,WAAW,IAAA,CAAK,SAAA;YAChB,OAAO,IAAA,CAAK,KAAA;QACd;IACF;AACF;AAjDoBD,MAAAO;;ACDb,SAAS,oBAAoB,EAClC,SAAA,EACA,WAAA,EACA,IAAA,EACA,IAAA,EACA,eAAA,EACA,gBAAA,EACA,aAAA,EACA,IAAA,EACA,UAAA,EACF;IAIE,IAAI,aAAa,MAAM;QACrB,IAAI,CAAC,OAAO,SAAA,CAAU,YAAY;YAChC,MAAM,IAAI,qBAAqB;gBAC7B,WAAW;gBACX,OAAO;gBACP,SAAS;YACX;QACF;QAEA,IAAI,YAAY,GAAG;YACjB,MAAM,IAAI,qBAAqB;gBAC7B,WAAW;gBACX,OAAO;gBACP,SAAS;YACX;QACF;IACF;IAEA,IAAI,eAAe,MAAM;QACvB,IAAI,OAAO,gBAAgB,UAAU;YACnC,MAAM,IAAI,qBAAqB;gBAC7B,WAAW;gBACX,OAAO;gBACP,SAAS;YACX;QACF;IACF;IAEA,IAAI,QAAQ,MAAM;QAChB,IAAI,OAAO,SAAS,UAAU;YAC5B,MAAM,IAAI,qBAAqB;gBAC7B,WAAW;gBACX,OAAO;gBACP,SAAS;YACX;QACF;IACF;IAEA,IAAI,QAAQ,MAAM;QAChB,IAAI,OAAO,SAAS,UAAU;YAC5B,MAAM,IAAI,qBAAqB;gBAC7B,WAAW;gBACX,OAAO;gBACP,SAAS;YACX;QACF;IACF;IAEA,IAAI,mBAAmB,MAAM;QAC3B,IAAI,OAAO,oBAAoB,UAAU;YACvC,MAAM,IAAI,qBAAqB;gBAC7B,WAAW;gBACX,OAAO;gBACP,SAAS;YACX;QACF;IACF;IAEA,IAAI,oBAAoB,MAAM;QAC5B,IAAI,OAAO,qBAAqB,UAAU;YACxC,MAAM,IAAI,qBAAqB;gBAC7B,WAAW;gBACX,OAAO;gBACP,SAAS;YACX;QACF;IACF;IAEA,IAAI,QAAQ,MAAM;QAChB,IAAI,CAAC,OAAO,SAAA,CAAU,OAAO;YAC3B,MAAM,IAAI,qBAAqB;gBAC7B,WAAW;gBACX,OAAO;gBACP,SAAS;YACX;QACF;IACF;IAEA,IAAI,cAAc,MAAM;QACtB,IAAI,CAAC,OAAO,SAAA,CAAU,aAAa;YACjC,MAAM,IAAI,qBAAqB;gBAC7B,WAAW;gBACX,OAAO;gBACP,SAAS;YACX;QACF;QAEA,IAAI,aAAa,GAAG;YAClB,MAAM,IAAI,qBAAqB;gBAC7B,WAAW;gBACX,OAAO;gBACP,SAAS;YACX;QACF;IACF;IAEA,OAAO;QACL;QACA,aAAa,eAAA,OAAA,cAAe;QAC5B;QACA;QACA;QACA;QACA,eACE,iBAAiB,QAAQ,cAAc,MAAA,GAAS,IAC5C,gBACA,KAAA;QACN;QACA,YAAY,cAAA,OAAA,aAAc;IAC5B;AACF;;;;;;;AI/HO,IAAM,kBAAwCI,sIAAAA,CAAAA,IAAAA,CAAE,IAAA,CAAK,IAC1DA,sIAAAA,CAAAA,IAAAA,CAAE,KAAA,CAAM;QACNA,sIAAAA,CAAAA,IAAAA,CAAE,IAAA;QACFA,sIAAAA,CAAAA,IAAAA,CAAE,MAAA;QACFA,sIAAAA,CAAAA,IAAAA,CAAE,MAAA;QACFA,sIAAAA,CAAAA,IAAAA,CAAE,OAAA;QACFA,sIAAAA,CAAAA,IAAAA,CAAE,MAAA,CAAOA,sIAAAA,CAAAA,IAAAA,CAAE,MAAA,IAAU;QACrBA,sIAAAA,CAAAA,IAAAA,CAAE,KAAA,CAAM;KACT;;ADAI,IAAM,yBAAsDA,sIAAAA,CAAAA,IAAAA,CAAE,MAAA,CACnEA,sIAAAA,CAAAA,IAAAA,CAAE,MAAA,IACFA,sIAAAA,CAAAA,IAAAA,CAAE,MAAA,CAAOA,sIAAAA,CAAAA,IAAAA,CAAE,MAAA,IAAU;;AEahB,IAAM,iBAAsCA,sIAAAA,CAAAA,IAAAA,CAAE,MAAA,CAAO;IAC1D,MAAMA,sIAAAA,CAAAA,IAAAA,CAAE,OAAA,CAAQ;IAChB,MAAMA,sIAAAA,CAAAA,IAAAA,CAAE,MAAA;IACR,+BAA+B,uBAAuB,QAAA;AACxD;AA6BO,IAAM,kBAAwCA,sIAAAA,CAAAA,IAAAA,CAAE,MAAA,CAAO;IAC5D,MAAMA,sIAAAA,CAAAA,IAAAA,CAAE,OAAA,CAAQ;IAChB,OAAOA,sIAAAA,CAAAA,IAAAA,CAAE,KAAA,CAAM;QAAC;QAAmBA,sIAAAA,CAAAA,IAAAA,CAAE,UAAA,CAAW;KAAK;IACrD,UAAUA,sIAAAA,CAAAA,IAAAA,CAAE,MAAA,GAAS,QAAA;IACrB,+BAA+B,uBAAuB,QAAA;AACxD;AA6BO,IAAM,iBAAsCA,sIAAAA,CAAAA,IAAAA,CAAE,MAAA,CAAO;IAC1D,MAAMA,sIAAAA,CAAAA,IAAAA,CAAE,OAAA,CAAQ;IAChB,MAAMA,sIAAAA,CAAAA,IAAAA,CAAE,KAAA,CAAM;QAAC;QAAmBA,sIAAAA,CAAAA,IAAAA,CAAE,UAAA,CAAW;KAAK;IACpD,UAAUA,sIAAAA,CAAAA,IAAAA,CAAE,MAAA;IACZ,+BAA+B,uBAAuB,QAAA;AACxD;AA+BO,IAAM,qBAA8CA,sIAAAA,CAAAA,IAAAA,CAAE,MAAA,CAAO;IAClE,MAAMA,sIAAAA,CAAAA,IAAAA,CAAE,OAAA,CAAQ;IAChB,YAAYA,sIAAAA,CAAAA,IAAAA,CAAE,MAAA;IACd,UAAUA,sIAAAA,CAAAA,IAAAA,CAAE,MAAA;IACZ,MAAMA,sIAAAA,CAAAA,IAAAA,CAAE,OAAA;AACV;AAoCO,IAAM,uBAAkDA,sIAAAA,CAAAA,IAAAA,CAAE,MAAA,CAAO;IACtE,MAAMA,sIAAAA,CAAAA,IAAAA,CAAE,OAAA,CAAQ;IAChB,YAAYA,sIAAAA,CAAAA,IAAAA,CAAE,MAAA;IACd,UAAUA,sIAAAA,CAAAA,IAAAA,CAAE,MAAA;IACZ,QAAQA,sIAAAA,CAAAA,IAAAA,CAAE,OAAA;IACV,SAASA,sIAAAA,CAAAA,IAAAA,CAAE,OAAA,GAAU,QAAA;IACrB,+BAA+B,uBAAuB,QAAA;AACxD;;AH9IO,IAAM,0BAAwDA,sIAAAA,CAAAA,IAAAA,CAAE,MAAA,CAAO;IAC5E,MAAMA,sIAAAA,CAAAA,IAAAA,CAAE,OAAA,CAAQ;IAChB,SAASA,sIAAAA,CAAAA,IAAAA,CAAE,MAAA;IACX,+BAA+B,uBAAuB,QAAA;AACxD;AAsBO,IAAM,wBAAoDA,sIAAAA,CAAAA,IAAAA,CAAE,MAAA,CAAO;IACxE,MAAMA,sIAAAA,CAAAA,IAAAA,CAAE,OAAA,CAAQ;IAChB,SAASA,sIAAAA,CAAAA,IAAAA,CAAE,KAAA,CAAM;QACfA,sIAAAA,CAAAA,IAAAA,CAAE,MAAA;QACFA,sIAAAA,CAAAA,IAAAA,CAAE,KAAA,CAAMA,sIAAAA,CAAAA,IAAAA,CAAE,KAAA,CAAM;YAAC;YAAgB;YAAiB;SAAe;KAClE;IACD,+BAA+B,uBAAuB,QAAA;AACxD;AA2BO,IAAM,6BACXA,sIAAAA,CAAAA,IAAAA,CAAE,MAAA,CAAO;IACP,MAAMA,sIAAAA,CAAAA,IAAAA,CAAE,OAAA,CAAQ;IAChB,SAASA,sIAAAA,CAAAA,IAAAA,CAAE,KAAA,CAAM;QACfA,sIAAAA,CAAAA,IAAAA,CAAE,MAAA;QACFA,sIAAAA,CAAAA,IAAAA,CAAE,KAAA,CAAMA,sIAAAA,CAAAA,IAAAA,CAAE,KAAA,CAAM;YAAC;YAAgB;SAAmB;KACrD;IACD,+BAA+B,uBAAuB,QAAA;AACxD;AA2BK,IAAM,wBAAoDA,sIAAAA,CAAAA,IAAAA,CAAE,MAAA,CAAO;IACxE,MAAMA,sIAAAA,CAAAA,IAAAA,CAAE,OAAA,CAAQ;IAChB,SAASA,sIAAAA,CAAAA,IAAAA,CAAE,KAAA,CAAM;IACjB,+BAA+B,uBAAuB,QAAA;AACxD;AAsBO,IAAM,oBAA4CA,sIAAAA,CAAAA,IAAAA,CAAE,KAAA,CAAM;IAC/D;IACA;IACA;IACA;CACD;;AD7IM,SAAS,eAAe,MAAA;IAC7B,IAAI,OAAO,MAAA,IAAU,QAAQ,OAAO,QAAA,IAAY,MAAM;QACpD,MAAM,IAAI,iLAAA,CAAA,qBAAA,CAAmB;YAC3B;YACA,SAAS;QACX;IACF;IAEA,IAAI,OAAO,MAAA,IAAU,QAAQ,OAAO,QAAA,IAAY,MAAM;QACpD,MAAM,IAAI,iLAAA,CAAA,qBAAA,CAAmB;YAC3B;YACA,SAAS;QACX;IACF;IAGA,IAAI,OAAO,MAAA,IAAU,QAAQ,OAAO,OAAO,MAAA,KAAW,UAAU;QAC9D,MAAM,IAAI,iLAAA,CAAA,qBAAA,CAAmB;YAC3B;YACA,SAAS;QACX;IACF;IAGA,IAAI,OAAO,MAAA,IAAU,MAAM;QAEzB,IAAI,OAAO,OAAO,MAAA,KAAW,UAAU;YACrC,MAAM,IAAI,iLAAA,CAAA,qBAAA,CAAmB;gBAC3B;gBACA,SAAS;YACX;QACF;QAEA,OAAO;YACL,MAAM;YACN,QAAQ,OAAO,MAAA;YACf,UAAU,KAAA;YACV,QAAQ,OAAO,MAAA;QACjB;IACF;IAGA,IAAI,OAAO,QAAA,IAAY,MAAM;QAC3B,MAAM,mBAAmB,CAAA,GAAA,0LAAA,CAAA,oBAAA,EAAkB;YACzC,OAAO,OAAO,QAAA;YACd,QAAQA,sIAAAA,CAAAA,IAAAA,CAAE,KAAA,CAAM;QAClB;QAEA,IAAI,CAAC,iBAAiB,OAAA,EAAS;YAC7B,MAAM,IAAI,iLAAA,CAAA,qBAAA,CAAmB;gBAC3B;gBACA,SAAS;gBACT,OAAO,iBAAiB,KAAA;YAC1B;QACF;QAEA,OAAO;YACL,MAAM;YACN,QAAQ,KAAA;YACR,UAAU,OAAO,QAAA;YAAA,wCAAA;YACjB,QAAQ,OAAO,MAAA;QACjB;IACF;IAEA,MAAM,IAAI,MAAM;AAClB;;AKzCO,SAAS,4BAA4B,KAAA;IAI1C,OAAO;QACL,cAAc,MAAM,YAAA;QACpB,kBAAkB,MAAM,gBAAA;QACxB,aAAa,MAAM,YAAA,GAAe,MAAM,gBAAA;IAC1C;AACF;;ACrDO,SAAS,uBACd,IAAA,EACA,EACE,WAAA,EACA,iBAAA,EACF;IALF,IAAAX;IAOE,MAAM,UAAU,IAAI,QAAA,CAAQA,OAAA,QAAA,OAAA,KAAA,IAAA,KAAM,OAAA,KAAN,OAAAA,OAAiB,CAAC;IAE9C,IAAI,CAAC,QAAQ,GAAA,CAAI,iBAAiB;QAChC,QAAQ,GAAA,CAAI,gBAAgB;IAC9B;IAEA,IAAI,sBAAsB,KAAA,GAAW;QACnC,QAAQ,GAAA,CAAI,2BAA2B;IACzC;IAEA,OAAO;AACT;;AChBA,IAAM,wBAAwB;AAC9B,IAAM,wBACJ;AACF,IAAM,yBAAyB;AAExB,SAAS,sBAAsB,EACpC,MAAA,EACA,MAAA,EACA,eAAe,UAAU,OAAO,wBAAwB,KAAA,CAAA,EACxD,eAAe,UAAU,OACrB,wBACA,sBAAA,EACN;IAME,OAAO;QACL,UAAU,QAAQ,OAAO,MAAA,GAAS,IAAI,SAAS,KAAA;QAC/C,UAAU,QAAQ,OAAO,MAAA,GAAS,IAAI,KAAK,KAAA;QAAA,sCAAA;QAC3C;QACA,UAAU,OAAO,KAAK,SAAA,CAAU,UAAU,KAAA;QAC1C;KACF,CACG,MAAA,CAAO,CAAA,OAAQ,QAAQ,MACvB,IAAA,CAAK;AACV;;AC3BA,IAAMC,QAAO;AACb,IAAMK,UAAS,CAAA,gBAAA,EAAmBL,MAAI,CAAA;AACtC,IAAMM,UAAS,OAAO,GAAA,CAAID;AAJ1B,IAAAN;AASO,IAAM,yBAAN,cAAqCQ,iLAAAA,CAAAA,aAAAA;IAAW,qBAAA;IAGrD,YAAY,EAAE,UAAU,sBAAA,EAAuB,GAA0B,CAAC,CAAA,CAAG;QAC3E,KAAA,CAAM;YAAE,MAAAP;YAAM;QAAQ;QAHxB,IAAA,CAAkBD,IAAAA,GAAU;IAI5B;IAEA,OAAO,WAAW,KAAA,EAAiD;QACjE,OAAOQ,iLAAAA,CAAAA,aAAAA,CAAW,SAAA,CAAU,OAAOF;IACrC;IAAA;;GAAA,GAKA,OAAO,yBACL,KAAA,EACiC;QACjC,OAAO,iBAAiB,SAAS,MAAM,IAAA,KAASL;IAClD;IAAA;;GAAA,GAKA,SAAS;QACP,OAAO;YACL,MAAM,IAAA,CAAK,IAAA;YACX,OAAO,IAAA,CAAK,KAAA;YACZ,SAAS,IAAA,CAAK,OAAA;YACd,OAAO,IAAA,CAAK,KAAA;QACd;IACF;AACF;AA9BoBD,MAAAO;;;;;AERb,SAAS,0BACd,MAAA,EACA,WAAA;IAEA,MAAM,oBAAyB,OAAO,WAAA,CACpC,IAAI,gBAAgB;IAGtB,iBAAA,CAAkB,OAAO,aAAa,CAAA,GAAI;QACxC,MAAM,SAAS,kBAAkB,SAAA;QACjC,OAAO;YACL,MAAM;gBACJ,MAAM,EAAE,IAAA,EAAM,KAAA,EAAM,GAAI,MAAM,OAAO,IAAA;gBACrC,OAAO,OAAO;oBAAE,MAAM;oBAAM,OAAO,KAAA;gBAAU,IAAI;oBAAE,MAAM;oBAAO;gBAAM;YACxE;QACF;IACF;IAEA,OAAO;AACT;;ADuBA,IAAM,yBAAsE;IAC1E,MAAM;IACN,YAAY,KAAA;IAEZ,uBAAsB,EAAE,KAAA,EAAO,SAAA,EAAU;QACvC,OAAO;YAAE,SAAS;YAAM,OAAO;gBAAE,SAAS;gBAAO;YAAU;QAAE;IAC/D;IAEA,qBACE,KAAA;QAEA,OAAO,UAAU,KAAA,IACb;YAAE,SAAS;YAAO,OAAO,IAAI;QAAyB,IACtD;YAAE,SAAS;YAAM;QAAM;IAC7B;IAEA;QACE,MAAM,IAAI,iLAAA,CAAA,gCAAA,CAA8B;YACtC,eAAe;QACjB;IACF;AACF;AAEA,IAAM,uBAAuB,CAC3B,SAAA,CACwD;QACxD,MAAM;QACN,YAAY,OAAO,UAAA;QAEnB,uBAAsB,EAAE,KAAA,EAAO,SAAA,EAAU;YACvC,OAAO;gBACL,SAAS;gBACT,OAAO;oBAAA,oDAAA;oBAEL,SAAS;oBACT;gBACF;YACF;QACF;QAEA,qBAAoB,KAAA;YAClB,OAAOK,CAAAA,GAAAA,0LAAAA,CAAAA,oBAAAA,EAAkB;gBAAE;gBAAO;YAAO;QAC3C;QAEA;YACE,MAAM,IAAI,iLAAA,CAAA,gCAAA,CAA8B;gBACtC,eAAe;YACjB;QACF;IACF,CAAA;AAEA,IAAM,sBAAsB,CAC1B;IAGA,MAAM,EAAE,OAAA,EAAS,GAAG,YAAW,GAAI,OAAO,UAAA;IAE1C,OAAO;QACL,MAAM;QAAA,2EAAA;QAAA,yCAAA;QAAA,kGAAA;QAKN,YAAY;YACV,SAAS;YACT,MAAM;YACN,YAAY;gBACV,UAAU;oBAAE,MAAM;oBAAS,OAAO;gBAAW;YAC/C;YACA,UAAU;gBAAC;aAAU;YACrB,sBAAsB;QACxB;QAEA,uBAAsB,EAAE,KAAA,EAAO,YAAA,EAAc,YAAA,EAAc,YAAA,EAAa;YArH5E,IAAAZ;YAuHM,IAAI,CAAC,CAAA,GAAA,iLAAA,CAAA,eAAA,EAAa,UAAU,CAAC,CAAA,GAAA,iLAAA,CAAA,cAAA,EAAY,MAAM,QAAQ,GAAG;gBACxD,OAAO;oBACL,SAAS;oBACT,OAAO,IAAI,iLAAA,CAAA,sBAAA,CAAoB;wBAC7B;wBACA,OAAO;oBACT;gBACF;YACF;YAEA,MAAM,aAAa,MAAM,QAAA;YACzB,MAAM,cAA8B,EAAC;YAErC,IAAA,IAAS,IAAI,GAAG,IAAI,WAAW,MAAA,EAAQ,IAAK;gBAC1C,MAAM,UAAU,UAAA,CAAW,EAAC;gBAC5B,MAAM,SAASY,CAAAA,GAAAA,0LAAAA,CAAAA,oBAAAA,EAAkB;oBAAE,OAAO;oBAAS;gBAAO;gBAM1D,IAAI,MAAM,WAAW,MAAA,GAAS,KAAK,CAAC,cAAc;oBAChD;gBACF;gBAEA,IAAI,CAAC,OAAO,OAAA,EAAS;oBACnB,OAAO;gBACT;gBAEA,YAAY,IAAA,CAAK,OAAO,KAAK;YAC/B;YAGA,MAAM,wBAAA,CAAwBZ,OAAA,gBAAA,OAAA,KAAA,IAAA,aAAc,MAAA,KAAd,OAAAA,OAAwB;YAEtD,IAAI,YAAY;YAEhB,IAAI,cAAc;gBAChB,aAAa;YACf;YAEA,IAAI,wBAAwB,GAAG;gBAC7B,aAAa;YACf;YAEA,aAAa,YACV,KAAA,CAAM,uBACN,GAAA,CAAI,CAAA,UAAW,KAAK,SAAA,CAAU,UAC9B,IAAA,CAAK;YAER,IAAI,cAAc;gBAChB,aAAa;YACf;YAEA,OAAO;gBACL,SAAS;gBACT,OAAO;oBACL,SAAS;oBACT;gBACF;YACF;QACF;QAEA,qBACE,KAAA;YAGA,IAAI,CAAC,CAAA,GAAA,iLAAA,CAAA,eAAA,EAAa,UAAU,CAAC,CAAA,GAAA,iLAAA,CAAA,cAAA,EAAY,MAAM,QAAQ,GAAG;gBACxD,OAAO;oBACL,SAAS;oBACT,OAAO,IAAI,iLAAA,CAAA,sBAAA,CAAoB;wBAC7B;wBACA,OAAO;oBACT;gBACF;YACF;YAEA,MAAM,aAAa,MAAM,QAAA;YAGzB,KAAA,MAAW,WAAW,WAAY;gBAChC,MAAM,SAASY,CAAAA,GAAAA,0LAAAA,CAAAA,oBAAAA,EAAkB;oBAAE,OAAO;oBAAS;gBAAO;gBAC1D,IAAI,CAAC,OAAO,OAAA,EAAS;oBACnB,OAAO;gBACT;YACF;YAEA,OAAO;gBAAE,SAAS;gBAAM,OAAO;YAA6B;QAC9D;QAEA,qBACE,cAAA;YAEA,IAAI,oBAAoB;YAExB,OAAO,0BAA0B,gBAAgB;gBAC/C,WAAU,KAAA,EAAO,UAAA;oBACf,OAAQ,MAAM,IAAA;wBACZ,KAAK;4BAAU;gCACb,MAAM,QAAQ,MAAM,MAAA;gCAGpB,MAAO,oBAAoB,MAAM,MAAA,EAAQ,oBAAqB;oCAC5D,WAAW,OAAA,CAAQ,KAAA,CAAM,kBAAkB;gCAC7C;gCAEA;4BACF;wBAEA,KAAK;wBACL,KAAK;4BACH;wBAEF,KAAK;4BACH,WAAW,KAAA,CAAM,MAAM,KAAK;4BAC5B;wBAEF;4BAAS;gCACP,MAAM,mBAA0B;gCAChC,MAAM,IAAI,MAAM,CAAA,wBAAA,EAA2B,iBAAgB,CAAE;4BAC/D;oBACF;gBACF;YACF;QACF;IACF;AACF;AAEA,IAAM,qBAAqB,CACzB;IAEA,OAAO;QACL,MAAM;QAAA,gEAAA;QAAA,8CAAA;QAAA,uFAAA;QAKN,YAAY;YACV,SAAS;YACT,MAAM;YACN,YAAY;gBACV,QAAQ;oBAAE,MAAM;oBAAU,MAAM;gBAAW;YAC7C;YACA,UAAU;gBAAC;aAAQ;YACnB,sBAAsB;QACxB;QAEA,qBAAoB,KAAA;YAElB,IAAI,CAAC,CAAA,GAAA,iLAAA,CAAA,eAAA,EAAa,UAAU,OAAO,MAAM,MAAA,KAAW,UAAU;gBAC5D,OAAO;oBACL,SAAS;oBACT,OAAO,IAAI,iLAAA,CAAA,sBAAA,CAAoB;wBAC7B;wBACA,OACE;oBACJ;gBACF;YACF;YAEA,MAAM,SAAS,MAAM,MAAA;YAErB,OAAO,WAAW,QAAA,CAAS,UACvB;gBAAE,SAAS;gBAAM,OAAO;YAAe,IACvC;gBACE,SAAS;gBACT,OAAO,IAAI,iLAAA,CAAA,sBAAA,CAAoB;oBAC7B;oBACA,OAAO;gBACT;YACF;QACN;QAEA;YAEE,MAAM,IAAI,iLAAA,CAAA,gCAAA,CAA8B;gBACtC,eAAe;YACjB;QACF;QAEA;YAEE,MAAM,IAAI,iLAAA,CAAA,gCAAA,CAA8B;gBACtC,eAAe;YACjB;QACF;IACF;AACF;AAEO,SAAS,kBAA0B,EACxC,MAAA,EACA,MAAA,EACA,UAAA,EACF;IAKE,OAAQ;QACN,KAAK;YACH,OAAO,qBAAqB,CAAA,GAAA,8KAAA,CAAA,WAAA,EAAS;QACvC,KAAK;YACH,OAAO,oBAAoB,CAAA,GAAA,8KAAA,CAAA,WAAA,EAAS;QACtC,KAAK;YACH,OAAO,mBAAmB;QAC5B,KAAK;YACH,OAAO;QACT;YAAS;gBACP,MAAM,mBAA0B;gBAChC,MAAM,IAAI,MAAM,CAAA,oBAAA,EAAuB,iBAAgB,CAAE;YAC3D;IACF;AACF;;AEvUO,SAAS,8BAA8B,EAC5C,MAAA,EACA,IAAA,EACA,MAAA,EACA,UAAA,EACA,iBAAA,EACA,UAAA,EACF;IAQE,IACE,UAAU,QACV,WAAW,YACX,WAAW,WACX,WAAW,UACX,WAAW,aACX;QACA,MAAM,IAAI,qBAAqB;YAC7B,WAAW;YACX,OAAO;YACP,SAAS;QACX;IACF;IAEA,IAAI,WAAW,aAAa;QAC1B,IAAI,SAAS,UAAU,SAAS,QAAQ;YACtC,MAAM,IAAI,qBAAqB;gBAC7B,WAAW;gBACX,OAAO;gBACP,SAAS;YACX;QACF;QAEA,IAAI,UAAU,MAAM;YAClB,MAAM,IAAI,qBAAqB;gBAC7B,WAAW;gBACX,OAAO;gBACP,SAAS;YACX;QACF;QAEA,IAAI,qBAAqB,MAAM;YAC7B,MAAM,IAAI,qBAAqB;gBAC7B,WAAW;gBACX,OAAO;gBACP,SAAS;YACX;QACF;QAEA,IAAI,cAAc,MAAM;YACtB,MAAM,IAAI,qBAAqB;gBAC7B,WAAW;gBACX,OAAO;gBACP,SAAS;YACX;QACF;QAEA,IAAI,cAAc,MAAM;YACtB,MAAM,IAAI,qBAAqB;gBAC7B,WAAW;gBACX,OAAO;gBACP,SAAS;YACX;QACF;IACF;IAEA,IAAI,WAAW,UAAU;QACvB,IAAI,UAAU,MAAM;YAClB,MAAM,IAAI,qBAAqB;gBAC7B,WAAW;gBACX,OAAO;gBACP,SAAS;YACX;QACF;QAEA,IAAI,cAAc,MAAM;YACtB,MAAM,IAAI,qBAAqB;gBAC7B,WAAW;gBACX,OAAO;gBACP,SAAS;YACX;QACF;IACF;IAEA,IAAI,WAAW,SAAS;QACtB,IAAI,UAAU,MAAM;YAClB,MAAM,IAAI,qBAAqB;gBAC7B,WAAW;gBACX,OAAO;gBACP,SAAS;YACX;QACF;QAEA,IAAI,cAAc,MAAM;YACtB,MAAM,IAAI,qBAAqB;gBAC7B,WAAW;gBACX,OAAO;gBACP,SAAS;YACX;QACF;IACF;IAEA,IAAI,WAAW,QAAQ;QACrB,IAAI,UAAU,MAAM;YAClB,MAAM,IAAI,qBAAqB;gBAC7B,WAAW;gBACX,OAAO;gBACP,SAAS;YACX;QACF;QAEA,IAAI,qBAAqB,MAAM;YAC7B,MAAM,IAAI,qBAAqB;gBAC7B,WAAW;gBACX,OAAO;gBACP,SAAS;YACX;QACF;QAEA,IAAI,cAAc,MAAM;YACtB,MAAM,IAAI,qBAAqB;gBAC7B,WAAW;gBACX,OAAO;gBACP,SAAS;YACX;QACF;QAEA,IAAI,cAAc,MAAM;YACtB,MAAM,IAAI,qBAAqB;gBAC7B,WAAW;gBACX,OAAO;gBACP,SAAS;YACX;QACF;QAEA,KAAA,MAAW,SAAS,WAAY;YAC9B,IAAI,OAAO,UAAU,UAAU;gBAC7B,MAAM,IAAI,qBAAqB;oBAC7B,WAAW;oBACX;oBACA,SAAS;gBACX;YACF;QACF;IACF;AACF;;AtB1HA,IAAM,qBAAqB,CAAA,GAAA,0LAAA,CAAA,oBAAA,EAAkB;IAAE,QAAQ;IAAU,MAAM;AAAG;AA4P1E,eAAsB,eAA+B,EACnD,KAAA,EACA,MAAM,UAAA,EAAA,2CAAA;AACN,QAAQ,WAAA,EACR,UAAA,EACA,iBAAA,EACA,IAAA,EACA,SAAS,QAAA,EACT,MAAA,EACA,MAAA,EACA,QAAA,EACA,UAAA,EACA,WAAA,EACA,OAAA,EACA,wBAAwB,SAAA,EACxB,+BAA+B,gBAAA,EAC/B,WAAW,EACT,YAAAC,cAAa,kBAAA,EACb,cAAc,IAAM,aAAA,GAAA,IAAI,MAAK,EAC/B,GAAI,CAAC,CAAA,EACL,GAAG,UACL;IAjTA,IAAAb;IA+UE,8BAA8B;QAC5B;QACA;QACA,QAAQ;QACR;QACA;QACA;IACF;IAEA,MAAM,iBAAiB,kBAAkB;QACvC;QACA,QAAQ;QACR;IACF;IAGA,IAAI,eAAe,IAAA,KAAS,eAAe,SAAS,KAAA,GAAW;QAC7D,OAAO;IACT;IAEA,MAAM,0BAA0B,2BAA2B;QACzD;QACA;QACA;QACA,UAAU;YAAE,GAAG,QAAA;YAAU;QAAW;IACtC;IAEA,MAAM,SAAS,UAAU;QAAE,WAAA,CAAWA,OAAA,aAAA,OAAA,KAAA,IAAA,UAAW,SAAA,KAAX,OAAAA,OAAwB;IAAM;IACpE,OAAO,WAAW;QAChB,MAAM;QACN,YAAY,0BAA0B;YACpC;YACA,YAAY;gBACV,GAAG,sBAAsB;oBACvB,aAAa;oBACb;gBACF,EAAC;gBACD,GAAG,uBAAA;gBAAA,6DAAA;gBAEH,aAAa;oBACX,OAAO,IAAM,KAAK,SAAA,CAAU;4BAAE;4BAAQ;4BAAQ;wBAAS;gBACzD;gBACA,aACE,eAAe,UAAA,IAAc,OACzB;oBAAE,OAAO,IAAM,KAAK,SAAA,CAAU,eAAe,UAAU;gBAAE,IACzD,KAAA;gBACN,kBAAkB;gBAClB,yBAAyB;gBACzB,sBAAsB,eAAe,IAAA;gBACrC,oBAAoB;YACtB;QACF;QACA;QACA,IAAI,OAAM;YACR,MAAM,QAAQ,4BAA4B;gBAAE;YAAW;YAGvD,IAAI,SAAS,UAAU,QAAQ,MAAM;gBACnC,OAAO,MAAM,2BAAA;YACf;YAEA,IAAI;YACJ,IAAI;YACJ,IAAI;YACJ,IAAI;YACJ,IAAI;YACJ,IAAI;YACJ,IAAI;YACJ,IAAI;YAEJ,OAAQ;gBACN,KAAK;oBAAQ;wBACX,MAAM,kBAAkB,eAAe;4BACrC,QACE,eAAe,UAAA,IAAc,OACzB,sBAAsB;gCAAE,QAAQ;4BAAO,KACvC,MAAM,yBAAA,GACN,SACA,sBAAsB;gCACpB,QAAQ;gCACR,QAAQ,eAAe,UAAA;4BACzB;4BACN;4BACA;wBACF;wBAEA,MAAM,iBAAiB,MAAM,6BAA6B;4BACxD,QAAQ;4BACR,wBAAwB,MAAM,iBAAA;wBAChC;wBAEA,MAAM,cAAc,gBAAgB,IAAA;wBAEpC,MAAM,iBAAiB,MAAM,MAAM,IACjC,WAAW;gCACT,MAAM;gCACN,YAAY,0BAA0B;oCACpC;oCACA,YAAY;wCACV,GAAG,sBAAsB;4CACvB,aAAa;4CACb;wCACF,EAAC;wCACD,GAAG,uBAAA;wCACH,oBAAoB;4CAClB,OAAO,IAAM;wCACf;wCACA,sBAAsB;4CACpB,OAAO,IAAM,KAAK,SAAA,CAAU;wCAC9B;wCACA,oBAAoB;wCAAA,2CAAA;wCAGpB,iBAAiB,MAAM,QAAA;wCACvB,wBAAwB,MAAM,OAAA;wCAC9B,oCAAoC,SAAS,gBAAA;wCAC7C,6BAA6B,SAAS,SAAA;wCACtC,mCAAmC,SAAS,eAAA;wCAC5C,8BAA8B,SAAS,WAAA;wCACvC,wBAAwB,SAAS,IAAA;wCACjC,wBAAwB,SAAS,IAAA;oCACnC;gCACF;gCACA;gCACA,IAAI,OAAMc;oCA3cxB,IAAAd,MAAA,IAAA,IAAA,IAAA,IAAA;oCA4cgB,MAAMe,UAAS,MAAM,MAAM,UAAA,CAAW;wCACpC,MAAM;4CACJ,MAAM;4CACN,QAAQ,eAAe,UAAA;4CACvB,MAAM;4CACN,aAAa;wCACf;wCACA,GAAG,oBAAoB,SAAQ;wCAC/B;wCACA,QAAQ;wCACR;wCACA;wCACA;oCACF;oCAEA,IAAIA,QAAO,IAAA,KAAS,KAAA,GAAW;wCAC7B,MAAM,IAAI;oCACZ;oCAEA,MAAM,eAAe;wCACnB,IAAA,CAAI,KAAA,CAAAf,OAAAe,QAAO,QAAA,KAAP,OAAA,KAAA,IAAAf,KAAiB,EAAA,KAAjB,OAAA,KAAuBa;wCAC3B,WAAA,CAAW,KAAA,CAAA,KAAAE,QAAO,QAAA,KAAP,OAAA,KAAA,IAAA,GAAiB,SAAA,KAAjB,OAAA,KAA8B;wCACzC,SAAA,CAAS,KAAA,CAAA,KAAAA,QAAO,QAAA,KAAP,OAAA,KAAA,IAAA,GAAiB,OAAA,KAAjB,OAAA,KAA4B,MAAM,OAAA;oCAC7C;oCAGAD,MAAK,aAAA,CACH,0BAA0B;wCACxB;wCACA,YAAY;4CACV,4BAA4BC,QAAO,YAAA;4CACnC,sBAAsB;gDAAE,QAAQ,IAAMA,QAAO,IAAA;4CAAK;4CAClD,kBAAkB,aAAa,EAAA;4CAC/B,qBAAqB,aAAa,OAAA;4CAClC,yBACE,aAAa,SAAA,CAAU,WAAA;4CAEzB,yBAAyBA,QAAO,KAAA,CAAM,YAAA;4CACtC,6BACEA,QAAO,KAAA,CAAM,gBAAA;4CAAA,cAAA;4CAGf,mBAAmBA,QAAO,YAAA;4CAC1B,oBAAoB;gDAAE,QAAQ,IAAMA,QAAO,IAAA;4CAAK;4CAAA,2CAAA;4CAGhD,kCAAkC;gDAACA,QAAO,YAAY;6CAAA;4CACtD,sBAAsB,aAAa,EAAA;4CACnC,yBAAyB,aAAa,OAAA;4CACtC,8BAA8BA,QAAO,KAAA,CAAM,YAAA;4CAC3C,kCACEA,QAAO,KAAA,CAAM,gBAAA;wCACjB;oCACF;oCAGF,OAAO;wCAAE,GAAGA,OAAAA;wCAAQ,YAAYA,QAAO,IAAA;wCAAM;oCAAa;gCAC5D;4BACF;wBAGF,SAAS,eAAe,UAAA;wBACxB,eAAe,eAAe,YAAA;wBAC9B,QAAQ,eAAe,KAAA;wBACvB,WAAW,eAAe,QAAA;wBAC1B,cAAc,eAAe,WAAA;wBAC7B,WAAW,eAAe,QAAA;wBAC1B,yBAAyB,eAAe,gBAAA;wBACxC,WAAW,eAAe,YAAA;wBAE1B;oBACF;gBAEA,KAAK;oBAAQ;wBACX,MAAM,kBAAkB,eAAe;4BACrC;4BACA;4BACA;wBACF;wBAEA,MAAM,iBAAiB,MAAM,6BAA6B;4BACxD,QAAQ;4BACR,wBAAwB,MAAM,iBAAA;wBAChC;wBACA,MAAM,cAAc,gBAAgB,IAAA;wBAEpC,MAAM,iBAAiB,MAAM,MAAM,IACjC,WAAW;gCACT,MAAM;gCACN,YAAY,0BAA0B;oCACpC;oCACA,YAAY;wCACV,GAAG,sBAAsB;4CACvB,aAAa;4CACb;wCACF,EAAC;wCACD,GAAG,uBAAA;wCACH,oBAAoB;4CAClB,OAAO,IAAM;wCACf;wCACA,sBAAsB;4CACpB,OAAO,IAAM,KAAK,SAAA,CAAU;wCAC9B;wCACA,oBAAoB;wCAAA,2CAAA;wCAGpB,iBAAiB,MAAM,QAAA;wCACvB,wBAAwB,MAAM,OAAA;wCAC9B,oCAAoC,SAAS,gBAAA;wCAC7C,6BAA6B,SAAS,SAAA;wCACtC,mCAAmC,SAAS,eAAA;wCAC5C,8BAA8B,SAAS,WAAA;wCACvC,wBAAwB,SAAS,IAAA;wCACjC,wBAAwB,SAAS,IAAA;oCACnC;gCACF;gCACA;gCACA,IAAI,OAAMD;oCAjkBxB,IAAAd,MAAA,IAAA,IAAA,IAAA,IAAA,IAAA,IAAA;oCAkkBgB,MAAMe,UAAS,MAAM,MAAM,UAAA,CAAW;wCACpC,MAAM;4CACJ,MAAM;4CACN,MAAM;gDACJ,MAAM;gDACN,MAAM,cAAA,OAAA,aAAc;gDACpB,aACE,qBAAA,OAAA,oBAAqB;gDACvB,YAAY,eAAe,UAAA;4CAC7B;wCACF;wCACA,GAAG,oBAAoB,SAAQ;wCAC/B;wCACA,QAAQ;wCACR;wCACA;wCACA;oCACF;oCAEA,MAAM,aAAA,CAAa,KAAA,CAAAf,OAAAe,QAAO,SAAA,KAAP,OAAA,KAAA,IAAAf,IAAAA,CAAmB,EAAA,KAAnB,OAAA,KAAA,IAAA,GAAuB,IAAA;oCAE1C,IAAI,eAAe,KAAA,GAAW;wCAC5B,MAAM,IAAI;oCACZ;oCAEA,MAAM,eAAe;wCACnB,IAAA,CAAI,KAAA,CAAA,KAAAe,QAAO,QAAA,KAAP,OAAA,KAAA,IAAA,GAAiB,EAAA,KAAjB,OAAA,KAAuBF;wCAC3B,WAAA,CAAW,KAAA,CAAA,KAAAE,QAAO,QAAA,KAAP,OAAA,KAAA,IAAA,GAAiB,SAAA,KAAjB,OAAA,KAA8B;wCACzC,SAAA,CAAS,KAAA,CAAA,KAAAA,QAAO,QAAA,KAAP,OAAA,KAAA,IAAA,GAAiB,OAAA,KAAjB,OAAA,KAA4B,MAAM,OAAA;oCAC7C;oCAGAD,MAAK,aAAA,CACH,0BAA0B;wCACxB;wCACA,YAAY;4CACV,4BAA4BC,QAAO,YAAA;4CACnC,sBAAsB;gDAAE,QAAQ,IAAM;4CAAW;4CACjD,kBAAkB,aAAa,EAAA;4CAC/B,qBAAqB,aAAa,OAAA;4CAClC,yBACE,aAAa,SAAA,CAAU,WAAA;4CAEzB,yBAAyBA,QAAO,KAAA,CAAM,YAAA;4CACtC,6BACEA,QAAO,KAAA,CAAM,gBAAA;4CAAA,cAAA;4CAGf,mBAAmBA,QAAO,YAAA;4CAC1B,oBAAoB;gDAAE,QAAQ,IAAM;4CAAW;4CAAA,2CAAA;4CAG/C,kCAAkC;gDAACA,QAAO,YAAY;6CAAA;4CACtD,sBAAsB,aAAa,EAAA;4CACnC,yBAAyB,aAAa,OAAA;4CACtC,6BAA6BA,QAAO,KAAA,CAAM,YAAA;4CAC1C,8BACEA,QAAO,KAAA,CAAM,gBAAA;wCACjB;oCACF;oCAGF,OAAO;wCAAE,GAAGA,OAAAA;wCAAQ;wCAAY;oCAAa;gCAC/C;4BACF;wBAGF,SAAS,eAAe,UAAA;wBACxB,eAAe,eAAe,YAAA;wBAC9B,QAAQ,eAAe,KAAA;wBACvB,WAAW,eAAe,QAAA;wBAC1B,cAAc,eAAe,WAAA;wBAC7B,WAAW,eAAe,QAAA;wBAC1B,yBAAyB,eAAe,gBAAA;wBACxC,WAAW,eAAe,YAAA;wBAE1B;oBACF;gBAEA,KAAK,KAAA;oBAAW;wBACd,MAAM,IAAI,MACR;oBAEJ;gBAEA;oBAAS;wBACP,MAAM,mBAA0B;wBAChC,MAAM,IAAI,MAAM,CAAA,kBAAA,EAAqB,iBAAgB,CAAE;oBACzD;YACF;YAEA,MAAM,cAAc,CAAA,GAAA,0LAAA,CAAA,gBAAA,EAAc;gBAAE,MAAM;YAAO;YAEjD,IAAI,CAAC,YAAY,OAAA,EAAS;gBACxB,MAAM,YAAY,KAAA;YACpB;YAEA,MAAM,mBAAmB,eAAe,mBAAA,CACtC,YAAY,KAAA;YAGd,IAAI,CAAC,iBAAiB,OAAA,EAAS;gBAC7B,MAAM,iBAAiB,KAAA;YACzB;YAGA,KAAK,aAAA,CACH,0BAA0B;gBACxB;gBACA,YAAY;oBACV,4BAA4B;oBAC5B,sBAAsB;wBACpB,QAAQ,IAAM,KAAK,SAAA,CAAU,iBAAiB,KAAK;oBACrD;oBAEA,yBAAyB,MAAM,YAAA;oBAC/B,6BAA6B,MAAM,gBAAA;oBAAA,cAAA;oBAGnC,mBAAmB;oBACnB,oBAAoB;wBAClB,QAAQ,IAAM,KAAK,SAAA,CAAU,iBAAiB,KAAK;oBACrD;gBACF;YACF;YAGF,OAAO,IAAI,4BAA4B;gBACrC,QAAQ,iBAAiB,KAAA;gBACzB;gBACA,OAAO,4BAA4B;gBACnC;gBACA,UAAU;oBACR,GAAG,QAAA;oBACH,SAAS,eAAA,OAAA,KAAA,IAAA,YAAa,OAAA;gBACxB;gBACA;gBACA,kBAAkB;YACpB;QACF;IACF;AACF;AAEA,IAAM,8BAAN;IAUE,YAAY,OAAA,CAQT;QACD,IAAA,CAAK,MAAA,GAAS,QAAQ,MAAA;QACtB,IAAA,CAAK,YAAA,GAAe,QAAQ,YAAA;QAC5B,IAAA,CAAK,KAAA,GAAQ,QAAQ,KAAA;QACrB,IAAA,CAAK,QAAA,GAAW,QAAQ,QAAA;QACxB,IAAA,CAAK,6BAAA,GAAgC,QAAQ,gBAAA;QAC7C,IAAA,CAAK,QAAA,GAAW,QAAQ,QAAA;QAGxB,IAAA,CAAK,WAAA,GAAc;YACjB,SAAS,QAAQ,QAAA,CAAS,OAAA;QAC5B;QACA,IAAA,CAAK,QAAA,GAAW,QAAQ,QAAA;IAC1B;IAEA,eAAe,IAAA,EAA+B;QAlvBhD,IAAAf;QAmvBI,OAAO,IAAI,SAAS,KAAK,SAAA,CAAU,IAAA,CAAK,MAAM,GAAG;YAC/C,QAAA,CAAQA,OAAA,QAAA,OAAA,KAAA,IAAA,KAAM,MAAA,KAAN,OAAAA,OAAgB;YACxB,SAAS,uBAAuB,MAAM;gBACpC,aAAa;YACf;QACF;IACF;AACF;AAKO,IAAM,8BAA8B;;;AwBtvBpC,SAAS;IAKd,IAAI;IACJ,IAAI;IAEJ,MAAM,UAAU,IAAI,QAAW,CAAC,KAAK;QACnC,UAAU;QACV,SAAS;IACX;IAEA,OAAO;QACL;QACA;QACA;IACF;AACF;;ACtBO,IAAM,iBAAN;IAAA,aAAA;QACL,IAAA,CAAQ,MAAA,GAGmC;YAAE,MAAM;QAAU;QAE7D,IAAA,CAAQ,QAAA,GAA6C,KAAA;QACrD,IAAA,CAAQ,OAAA,GAAkD,KAAA;IAAA;IAE1D,IAAI,QAAoB;QACtB,IAAI,IAAA,CAAK,OAAA,EAAS;YAChB,OAAO,IAAA,CAAK,OAAA;QACd;QAEA,IAAA,CAAK,OAAA,GAAU,IAAI,QAAW,CAAC,SAAS;YACtC,IAAI,IAAA,CAAK,MAAA,CAAO,IAAA,KAAS,YAAY;gBACnC,QAAQ,IAAA,CAAK,MAAA,CAAO,KAAK;YAC3B,OAAA,IAAW,IAAA,CAAK,MAAA,CAAO,IAAA,KAAS,YAAY;gBAC1C,OAAO,IAAA,CAAK,MAAA,CAAO,KAAK;YAC1B;YAEA,IAAA,CAAK,QAAA,GAAW;YAChB,IAAA,CAAK,OAAA,GAAU;QACjB;QAEA,OAAO,IAAA,CAAK,OAAA;IACd;IAEA,QAAQ,KAAA,EAAgB;QAjC1B,IAAAA;QAkCI,IAAA,CAAK,MAAA,GAAS;YAAE,MAAM;YAAY;QAAM;QAExC,IAAI,IAAA,CAAK,OAAA,EAAS;YAChB,CAAAA,OAAA,IAAA,CAAK,QAAA,KAAL,OAAA,KAAA,IAAAA,KAAA,IAAA,CAAA,IAAA,EAAgB;QAClB;IACF;IAEA,OAAO,KAAA,EAAsB;QAzC/B,IAAAA;QA0CI,IAAA,CAAK,MAAA,GAAS;YAAE,MAAM;YAAY;QAAM;QAExC,IAAI,IAAA,CAAK,OAAA,EAAS;YAChB,CAAAA,OAAA,IAAA,CAAK,OAAA,KAAL,OAAA,KAAA,IAAAA,KAAA,IAAA,CAAA,IAAA,EAAe;QACjB;IACF;AACF;;AC/CO,SAAS;IADhB,IAAAA,MAAA;IAEE,OAAA,CAAO,KAAA,CAAAA,OAAA,cAAA,OAAA,KAAA,IAAA,WAAY,WAAA,KAAZ,OAAA,KAAA,IAAAA,KAAyB,GAAA,EAAA,KAAzB,OAAA,KAAkC,KAAK,GAAA;AAChD;;;ACHO,SAAS,2BACd,IAAA,EACA,EACE,WAAA,EACA,iBAAA,EACF;IAEA,MAAM,UAAsD,CAAC;IAE7D,IAAA,CAAI,QAAA,OAAA,KAAA,IAAA,KAAM,OAAA,KAAW,MAAM;QACzB,KAAA,MAAW,CAAC,KAAK,MAAK,IAAK,OAAO,OAAA,CAAQ,KAAK,OAAO,EAAG;YACvD,OAAA,CAAQ,IAAG,GAAI;QACjB;IACF;IAEA,IAAI,OAAA,CAAQ,eAAc,IAAK,MAAM;QACnC,OAAA,CAAQ,eAAc,GAAI;IAC5B;IAEA,IAAI,sBAAsB,KAAA,GAAW;QACnC,OAAA,CAAQ,0BAAyB,GAAI;IACvC;IAEA,OAAO;AACT;;ACnBO,SAAS,sBAAsB,EACpC,QAAA,EACA,MAAA,EACA,UAAA,EACA,OAAA,EACA,MAAA,EACF;IAOE,SAAS,SAAA,CAAU,UAAA,OAAA,SAAU,KAAK,YAAY;IAE9C,MAAM,SAAS,OAAO,SAAA;IACtB,MAAM,OAAO;QACX,IAAI;YACF,MAAO,KAAM;gBACX,MAAM,EAAE,IAAA,EAAM,KAAA,EAAM,GAAI,MAAM,OAAO,IAAA;gBACrC,IAAI,MAAM;gBACV,SAAS,KAAA,CAAM;YACjB;QACF,EAAA,OAAS,OAAO;YACd,MAAM;QACR,SAAE;YACA,SAAS,GAAA;QACX;IACF;IAEA;AACF;;ALmBA,IAAMgB,sBAAqBC,CAAAA,GAAAA,0LAAAA,CAAAA,oBAAAA,EAAkB;IAAE,QAAQ;IAAU,MAAM;AAAG;AAsQ1E,eAAsB,aAAsD,EAC1E,KAAA,EACA,QAAQ,WAAA,EACR,UAAA,EACA,iBAAA,EACA,IAAA,EACA,SAAS,QAAA,EACT,MAAA,EACA,MAAA,EACA,QAAA,EACA,UAAA,EACA,WAAA,EACA,OAAA,EACA,wBAAwB,SAAA,EACxB,+BAA+B,gBAAA,EAC/B,QAAA,EACA,WAAW,EACT,YAAAJ,cAAaG,mBAAAA,EACb,cAAc,IAAM,aAAA,GAAA,IAAI,MAAK,EAC7B,KAAAE,OAAM,GAAA,EACR,GAAI,CAAC,CAAA,EACL,GAAG,UACL;IAnVA,IAAAlB;IA8WE,8BAA8B;QAC5B;QACA;QACA,QAAQ;QACR;QACA;IACF;IAEA,MAAM,iBAAiB,kBAAkB;QAAE;QAAQ,QAAQ;IAAY;IAGvE,IAAI,eAAe,IAAA,KAAS,eAAe,SAAS,KAAA,GAAW;QAC7D,OAAO;IACT;IAEA,MAAM,0BAA0B,2BAA2B;QACzD;QACA;QACA;QACA,UAAU;YAAE,GAAG,QAAA;YAAU;QAAW;IACtC;IAEA,MAAM,SAAS,UAAU;QAAE,WAAA,CAAWA,OAAA,aAAA,OAAA,KAAA,IAAA,UAAW,SAAA,KAAX,OAAAA,OAAwB;IAAM;IAEpE,MAAM,QAAQ,4BAA4B;QAAE;IAAW;IAEvD,OAAO,WAAW;QAChB,MAAM;QACN,YAAY,0BAA0B;YACpC;YACA,YAAY;gBACV,GAAG,sBAAsB;oBACvB,aAAa;oBACb;gBACF,EAAC;gBACD,GAAG,uBAAA;gBAAA,6DAAA;gBAEH,aAAa;oBACX,OAAO,IAAM,KAAK,SAAA,CAAU;4BAAE;4BAAQ;4BAAQ;wBAAS;gBACzD;gBACA,aACE,eAAe,UAAA,IAAc,OACzB;oBAAE,OAAO,IAAM,KAAK,SAAA,CAAU,eAAe,UAAU;gBAAE,IACzD,KAAA;gBACN,kBAAkB;gBAClB,yBAAyB;gBACzB,sBAAsB,eAAe,IAAA;gBACrC,oBAAoB;YACtB;QACF;QACA;QACA,aAAa;QACb,IAAI,OAAM;YAER,IAAI,SAAS,UAAU,QAAQ,MAAM;gBACnC,OAAO,MAAM,2BAAA;YACf;YAEA,IAAI;YACJ,IAAI;YAKJ,OAAQ;gBACN,KAAK;oBAAQ;wBACX,MAAM,kBAAkB,eAAe;4BACrC,QACE,eAAe,UAAA,IAAc,OACzB,sBAAsB;gCAAE,QAAQ;4BAAO,KACvC,MAAM,yBAAA,GACN,SACA,sBAAsB;gCACpB,QAAQ;gCACR,QAAQ,eAAe,UAAA;4BACzB;4BACN;4BACA;wBACF;wBAEA,cAAc;4BACZ,MAAM;gCACJ,MAAM;gCACN,QAAQ,eAAe,UAAA;gCACvB,MAAM;gCACN,aAAa;4BACf;4BACA,GAAG,oBAAoB,SAAQ;4BAC/B,aAAa,gBAAgB,IAAA;4BAC7B,QAAQ,MAAM,6BAA6B;gCACzC,QAAQ;gCACR,wBAAwB,MAAM,iBAAA;4BAChC;4BACA;4BACA;4BACA;wBACF;wBAEA,cAAc;4BACZ,WAAW,CAAC,OAAO;gCACjB,OAAQ,MAAM,IAAA;oCACZ,KAAK;wCACH,WAAW,OAAA,CAAQ,MAAM,SAAS;wCAClC;oCACF,KAAK;oCACL,KAAK;oCACL,KAAK;wCACH,WAAW,OAAA,CAAQ;wCACnB;gCACJ;4BACF;wBACF;wBAEA;oBACF;gBAEA,KAAK;oBAAQ;wBACX,MAAM,kBAAkB,eAAe;4BACrC;4BACA;4BACA;wBACF;wBAEA,cAAc;4BACZ,MAAM;gCACJ,MAAM;gCACN,MAAM;oCACJ,MAAM;oCACN,MAAM,cAAA,OAAA,aAAc;oCACpB,aAAa,qBAAA,OAAA,oBAAqB;oCAClC,YAAY,eAAe,UAAA;gCAC7B;4BACF;4BACA,GAAG,oBAAoB,SAAQ;4BAC/B,aAAa,gBAAgB,IAAA;4BAC7B,QAAQ,MAAM,6BAA6B;gCACzC,QAAQ;gCACR,wBAAwB,MAAM,iBAAA;4BAChC;4BACA;4BACA;4BACA;wBACF;wBAEA,cAAc;4BACZ,WAAU,KAAA,EAAO,UAAA;gCACf,OAAQ,MAAM,IAAA;oCACZ,KAAK;wCACH,WAAW,OAAA,CAAQ,MAAM,aAAa;wCACtC;oCACF,KAAK;oCACL,KAAK;oCACL,KAAK;wCACH,WAAW,OAAA,CAAQ;wCACnB;gCACJ;4BACF;wBACF;wBAEA;oBACF;gBAEA,KAAK,KAAA;oBAAW;wBACd,MAAM,IAAI,MACR;oBAEJ;gBAEA;oBAAS;wBACP,MAAM,mBAA0B;wBAChC,MAAM,IAAI,MAAM,CAAA,kBAAA,EAAqB,iBAAgB,CAAE;oBACzD;YACF;YAEA,MAAM,EACJ,QAAQ,EAAE,MAAA,EAAQ,QAAA,EAAU,WAAA,EAAY,EACxC,YAAA,EACA,gBAAA,EACF,GAAI,MAAM,MAAM,IACd,WAAW;oBACT,MAAM;oBACN,YAAY,0BAA0B;wBACpC;wBACA,YAAY;4BACV,GAAG,sBAAsB;gCACvB,aAAa;gCACb;4BACF,EAAC;4BACD,GAAG,uBAAA;4BACH,oBAAoB;gCAClB,OAAO,IAAM,YAAY,WAAA;4BAC3B;4BACA,sBAAsB;gCACpB,OAAO,IAAM,KAAK,SAAA,CAAU,YAAY,MAAM;4BAChD;4BACA,oBAAoB;4BAAA,2CAAA;4BAGpB,iBAAiB,MAAM,QAAA;4BACvB,wBAAwB,MAAM,OAAA;4BAC9B,oCAAoC,SAAS,gBAAA;4BAC7C,6BAA6B,SAAS,SAAA;4BACtC,mCAAmC,SAAS,eAAA;4BAC5C,8BAA8B,SAAS,WAAA;4BACvC,wBAAwB,SAAS,IAAA;4BACjC,wBAAwB,SAAS,IAAA;wBACnC;oBACF;oBACA;oBACA,aAAa;oBACb,IAAI,OAAMmB,gBAAAA,CAAiB;4BACzB,kBAAkBD;4BAClB,cAAAC;4BACA,QAAQ,MAAM,MAAM,QAAA,CAAS;wBAC/B,CAAA;gBACF;YAGF,OAAO,IAAI,0BAA2D;gBACpE;gBACA,QAAQ,OAAO,WAAA,CAAY,IAAI,gBAAgB;gBAC/C;gBACA;gBACA;gBACA;gBACA;gBACA;gBACA;gBACA,SAAS,MAAM,OAAA;gBACf,KAAAD;gBACA;gBACA,YAAAL;YACF;QACF;IACF;AACF;AAEA,IAAM,4BAAN;IA6BE,YAAY,EACV,MAAA,EACA,QAAA,EACA,WAAA,EACA,cAAA,EACA,QAAA,EACA,QAAA,EACA,YAAA,EACA,SAAA,EACA,gBAAA,EACA,OAAA,EACA,KAAAK,IAAAA,EACA,WAAA,EACA,YAAAL,WAAAA,EACF,CAoBG;QACD,IAAA,CAAK,QAAA,GAAW;QAChB,IAAA,CAAK,WAAA,GAAc;QACnB,IAAA,CAAK,cAAA,GAAiB;QAGtB,IAAA,CAAK,aAAA,GAAgB,IAAI;QAGzB,MAAM,EAAE,SAAS,YAAA,EAAc,SAAS,YAAA,EAAa,GACnD;QACF,IAAA,CAAK,KAAA,GAAQ;QAGb,MAAM,EAAE,SAAS,eAAA,EAAiB,SAAS,eAAA,EAAgB,GACzD;QACF,IAAA,CAAK,QAAA,GAAW;QAGhB,MAAM,EACJ,SAAS,uBAAA,EACT,SAAS,uBAAA,EACX,GAAI;QACJ,IAAA,CAAK,6BAAA,GAAgC;QAGrC,IAAI;QACJ,IAAI;QACJ,IAAI;QACJ,IAAI;QACJ,IAAI;QAGJ,IAAI,kBAAkB;QACtB,IAAI,YAAY;QAChB,IAAI,WAIA;YACF,IAAIA;YACJ,WAAW;YACX;QACF;QAIA,IAAI,mBAA0C,KAAA;QAC9C,IAAI,eAAoC,KAAA;QACxC,IAAI,eAAe;QACnB,IAAI,eAAe;QAEnB,MAAM,OAAO,IAAA;QACb,IAAA,CAAK,cAAA,GAAiB,OAAO,WAAA,CAC3B,IAAI,gBAGF;YACA,MAAM,WAAU,KAAA,EAAO,UAAA;gBAptB/B,IAAAb,MAAA,IAAA;gBAstBU,IAAI,cAAc;oBAChB,MAAM,iBAAiBkB,SAAQ;oBAE/B,eAAe;oBAEf,aAAa,QAAA,CAAS,wBAAwB;wBAC5C,4BAA4B;oBAC9B;oBAEA,aAAa,aAAA,CAAc;wBACzB,4BAA4B;oBAC9B;gBACF;gBAGA,IAAI,OAAO,UAAU,UAAU;oBAC7B,mBAAmB;oBACnB,aAAa;oBAEb,MAAM,EAAE,OAAO,iBAAA,EAAmB,OAAO,UAAA,EAAW,GAClD,CAAA,GAAA,8KAAA,CAAA,mBAAA,EAAiB;oBAEnB,IACE,sBAAsB,KAAA,KACtB,CAAC,CAAA,GAAA,8KAAA,CAAA,kBAAA,EAAgB,kBAAkB,oBACnC;wBACA,MAAM,mBAAmB,eAAe,qBAAA,CAAsB;4BAC5D,OAAO;4BACP;4BACA;4BACA;4BACA,cAAc,eAAe;wBAC/B;wBAEA,IACE,iBAAiB,OAAA,IACjB,CAAC,CAAA,GAAA,8KAAA,CAAA,kBAAA,EAAgB,cAAc,iBAAiB,KAAA,CAAM,OAAO,GAC7D;4BAEA,mBAAmB;4BACnB,eAAe,iBAAiB,KAAA,CAAM,OAAA;4BAEtC,WAAW,OAAA,CAAQ;gCACjB,MAAM;gCACN,QAAQ;4BACV;4BAEA,WAAW,OAAA,CAAQ;gCACjB,MAAM;gCACN,WAAW,iBAAiB,KAAA,CAAM,SAAA;4BACpC;4BAEA,YAAY;4BACZ,eAAe;wBACjB;oBACF;oBAEA;gBACF;gBAEA,OAAQ,MAAM,IAAA;oBACZ,KAAK;wBAAqB;4BACxB,WAAW;gCACT,IAAA,CAAIlB,OAAA,MAAM,EAAA,KAAN,OAAAA,OAAY,SAAS,EAAA;gCACzB,WAAA,CAAW,KAAA,MAAM,SAAA,KAAN,OAAA,KAAmB,SAAS,SAAA;gCACvC,SAAA,CAAS,KAAA,MAAM,OAAA,KAAN,OAAA,KAAiB,SAAS,OAAA;4BACrC;4BACA;wBACF;oBAEA,KAAK;wBAAU;4BAEb,IAAI,cAAc,IAAI;gCACpB,WAAW,OAAA,CAAQ;oCAAE,MAAM;oCAAc;gCAAU;4BACrD;4BAGA,eAAe,MAAM,YAAA;4BAGrB,QAAQ,4BAA4B,MAAM,KAAK;4BAC/C,mBAAmB,MAAM,gBAAA;4BAEzB,WAAW,OAAA,CAAQ;gCAAE,GAAG,KAAA;gCAAO;gCAAO;4BAAS;4BAG/C,aAAa;4BACb,wBAAwB;4BACxB,gBAAgB;gCACd,GAAG,QAAA;gCACH,SAAS,eAAA,OAAA,KAAA,IAAA,YAAa,OAAA;4BACxB;4BAGA,MAAM,mBACJ,eAAe,mBAAA,CAAoB;4BAErC,IAAI,iBAAiB,OAAA,EAAS;gCAC5B,SAAS,iBAAiB,KAAA;gCAC1B,KAAK,aAAA,CAAc,OAAA,CAAQ;4BAC7B,OAAO;gCACL,QAAQ,iBAAiB,KAAA;gCACzB,KAAK,aAAA,CAAc,MAAA,CAAO;4BAC5B;4BAEA;wBACF;oBAEA;wBAAS;4BACP,WAAW,OAAA,CAAQ;4BACnB;wBACF;gBACF;YACF;YAAA,8FAAA;YAGA,MAAM,OAAM,UAAA;gBACV,IAAI;oBACF,MAAM,aAAa,SAAA,OAAA,QAAS;wBAC1B,cAAc;wBACd,kBAAkB;wBAClB,aAAa;oBACf;oBAEA,aAAa,aAAA,CACX,0BAA0B;wBACxB;wBACA,YAAY;4BACV,4BAA4B;4BAC5B,sBAAsB;gCACpB,QAAQ,IAAM,KAAK,SAAA,CAAU;4BAC/B;4BACA,kBAAkB,SAAS,EAAA;4BAC3B,qBAAqB,SAAS,OAAA;4BAC9B,yBAAyB,SAAS,SAAA,CAAU,WAAA;4BAE5C,yBAAyB,WAAW,YAAA;4BACpC,6BAA6B,WAAW,gBAAA;4BAAA,aAAA;4BAGxC,mBAAmB;4BACnB,oBAAoB;gCAAE,QAAQ,IAAM,KAAK,SAAA,CAAU;4BAAQ;4BAAA,2CAAA;4BAG3D,kCAAkC;gCAAC;6BAAY;4BAC/C,sBAAsB,SAAS,EAAA;4BAC/B,yBAAyB,SAAS,OAAA;4BAClC,6BAA6B,WAAW,YAAA;4BACxC,8BAA8B,WAAW,gBAAA;wBAC3C;oBACF;oBAIF,aAAa,GAAA;oBAGb,SAAS,aAAA,CACP,0BAA0B;wBACxB;wBACA,YAAY;4BACV,yBAAyB,WAAW,YAAA;4BACpC,6BAA6B,WAAW,gBAAA;4BACxC,sBAAsB;gCACpB,QAAQ,IAAM,KAAK,SAAA,CAAU;4BAC/B;4BAAA,aAAA;4BAGA,oBAAoB;gCAAE,QAAQ,IAAM,KAAK,SAAA,CAAU;4BAAQ;wBAC7D;oBACF;oBAIF,MAAA,CAAM,YAAA,OAAA,KAAA,IAAA,SAAW;wBACf,OAAO;wBACP;wBACA;wBACA;wBACA,UAAU;4BACR,GAAG,QAAA;4BACH,SAAS,eAAA,OAAA,KAAA,IAAA,YAAa,OAAA;wBACxB;wBACA;wBACA,+BAA+B;oBACjC,EAAA;gBACF,EAAA,OAASoB,QAAO;oBACd,WAAW,KAAA,CAAMA;gBACnB,SAAE;oBACA,SAAS,GAAA;gBACX;YACF;QACF;IAEJ;IAEA,IAAI,SAA0B;QAC5B,OAAO,IAAA,CAAK,aAAA,CAAc,KAAA;IAC5B;IAEA,IAAI,sBAAoD;QACtD,OAAO,0BAA0B,IAAA,CAAK,cAAA,EAAgB;YACpD,WAAU,KAAA,EAAO,UAAA;gBACf,OAAQ,MAAM,IAAA;oBACZ,KAAK;wBACH,WAAW,OAAA,CAAQ,MAAM,MAAM;wBAC/B;oBAEF,KAAK;oBACL,KAAK;wBACH;oBAEF,KAAK;wBACH,WAAW,KAAA,CAAM,MAAM,KAAK;wBAC5B;oBAEF;wBAAS;4BACP,MAAM,mBAA0B;4BAChC,MAAM,IAAI,MAAM,CAAA,wBAAA,EAA2B,iBAAgB,CAAE;wBAC/D;gBACF;YACF;QACF;IACF;IAEA,IAAI,gBAAgC;QAClC,OAAO,IAAA,CAAK,cAAA,CAAe,mBAAA,CAAoB,IAAA,CAAK,cAAc;IACpE;IAEA,IAAI,aAA0C;QAC5C,OAAO,0BAA0B,IAAA,CAAK,cAAA,EAAgB;YACpD,WAAU,KAAA,EAAO,UAAA;gBACf,OAAQ,MAAM,IAAA;oBACZ,KAAK;wBACH,WAAW,OAAA,CAAQ,MAAM,SAAS;wBAClC;oBAEF,KAAK;oBACL,KAAK;wBACH;oBAEF,KAAK;wBACH,WAAW,KAAA,CAAM,MAAM,KAAK;wBAC5B;oBAEF;wBAAS;4BACP,MAAM,mBAA0B;4BAChC,MAAM,IAAI,MAAM,CAAA,wBAAA,EAA2B,iBAAgB,CAAE;wBAC/D;gBACF;YACF;QACF;IACF;IAEA,IAAI,aAA6D;QAC/D,OAAO,0BAA0B,IAAA,CAAK,cAAA,EAAgB;YACpD,WAAU,KAAA,EAAO,UAAA;gBACf,WAAW,OAAA,CAAQ;YACrB;QACF;IACF;IAEA,yBAAyB,QAAA,EAA0B,IAAA,EAAqB;QACtE,sBAAsB;YACpB;YACA,QAAQ,QAAA,OAAA,KAAA,IAAA,KAAM,MAAA;YACd,YAAY,QAAA,OAAA,KAAA,IAAA,KAAM,UAAA;YAClB,SAAS,2BAA2B,MAAM;gBACxC,aAAa;YACf;YACA,QAAQ,IAAA,CAAK,UAAA,CAAW,WAAA,CAAY,IAAI;QAC1C;IACF;IAEA,qBAAqB,IAAA,EAA+B;QAx+BtD,IAAApB;QAy+BI,OAAO,IAAI,SAAS,IAAA,CAAK,UAAA,CAAW,WAAA,CAAY,IAAI,sBAAsB;YACxE,QAAA,CAAQA,OAAA,QAAA,OAAA,KAAA,IAAA,KAAM,MAAA,KAAN,OAAAA,OAAgB;YACxB,SAAS,uBAAuB,MAAM;gBACpC,aAAa;YACf;QACF;IACF;AACF;AAKO,IAAM,4BAA4B;;;;AQn/BzC,IAAMC,QAAO;AACb,IAAMK,UAAS,CAAA,gBAAA,EAAmBL,MAAI,CAAA;AACtC,IAAMM,UAAS,OAAO,GAAA,CAAID;AAJ1B,IAAAN;AAMO,IAAM,4BAAN,cAAwCQ,iLAAAA,CAAAA,aAAAA;IAM7C,YAAY,EACV,QAAA,EACA,QAAA,EACA,KAAA,EACA,UAAU,CAAA,2BAAA,EAA8B,SAAQ,EAAA,EAAKC,CAAAA,GAAAA,iLAAAA,CAAAA,kBAAAA,EACnD,OACD,CAAA,EACH,CAKG;QACD,KAAA,CAAM;YAAE,MAAAR;YAAM;YAAS;QAAM;QAlB/B,IAAA,CAAkBD,IAAAA,GAAU;QAoB1B,IAAA,CAAK,QAAA,GAAW;QAChB,IAAA,CAAK,QAAA,GAAW;IAClB;IAEA,OAAO,WAAW,KAAA,EAAoD;QACpE,OAAOQ,iLAAAA,CAAAA,aAAAA,CAAW,SAAA,CAAU,OAAOF;IACrC;IAAA;;GAAA,GAKA,OAAO,4BACL,KAAA,EACoC;QACpC,OACE,iBAAiB,SACjB,MAAM,IAAA,KAASL,SACf,OAAQ,MAAoC,QAAA,KAAa,YACzD,OAAQ,MAAoC,QAAA,KAAa;IAE7D;IAAA;;GAAA,GAKA,SAAS;QACP,OAAO;YACL,MAAM,IAAA,CAAK,IAAA;YACX,SAAS,IAAA,CAAK,OAAA;YACd,OAAO,IAAA,CAAK,KAAA;YACZ,OAAO,IAAA,CAAK,KAAA;YAEZ,UAAU,IAAA,CAAK,QAAA;YACf,UAAU,IAAA,CAAK,QAAA;QACjB;IACF;AACF;AAxDoBD,MAAAO;;ACLpB,IAAMN,QAAO;AACb,IAAMK,UAAS,CAAA,gBAAA,EAAmBL,MAAI,CAAA;AACtC,IAAMM,UAAS,OAAO,GAAA,CAAID;AAJ1B,IAAAN;AAMO,IAAM,kBAAN,cAA8BQ,iLAAAA,CAAAA,aAAAA;IAMnC,YAAY,EACV,QAAA,EACA,iBAAiB,KAAA,CAAA,EACjB,UAAU,CAAA,sCAAA,EAAyC,SAAQ,GAAA,EACzD,mBAAmB,KAAA,IACf,4BACA,CAAA,iBAAA,EAAoB,eAAe,IAAA,CAAK,MAAK,CAAA,CACnD,CAAA,CAAA,EACF,CAIG;QACD,KAAA,CAAM;YAAE,MAAAP;YAAM;QAAQ;QAlBxB,IAAA,CAAkBD,IAAAA,GAAU;QAoB1B,IAAA,CAAK,QAAA,GAAW;QAChB,IAAA,CAAK,cAAA,GAAiB;IACxB;IAEA,OAAO,WAAW,KAAA,EAA0C;QAC1D,OAAOQ,iLAAAA,CAAAA,aAAAA,CAAW,SAAA,CAAU,OAAOF;IACrC;IAAA;;GAAA,GAKA,OAAO,kBAAkB,KAAA,EAA0C;QACjE,OACE,iBAAiB,SACjB,MAAM,IAAA,KAASL,SACf,cAAc,SACd,MAAM,QAAA,IAAY,KAAA,KAClB,OAAO,MAAM,IAAA,KAAS;IAE1B;IAAA;;GAAA,GAKA,SAAS;QACP,OAAO;YACL,MAAM,IAAA,CAAK,IAAA;YACX,SAAS,IAAA,CAAK,OAAA;YACd,OAAO,IAAA,CAAK,KAAA;YAEZ,UAAU,IAAA,CAAK,QAAA;YACf,gBAAgB,IAAA,CAAK,cAAA;QACvB;IACF;AACF;AAtDoBD,MAAAO;;ACJpB,IAAMN,QAAO;AACb,IAAMK,UAAS,CAAA,gBAAA,EAAmBL,MAAI,CAAA;AACtC,IAAMM,UAAS,OAAO,GAAA,CAAID;AAL1B,IAAAN;AAOO,IAAM,yBAAN,cAAqCQ,iLAAAA,CAAAA,aAAAA;IAK1C,YAAY,EACV,eAAA,EACA,OAAA,EACF,CAGG;QACD,KAAA,CAAM;YAAE,MAAAP;YAAM;QAAQ;QAXxB,IAAA,CAAkBD,IAAAA,GAAU;QAa1B,IAAA,CAAK,eAAA,GAAkB;IACzB;IAEA,OAAO,WAAW,KAAA,EAAiD;QACjE,OAAOQ,iLAAAA,CAAAA,aAAAA,CAAW,SAAA,CAAU,OAAOF;IACrC;AACF;AAnBoBN,MAAAO;;;AERb,SAAS,iBACd,MAAA;IAEA,OAAO,UAAU,QAAQ,OAAO,IAAA,CAAK,QAAQ,MAAA,GAAS;AACxD;;ADKO,SAAS,0BAEd,EACA,KAAA,EACA,UAAA,EACF;IAOE,IAAI,CAAC,iBAAiB,QAAQ;QAC5B,OAAO;YACL,OAAO,KAAA;YACP,YAAY,KAAA;QACd;IACF;IAEA,OAAO;QACL,OAAO,OAAO,OAAA,CAAQ,OAAO,GAAA,CAAI,CAAC,CAACN,QAAMoB,MAAI,GAAA,CAAO;gBAClD,MAAM;gBACN,MAAApB;gBACA,aAAaoB,MAAK,WAAA;gBAClB,YAAYC,CAAAA,GAAAA,8KAAAA,CAAAA,WAAAA,EAASD,MAAK,UAAU,EAAE,UAAA;YACxC,CAAA;QACA,YACE,cAAc,OACV;YAAE,MAAM;QAAO,IACf,OAAO,eAAe,WACtB;YAAE,MAAM;QAAW,IACnB;YAAE,MAAM;YAAiB,UAAU,WAAW,QAAA;QAAmB;IACzE;AACF;;AE1CA,IAAM,uBAAuB;AAWtB,SAAS,sBAAsB,IAAA;IAOpC,MAAM,QAAQ,KAAK,KAAA,CAAM;IACzB,OAAO,QACH;QAAE,QAAQ,KAAA,CAAM,EAAC;QAAG,YAAY,KAAA,CAAM,EAAC;QAAG,QAAQ,KAAA,CAAM,EAAC;IAAE,IAC3D,KAAA;AACN;;ACpBO,SAAS,8BAA8B,IAAA;IAC5C,MAAM,QAAQ,sBAAsB;IACpC,OAAO,QAAQ,MAAM,MAAA,GAAS,MAAM,UAAA,GAAa;AACnD;;;ACIO,SAAS,cAAsD,EACpE,QAAA,EACA,KAAA,EACF;IAIE,MAAM,WAAW,SAAS,QAAA;IAE1B,IAAI,SAAS,MAAM;QACjB,MAAM,IAAI,gBAAgB;YAAE,UAAU,SAAS,QAAA;QAAS;IAC1D;IAEA,MAAMA,QAAO,KAAA,CAAM,SAAQ;IAE3B,IAAIA,SAAQ,MAAM;QAChB,MAAM,IAAI,gBAAgB;YACxB,UAAU,SAAS,QAAA;YACnB,gBAAgB,OAAO,IAAA,CAAK;QAC9B;IACF;IAEA,MAAM,SAASC,CAAAA,GAAAA,8KAAAA,CAAAA,WAAAA,EAASD,MAAK,UAAU;IAMvC,MAAM,cACJ,SAAS,IAAA,CAAK,IAAA,OAAW,KACrBT,CAAAA,GAAAA,0LAAAA,CAAAA,oBAAAA,EAAkB;QAAE,OAAO,CAAC;QAAG;IAAO,KACtCW,CAAAA,GAAAA,0LAAAA,CAAAA,gBAAAA,EAAc;QAAE,MAAM,SAAS,IAAA;QAAM;IAAO;IAElD,IAAI,YAAY,OAAA,KAAY,OAAO;QACjC,MAAM,IAAI,0BAA0B;YAClC;YACA,UAAU,SAAS,IAAA;YACnB,OAAO,YAAY,KAAA;QACrB;IACF;IAEA,OAAO;QACL,MAAM;QACN,YAAY,SAAS,UAAA;QACrB;QACA,MAAM,YAAY,KAAA;IACpB;AACF;;AChDO,SAAS,mBAA2D,EACzE,OAAO,EAAA,EACP,SAAA,EACA,WAAA,EACF;IAKE,MAAM,mBAAkE,EAAC;IAEzE,iBAAiB,IAAA,CAAK;QACpB,MAAM;QACN,SAAS;YAAC;gBAAE,MAAM;gBAAQ;YAAK;eAAM;SAAS;IAChD;IAEA,IAAI,YAAY,MAAA,GAAS,GAAG;QAC1B,iBAAiB,IAAA,CAAK;YACpB,MAAM;YACN,SAAS,YAAY,GAAA,CAAI,CAAA,SAAA,CAAW;oBAClC,MAAM;oBACN,YAAY,OAAO,UAAA;oBACnB,UAAU,OAAO,QAAA;oBACjB,QAAQ,OAAO,MAAA;gBACjB,CAAA;QACF;IACF;IAEA,OAAO;AACT;;AVHA,IAAMP,sBAAqBC,CAAAA,GAAAA,0LAAAA,CAAAA,oBAAAA,EAAkB;IAAE,QAAQ;IAAU,MAAM;AAAG;AAgD1E,eAAsB,aAAqD,EACzE,KAAA,EACA,KAAA,EACA,UAAA,EACA,MAAA,EACA,MAAA,EACA,QAAA,EACA,UAAA,EACA,WAAA,EACA,OAAA,EACA,yBAAyB,CAAA,EACzB,oBAAoB,sBAAA,EACpB,WAAW,qBAAqB,OAAO,oBAAoB,IAAI,CAAA,EAC/D,8BAAA,EACA,4BAA4B,gBAAgB,kCAAA,OAAA,iCAC1C,KAAA,EACF,wBAAwB,SAAA,EACxB,+BAA+B,gBAAA,EAC/B,WAAW,EACT,YAAAJ,cAAaG,mBAAAA,EACb,cAAc,IAAM,aAAA,GAAA,IAAI,MAAK,EAC/B,GAAI,CAAC,CAAA,EACL,YAAA,EACA,GAAG,UACL;IA1GA,IAAAhB;IA8LE,IAAI,WAAW,GAAG;QAChB,MAAM,IAAI,qBAAqB;YAC7B,WAAW;YACX,OAAO;YACP,SAAS;QACX;IACF;IAEA,MAAM,0BAA0B,2BAA2B;QACzD;QACA;QACA;QACA,UAAU;YAAE,GAAG,QAAA;YAAU;QAAW;IACtC;IAEA,MAAM,SAAS,UAAU;QAAE,WAAA,CAAWA,OAAA,aAAA,OAAA,KAAA,IAAA,UAAW,SAAA,KAAX,OAAAA,OAAwB;IAAM;IACpE,OAAO,WAAW;QAChB,MAAM;QACN,YAAY,0BAA0B;YACpC;YACA,YAAY;gBACV,GAAG,sBAAsB;oBACvB,aAAa;oBACb;gBACF,EAAC;gBACD,GAAG,uBAAA;gBAAA,6DAAA;gBAEH,aAAa;oBACX,OAAO,IAAM,KAAK,SAAA,CAAU;4BAAE;4BAAQ;4BAAQ;wBAAS;gBACzD;gBACA,wBAAwB;YAC1B;QACF;QACA;QACA,IAAI,OAAM;YAhOd,IAAAA,MAAA,IAAA,IAAA,IAAA;YAiOM,MAAM,QAAQ,4BAA4B;gBAAE;YAAW;YACvD,MAAM,kBAAkB,eAAe;gBACrC;gBACA;gBACA;YACF;YAEA,MAAM,OAAO;gBACX,MAAM;gBACN,GAAG,0BAA0B;oBAAE;oBAAO;gBAAW,EAAC;YACpD;YACA,MAAM,eAAe,oBAAoB;YACzC,MAAM,iBAAiB,MAAM,6BAA6B;gBACxD,QAAQ;gBACR,wBAAwB,MAAM,iBAAA;YAChC;YAEA,IAAI;YAGJ,IAAI,mBAA2C,EAAC;YAChD,IAAI,qBAA+C,EAAC;YACpD,IAAI,YAAY;YAChB,MAAM,mBACJ,EAAC;YACH,IAAI,OAAO;YACX,MAAM,QAA4C,EAAC;YACnD,MAAM,QAA4B;gBAChC,kBAAkB;gBAClB,cAAc;gBACd,aAAa;YACf;YAEA,IAAI,WAA4D;YAEhE,GAAG;gBAED,MAAM,qBACJ,cAAc,IAAI,gBAAgB,IAAA,GAAO;gBAE3C,uBAAuB,MAAM,MAAM,IACjC,WAAW;wBACT,MAAM;wBACN,YAAY,0BAA0B;4BACpC;4BACA,YAAY;gCACV,GAAG,sBAAsB;oCACvB,aAAa;oCACb;gCACF,EAAC;gCACD,GAAG,uBAAA;gCACH,oBAAoB;oCAAE,OAAO,IAAM;gCAAmB;gCACtD,sBAAsB;oCACpB,OAAO,IAAM,KAAK,SAAA,CAAU;gCAC9B;gCAAA,2CAAA;gCAGA,iBAAiB,MAAM,QAAA;gCACvB,wBAAwB,MAAM,OAAA;gCAC9B,oCAAoC,SAAS,gBAAA;gCAC7C,6BAA6B,SAAS,SAAA;gCACtC,mCAAmC,SAAS,eAAA;gCAC5C,iCAAiC,SAAS,aAAA;gCAC1C,8BAA8B,SAAS,WAAA;gCACvC,wBAAwB,SAAS,IAAA;gCACjC,wBAAwB,SAAS,IAAA;4BACnC;wBACF;wBACA;wBACA,IAAI,OAAMc;4BAtStB,IAAAd,MAAAwB,KAAAC,KAAAC,KAAAC,KAAA;4BAuSc,MAAM,SAAS,MAAM,MAAM,UAAA,CAAW;gCACpC;gCACA,GAAG,YAAA;gCACH,aAAa;gCACb,QAAQ;gCACR;gCACA;gCACA;4BACF;4BAGA,MAAM,eAAe;gCACnB,IAAA,CAAIH,MAAAA,CAAAxB,OAAA,OAAO,QAAA,KAAP,OAAA,KAAA,IAAAA,KAAiB,EAAA,KAAjB,OAAAwB,MAAuBX;gCAC3B,WAAA,CAAWa,MAAAA,CAAAD,MAAA,OAAO,QAAA,KAAP,OAAA,KAAA,IAAAA,IAAiB,SAAA,KAAjB,OAAAC,MAA8B;gCACzC,SAAA,CAAS,KAAA,CAAAC,MAAA,OAAO,QAAA,KAAP,OAAA,KAAA,IAAAA,IAAiB,OAAA,KAAjB,OAAA,KAA4B,MAAM,OAAA;4BAC7C;4BAGAb,MAAK,aAAA,CACH,0BAA0B;gCACxB;gCACA,YAAY;oCACV,4BAA4B,OAAO,YAAA;oCACnC,oBAAoB;wCAClB,QAAQ,IAAM,OAAO,IAAA;oCACvB;oCACA,yBAAyB;wCACvB,QAAQ,IAAM,KAAK,SAAA,CAAU,OAAO,SAAS;oCAC/C;oCACA,kBAAkB,aAAa,EAAA;oCAC/B,qBAAqB,aAAa,OAAA;oCAClC,yBACE,aAAa,SAAA,CAAU,WAAA;oCAEzB,yBAAyB,OAAO,KAAA,CAAM,YAAA;oCACtC,6BAA6B,OAAO,KAAA,CAAM,gBAAA;oCAAA,cAAA;oCAG1C,mBAAmB,OAAO,YAAA;oCAC1B,kBAAkB;wCAChB,QAAQ,IAAM,OAAO,IAAA;oCACvB;oCACA,uBAAuB;wCACrB,QAAQ,IAAM,KAAK,SAAA,CAAU,OAAO,SAAS;oCAC/C;oCAAA,2CAAA;oCAGA,kCAAkC;wCAAC,OAAO,YAAY;qCAAA;oCACtD,sBAAsB,aAAa,EAAA;oCACnC,yBAAyB,aAAa,OAAA;oCACtC,6BAA6B,OAAO,KAAA,CAAM,YAAA;oCAC1C,8BAA8B,OAAO,KAAA,CAAM,gBAAA;gCAC7C;4BACF;4BAGF,OAAO;gCAAE,GAAG,MAAA;gCAAQ,UAAU;4BAAa;wBAC7C;oBACF;gBAIF,mBAAA,CAAA,CAAoBd,OAAA,qBAAqB,SAAA,KAArB,OAAAA,OAAkC,EAAC,EAAG,GAAA,CACxD,CAAA,gBAAiB,cAAc;wBAAE,UAAU;wBAAe;oBAAM;gBAIlE,qBACE,SAAS,OACL,EAAC,GACD,MAAM,aAAa;oBACjB,WAAW;oBACX;oBACA;oBACA;gBACF;gBAGN,MAAM,eAAe,4BACnB,qBAAqB,KAAA;gBAEvB,MAAM,gBAAA,IAAoB,aAAa,gBAAA;gBACvC,MAAM,YAAA,IAAgB,aAAa,YAAA;gBACnC,MAAM,WAAA,IAAe,aAAa,WAAA;gBAGlC,IAAI,eAAoD;gBACxD,IAAI,EAAE,YAAY,UAAU;oBAC1B,IACE,iBACA,qBAAqB,YAAA,KAAiB,YAAA,kDAAA;oBAEtC,iBAAiB,MAAA,KAAW,GAC5B;wBACA,eAAe;oBACjB,OAAA,IAAA,wBAAA;oBAEE,iBAAiB,MAAA,GAAS,KAAA,uCAAA;oBAE1B,mBAAmB,MAAA,KAAW,iBAAiB,MAAA,EAC/C;wBACA,eAAe;oBACjB;gBACF;gBAGA,MAAM,WACJ,iBAAiB,aACb,8BAAA,CAA8B,KAAA,qBAAqB,IAAA,KAArB,OAAA,KAA6B,MAAE,CAC7D,KAAA,qBAAqB,IAAA,KAArB,OAAA,KAA6B;gBAGnC,OACE,iBAAiB,cAAc,aAAa,aACxC,OAAO,WACP;gBAGN,MAAM,cAAiC;oBACrC;oBACA,MAAM;oBACN,WAAW;oBACX,aAAa;oBACb,cAAc,qBAAqB,YAAA;oBACnC,OAAO;oBACP,UAAU,qBAAqB,QAAA;oBAC/B,UAAU,qBAAqB,QAAA;oBAC/B,UAAU;wBACR,GAAG,qBAAqB,QAAA;wBACxB,SAAA,CAAS,KAAA,qBAAqB,WAAA,KAArB,OAAA,KAAA,IAAA,GAAkC,OAAA;oBAC7C;oBACA,+BAA+B,qBAAqB,gBAAA;oBACpD,aAAa,iBAAiB;gBAChC;gBACA,MAAM,IAAA,CAAK;gBACX,MAAA,CAAM,gBAAA,OAAA,KAAA,IAAA,aAAe,YAAA;gBAGrB,IAAI,aAAa,YAAY;oBAI3B,MAAM,sBACJ,iBAAiB,GAAA;oBACnB,eAAe,GAAA;oBACf,IAAI,OAAO,oBAAoB,OAAA,KAAY,UAAU;wBACnD,oBAAoB,OAAA,GAAU;oBAChC,OAAO;wBACL,oBAAoB,OAAA,CAAQ,IAAA,CAAK;4BAC/B,MAAM;4BACN,MAAM;wBACR;oBACF;oBACA,iBAAiB,IAAA,CAAK;oBACtB,eAAe,IAAA,CACb,8BAA8B,qBAAqB;gBAEvD,OAAA,IAAW,iBAAiB,YAAY;oBACtC,MAAM,sBAAsB,mBAAmB;wBAC7C;wBACA,WAAW;wBACX,aAAa;oBACf;oBAEA,iBAAiB,IAAA,IAAQ;oBACzB,eAAe,IAAA,IACV,oBAAoB,GAAA,CAAI,CAAA,UACzB,8BAA8B,SAAS;gBAG7C,OAAO;oBAEL,MAAM,sBAAsB,mBAAmB;wBAC7C,MAAM,qBAAqB,IAAA;wBAC3B,WAAW;wBACX,aAAa;oBACf;oBAEA,iBAAiB,IAAA,IAAQ;oBACzB,eAAe,IAAA,IACV,oBAAoB,GAAA,CAAI,CAAA,UACzB,8BAA8B,SAAS;gBAG7C;gBAEA,WAAW;YACb,QAAS,aAAa,OAAA;YAGtB,KAAK,aAAA,CACH,0BAA0B;gBACxB;gBACA,YAAY;oBACV,4BAA4B,qBAAqB,YAAA;oBACjD,oBAAoB;wBAClB,QAAQ,IAAM,qBAAqB,IAAA;oBACrC;oBACA,yBAAyB;wBACvB,QAAQ,IAAM,KAAK,SAAA,CAAU,qBAAqB,SAAS;oBAC7D;oBAEA,yBAAyB,qBAAqB,KAAA,CAAM,YAAA;oBACpD,6BACE,qBAAqB,KAAA,CAAM,gBAAA;oBAAA,cAAA;oBAG7B,mBAAmB,qBAAqB,YAAA;oBACxC,kBAAkB;wBAChB,QAAQ,IAAM,qBAAqB,IAAA;oBACrC;oBACA,uBAAuB;wBACrB,QAAQ,IAAM,KAAK,SAAA,CAAU,qBAAqB,SAAS;oBAC7D;gBACF;YACF;YAGF,OAAO,IAAI,0BAA0B;gBACnC;gBACA,WAAW;gBACX,aAAa;gBACb,cAAc,qBAAqB,YAAA;gBACnC;gBACA,UAAU,qBAAqB,QAAA;gBAC/B,UAAU;oBACR,GAAG,qBAAqB,QAAA;oBACxB,SAAA,CAAS,KAAA,qBAAqB,WAAA,KAArB,OAAA,KAAA,IAAA,GAAkC,OAAA;gBAC7C;gBACA,UAAU,qBAAqB,QAAA;gBAC/B;gBACA;gBACA,kBAAkB,qBAAqB,gBAAA;YACzC;QACF;IACF;AACF;AAEA,eAAe,aAAqD,EAClE,SAAA,EACA,KAAA,EACA,MAAA,EACA,SAAA,EACF;IAME,MAAM,cAAc,MAAM,QAAQ,GAAA,CAChC,UAAU,GAAA,CAAI,OAAM;QAClB,MAAMqB,QAAO,KAAA,CAAM,SAAS,QAAQ,CAAA;QAEpC,IAAA,CAAIA,SAAA,OAAA,KAAA,IAAAA,MAAM,OAAA,KAAW,MAAM;YACzB,OAAO,KAAA;QACT;QAEA,MAAM,SAAS,MAAM,WAAW;YAC9B,MAAM;YACN,YAAY,0BAA0B;gBACpC;gBACA,YAAY;oBACV,GAAG,sBAAsB;wBACvB,aAAa;wBACb;oBACF,EAAC;oBACD,oBAAoB,SAAS,QAAA;oBAC7B,kBAAkB,SAAS,UAAA;oBAC3B,oBAAoB;wBAClB,QAAQ,IAAM,KAAK,SAAA,CAAU,SAAS,IAAI;oBAC5C;gBACF;YACF;YACA;YACA,IAAI,OAAM;gBACR,MAAMN,UAAS,MAAMM,MAAK,OAAA,CAAS,SAAS,IAAI;gBAEhD,IAAI;oBACF,KAAK,aAAA,CACH,0BAA0B;wBACxB;wBACA,YAAY;4BACV,sBAAsB;gCACpB,QAAQ,IAAM,KAAK,SAAA,CAAUN;4BAC/B;wBACF;oBACF;gBAEJ,EAAA,OAAS,SAAS,CAKlB;gBAEA,OAAOA;YACT;QACF;QAEA,OAAO;YACL,YAAY,SAAS,UAAA;YACrB,UAAU,SAAS,QAAA;YACnB,MAAM,SAAS,IAAA;YACf;QACF;IACF;IAGF,OAAO,YAAY,MAAA,CACjB,CAAC,SAAiD,UAAU;AAEhE;AAEA,IAAM,4BAAN;IAiBE,YAAY,OAAA,CAYT;QACD,IAAA,CAAK,IAAA,GAAO,QAAQ,IAAA;QACpB,IAAA,CAAK,SAAA,GAAY,QAAQ,SAAA;QACzB,IAAA,CAAK,WAAA,GAAc,QAAQ,WAAA;QAC3B,IAAA,CAAK,YAAA,GAAe,QAAQ,YAAA;QAC5B,IAAA,CAAK,KAAA,GAAQ,QAAQ,KAAA;QACrB,IAAA,CAAK,QAAA,GAAW,QAAQ,QAAA;QACxB,IAAA,CAAK,QAAA,GAAW,QAAQ,QAAA;QACxB,IAAA,CAAK,gBAAA,GAAmB,QAAQ,gBAAA;QAChC,IAAA,CAAK,UAAA,GAAa,QAAQ,KAAA;QAC1B,IAAA,CAAK,KAAA,GAAQ,QAAQ,KAAA;QACrB,IAAA,CAAK,6BAAA,GAAgC,QAAQ,gBAAA;QAG7C,IAAA,CAAK,WAAA,GAAc;YACjB,SAAS,QAAQ,QAAA,CAAS,OAAA;QAC5B;QACA,IAAA,CAAK,QAAA,GAAW,QAAQ,QAAA;IAC1B;AACF;AAKO,IAAM,4BAA4B;;;AY/oBlC,SAAS;IACd,IAAI,qBAAuD,EAAC;IAC5D,IAAI,aAAwD;IAC5D,IAAI,WAAW;IAEf,MAAM,cAAc;QAElB,IAAI,YAAY,mBAAmB,MAAA,KAAW,GAAG;YAC/C,cAAA,OAAA,KAAA,IAAA,WAAY,KAAA;YACZ;QACF;QAGA,IAAI,mBAAmB,MAAA,KAAW,GAAG;YACnC;QACF;QAEA,IAAI;YACF,MAAM,EAAE,KAAA,EAAO,IAAA,EAAK,GAAI,MAAM,kBAAA,CAAmB,EAAC,CAAE,IAAA;YAEpD,IAAI,MAAM;gBAER,mBAAmB,KAAA;gBAGnB,IAAI,mBAAmB,MAAA,GAAS,GAAG;oBACjC,MAAM;gBACR,OAAA,IAAW,UAAU;oBACnB,cAAA,OAAA,KAAA,IAAA,WAAY,KAAA;gBACd;YACF,OAAO;gBAEL,cAAA,OAAA,KAAA,IAAA,WAAY,OAAA,CAAQ;YACtB;QACF,EAAA,OAAS,OAAO;YAEd,cAAA,OAAA,KAAA,IAAA,WAAY,KAAA,CAAM;YAClB,mBAAmB,KAAA;YAEnB,IAAI,YAAY,mBAAmB,MAAA,KAAW,GAAG;gBAC/C,cAAA,OAAA,KAAA,IAAA,WAAY,KAAA;YACd;QACF;IACF;IAEA,OAAO;QACL,QAAQ,IAAI,eAAkB;YAC5B,OAAM,eAAA;gBACJ,aAAa;YACf;YACA,MAAM;YACN,MAAM;gBACJ,KAAA,MAAW,UAAU,mBAAoB;oBACvC,MAAM,OAAO,MAAA;gBACf;gBACA,qBAAqB,EAAC;gBACtB,WAAW;YACb;QACF;QACA,WAAW,CAAC;YACV,IAAI,UAAU;gBACZ,MAAM,IAAI,MAAM;YAClB;YAEA,mBAAmB,IAAA,CAAK,YAAY,SAAA;QACtC;QACA,OAAO;YACL,WAAW;YAEX,IAAI,mBAAmB,MAAA,KAAW,GAAG;gBACnC,cAAA,OAAA,KAAA,IAAA,WAAY,KAAA;YACd;QACF;IACF;AACF;;ACnEO,SAAS,aACd,OAAA,EACA,OAAA;IAEA,MAAM,UAAU,QAAQ,SAAA;IACxB,MAAM,UAAU,QAAQ,SAAA;IAExB,IAAI,YACF,KAAA;IACF,IAAI,YACF,KAAA;IAEF,IAAI,cAAc;IAClB,IAAI,cAAc;IAGlB,eAAe,YACb,UAAA;QAEA,IAAI;YACF,IAAI,aAAa,MAAM;gBACrB,YAAY,QAAQ,IAAA;YACtB;YAEA,MAAM,SAAS,MAAM;YACrB,YAAY,KAAA;YAEZ,IAAI,CAAC,OAAO,IAAA,EAAM;gBAChB,WAAW,OAAA,CAAQ,OAAO,KAAK;YACjC,OAAO;gBACL,WAAW,KAAA;YACb;QACF,EAAA,OAAS,OAAO;YACd,WAAW,KAAA,CAAM;QACnB;IACF;IAGA,eAAe,YACb,UAAA;QAEA,IAAI;YACF,IAAI,aAAa,MAAM;gBACrB,YAAY,QAAQ,IAAA;YACtB;YAEA,MAAM,SAAS,MAAM;YACrB,YAAY,KAAA;YAEZ,IAAI,CAAC,OAAO,IAAA,EAAM;gBAChB,WAAW,OAAA,CAAQ,OAAO,KAAK;YACjC,OAAO;gBACL,WAAW,KAAA;YACb;QACF,EAAA,OAAS,OAAO;YACd,WAAW,KAAA,CAAM;QACnB;IACF;IAEA,OAAO,IAAI,eAAgC;QACzC,MAAM,MAAK,UAAA;YACT,IAAI;gBAEF,IAAI,aAAa;oBACf,MAAM,YAAY;oBAClB;gBACF;gBAGA,IAAI,aAAa;oBACf,MAAM,YAAY;oBAClB;gBACF;gBAGA,IAAI,aAAa,MAAM;oBACrB,YAAY,QAAQ,IAAA;gBACtB;gBACA,IAAI,aAAa,MAAM;oBACrB,YAAY,QAAQ,IAAA;gBACtB;gBAKA,MAAM,EAAE,MAAA,EAAQ,MAAA,EAAO,GAAI,MAAM,QAAQ,IAAA,CAAK;oBAC5C,UAAU,IAAA,CAAK,CAAAA,UAAAA,CAAW;4BAAE,QAAAA;4BAAQ,QAAQ;wBAAQ,CAAA;oBACpD,UAAU,IAAA,CAAK,CAAAA,UAAAA,CAAW;4BAAE,QAAAA;4BAAQ,QAAQ;wBAAQ,CAAA;iBACrD;gBAED,IAAI,CAAC,OAAO,IAAA,EAAM;oBAChB,WAAW,OAAA,CAAQ,OAAO,KAAK;gBACjC;gBAEA,IAAI,WAAW,SAAS;oBACtB,YAAY,KAAA;oBACZ,IAAI,OAAO,IAAA,EAAM;wBAEf,MAAM,YAAY;wBAClB,cAAc;oBAChB;gBACF,OAAO;oBACL,YAAY,KAAA;oBAEZ,IAAI,OAAO,IAAA,EAAM;wBACf,cAAc;wBACd,MAAM,YAAY;oBACpB;gBACF;YACF,EAAA,OAAS,OAAO;gBACd,WAAW,KAAA,CAAM;YACnB;QACF;QACA;YACE,QAAQ,MAAA;YACR,QAAQ,MAAA;QACV;IACF;AACF;;ACrEO,SAAS,uBAA+D,EAC7E,KAAA,EACA,eAAA,EACA,iBAAA,EACA,MAAA,EACA,SAAA,EACF;IAOE,IAAI,WAAW;IACf,MAAM,uBAAuB,aAAA,GAAA,IAAI;IAGjC,IAAI,8BAEO;IACX,MAAM,oBAAoB,IAAI,eAE5B;QACA,OAAM,UAAA;YACJ,8BAA8B;QAChC;IACF;IAGA,MAAM,kBAA2C,CAAC;IAGlD,MAAM,gBAAgB,IAAI,gBAGxB;QACA,WACE,KAAA,EACA,UAAA;YAIA,MAAM,YAAY,MAAM,IAAA;YAExB,OAAQ;gBAEN,KAAK;gBACL,KAAK;gBACL,KAAK;oBAAS;wBACZ,WAAW,OAAA,CAAQ;wBACnB;oBACF;gBAGA,KAAK;oBAAmB;wBACtB,IAAI,mBAAmB;4BACrB,IAAI,CAAC,eAAA,CAAgB,MAAM,UAAU,CAAA,EAAG;gCACtC,WAAW,OAAA,CAAQ;oCACjB,MAAM;oCACN,YAAY,MAAM,UAAA;oCAClB,UAAU,MAAM,QAAA;gCAClB;gCAEA,eAAA,CAAgB,MAAM,UAAU,CAAA,GAAI;4BACtC;4BAEA,WAAW,OAAA,CAAQ;gCACjB,MAAM;gCACN,YAAY,MAAM,UAAA;gCAClB,UAAU,MAAM,QAAA;gCAChB,eAAe,MAAM,aAAA;4BACvB;wBACF;wBACA;oBACF;gBAGA,KAAK;oBAAa;wBAChB,MAAM,WAAW,MAAM,QAAA;wBAEvB,IAAI,SAAS,MAAM;4BACjB,4BAA6B,OAAA,CAAQ;gCACnC,MAAM;gCACN,OAAO,IAAI,gBAAgB;oCAAE,UAAU,MAAM,QAAA;gCAAS;4BACxD;4BACA;wBACF;wBAEA,MAAMM,QAAO,KAAA,CAAM,SAAQ;wBAE3B,IAAIA,SAAQ,MAAM;4BAChB,4BAA6B,OAAA,CAAQ;gCACnC,MAAM;gCACN,OAAO,IAAI,gBAAgB;oCACzB,UAAU,MAAM,QAAA;oCAChB,gBAAgB,OAAO,IAAA,CAAK;gCAC9B;4BACF;4BAEA;wBACF;wBAEA,IAAI;4BACF,MAAM,WAAW,cAAc;gCAC7B,UAAU;gCACV;4BACF;4BAEA,WAAW,OAAA,CAAQ;4BAEnB,IAAIA,MAAK,OAAA,IAAW,MAAM;gCACxB,MAAM,kBAAkB,CAAA,GAAA,kNAAA,CAAA,aAAA;gCACxB,qBAAqB,GAAA,CAAI;gCAKzB,WAAW;oCACT,MAAM;oCACN,YAAY,0BAA0B;wCACpC;wCACA,YAAY;4CACV,GAAG,sBAAsB;gDACvB,aAAa;gDACb;4CACF,EAAC;4CACD,oBAAoB,SAAS,QAAA;4CAC7B,kBAAkB,SAAS,UAAA;4CAC3B,oBAAoB;gDAClB,QAAQ,IAAM,KAAK,SAAA,CAAU,SAAS,IAAI;4CAC5C;wCACF;oCACF;oCACA;oCACA,IAAI,OAAM,OACRA,MAAK,OAAA,CAAS,SAAS,IAAI,EAAE,IAAA,CAC3B,CAAC;4CACC,4BAA6B,OAAA,CAAQ;gDACnC,GAAG,QAAA;gDACH,MAAM;gDACN;4CACF;4CAEA,qBAAqB,MAAA,CAAO;4CAG5B,IAAI,YAAY,qBAAqB,IAAA,KAAS,GAAG;gDAC/C,4BAA6B,KAAA;4CAC/B;4CAGA,IAAI;gDACF,KAAK,aAAA,CACH,0BAA0B;oDACxB;oDACA,YAAY;wDACV,sBAAsB;4DACpB,QAAQ,IAAM,KAAK,SAAA,CAAU;wDAC/B;oDACF;gDACF;4CAEJ,EAAA,OAAS,SAAS,CAKlB;wCACF,GACA,CAAC;4CACC,4BAA6B,OAAA,CAAQ;gDACnC,MAAM;gDACN;4CACF;4CAEA,qBAAqB,MAAA,CAAO;4CAG5B,IAAI,YAAY,qBAAqB,IAAA,KAAS,GAAG;gDAC/C,4BAA6B,KAAA;4CAC/B;wCACF;gCAEN;4BACF;wBACF,EAAA,OAAS,OAAO;4BACd,4BAA6B,OAAA,CAAQ;gCACnC,MAAM;gCACN;4BACF;wBACF;wBAEA;oBACF;gBAGA,KAAK;oBAAU;wBACb,WAAW,OAAA,CAAQ;4BACjB,MAAM;4BACN,cAAc,MAAM,YAAA;4BACpB,UAAU,MAAM,QAAA;4BAChB,OAAO,4BAA4B,MAAM,KAAK;4BAC9C,+BAA+B,MAAM,gBAAA;wBACvC;wBACA;oBACF;gBAEA;oBAAS;wBACP,MAAM,mBAA0B;wBAChC,MAAM,IAAI,MAAM,CAAA,sBAAA,EAAyB,iBAAgB,CAAE;oBAC7D;YACF;QACF;QAEA;YACE,WAAW;YAEX,IAAI,qBAAqB,IAAA,KAAS,GAAG;gBACnC,4BAA6B,KAAA;YAC/B;QACF;IACF;IAGA,OAAO,IAAI,eAAmD;QAC5D,MAAM,OAAM,UAAA;YAGV,OAAO,QAAQ,GAAA,CAAI;gBACjB,gBAAgB,WAAA,CAAY,eAAe,MAAA,CACzC,IAAI,eAAe;oBACjB,OAAM,KAAA;wBACJ,WAAW,OAAA,CAAQ;oBACrB;oBACA,UAEA;gBACF;gBAEF,kBAAkB,MAAA,CAChB,IAAI,eAAe;oBACjB,OAAM,KAAA;wBACJ,WAAW,OAAA,CAAQ;oBACrB;oBACA;wBACE,WAAW,KAAA;oBACb;gBACF;aAEH;QACH;IACF;AACF;;AH1PA,IAAML,sBAAqBC,CAAAA,GAAAA,0LAAAA,CAAAA,oBAAAA,EAAkB;IAAE,QAAQ;IAAU,MAAM;AAAG;AAiD1E,eAAsB,WAAmD,EACvE,KAAA,EACA,KAAA,EACA,UAAA,EACA,MAAA,EACA,MAAA,EACA,QAAA,EACA,UAAA,EACA,WAAA,EACA,OAAA,EACA,oBAAoB,CAAA,EACpB,WAAW,qBAAqB,OAAO,oBAAoB,IAAI,CAAA,EAC/D,4BAA4B,gBAAgB,KAAA,EAC5C,wBAAwB,SAAA,EACxB,+BAA+B,gBAAA,EAC/B,gCAAgC,oBAAoB,KAAA,EACpD,OAAA,EACA,QAAA,EACA,YAAA,EACA,WAAW,EACT,KAAAC,OAAM,GAAA,EACN,YAAAL,cAAaG,mBAAAA,EACb,cAAc,IAAM,aAAA,GAAA,IAAI,MAAK,EAC/B,GAAI,CAAC,CAAA,EACL,GAAG,UACL;IA1IA,IAAAhB;IAsQE,IAAI,WAAW,GAAG;QAChB,MAAM,IAAI,qBAAqB;YAC7B,WAAW;YACX,OAAO;YACP,SAAS;QACX;IACF;IAEA,MAAM,0BAA0B,2BAA2B;QACzD;QACA;QACA;QACA,UAAU;YAAE,GAAG,QAAA;YAAU;QAAW;IACtC;IAEA,MAAM,SAAS,UAAU;QAAE,WAAA,CAAWA,OAAA,aAAA,OAAA,KAAA,IAAA,UAAW,SAAA,KAAX,OAAAA,OAAwB;IAAM;IAEpE,OAAO,WAAW;QAChB,MAAM;QACN,YAAY,0BAA0B;YACpC;YACA,YAAY;gBACV,GAAG,sBAAsB;oBAAE,aAAa;oBAAiB;gBAAU,EAAC;gBACpE,GAAG,uBAAA;gBAAA,6DAAA;gBAEH,aAAa;oBACX,OAAO,IAAM,KAAK,SAAA,CAAU;4BAAE;4BAAQ;4BAAQ;wBAAS;gBACzD;gBACA,wBAAwB;YAC1B;QACF;QACA;QACA,aAAa;QACb,IAAI,OAAM;YACR,MAAM,QAAQ,4BAA4B;gBAAE;YAAW;YAEvD,MAAM,YAAsC,OAAO,EACjD,gBAAA4B,eAAAA,EACA,UAAA,EACF;gBAIE,MAAM,EACJ,QAAQ,EAAE,QAAAC,OAAAA,EAAQ,UAAAC,SAAAA,EAAU,aAAAC,YAAAA,EAAY,EACxC,cAAAZ,aAAAA,EACA,kBAAAa,iBAAAA,EACF,GAAI,MAAM,MAAM,IACd,WAAW;wBACT,MAAM;wBACN,YAAY,0BAA0B;4BACpC;4BACA,YAAY;gCACV,GAAG,sBAAsB;oCACvB,aAAa;oCACb;gCACF,EAAC;gCACD,GAAG,uBAAA;gCACH,oBAAoB;oCAClB,OAAO,IAAM;gCACf;gCACA,sBAAsB;oCACpB,OAAO,IAAM,KAAK,SAAA,CAAUJ;gCAC9B;gCAAA,2CAAA;gCAGA,iBAAiB,MAAM,QAAA;gCACvB,wBAAwB,MAAM,OAAA;gCAC9B,oCAAoC,SAAS,gBAAA;gCAC7C,6BAA6B,SAAS,SAAA;gCACtC,mCAAmC,SAAS,eAAA;gCAC5C,iCAAiC,SAAS,aAAA;gCAC1C,8BAA8B,SAAS,WAAA;gCACvC,wBAAwB,SAAS,IAAA;gCACjC,wBAAwB,SAAS,IAAA;4BACnC;wBACF;wBACA;wBACA,aAAa;wBACb,IAAI,OAAMT,gBAAAA,CAAiB;gCACzB,kBAAkBD;gCAAI,sBAAA;gCACtB,cAAAC;gCACA,QAAQ,MAAM,MAAM,QAAA,CAAS;oCAC3B,MAAM;wCACJ,MAAM;wCACN,GAAG,0BAA0B;4CAAE;4CAAO;wCAAW,EAAC;oCACpD;oCACA,GAAG,oBAAoB,SAAQ;oCAC/B,aAAa;oCACb,QAAQS;oCACR;oCACA;oCACA;gCACF;4BACF,CAAA;oBACF;gBAGF,OAAO;oBACL,QAAQ;wBACN,QAAQ,uBAAuB;4BAC7B;4BACA,iBAAiBC;4BACjB;4BACA;4BACA;wBACF;wBACA,UAAAC;wBACA,aAAAC;oBACF;oBACA,cAAAZ;oBACA,kBAAAa;gBACF;YACF;YAEA,MAAM,iBAAiB,MAAM,6BAA6B;gBACxD,QAAQ,eAAe;oBAAE;oBAAQ;oBAAQ;gBAAS;gBAClD,wBAAwB,MAAM,iBAAA;YAChC;YAEA,MAAM,EACJ,QAAQ,EAAE,MAAA,EAAQ,QAAA,EAAU,WAAA,EAAY,EACxC,YAAA,EACA,gBAAA,EACF,GAAI,MAAM,UAAU;gBAClB,YAAY,eAAe;oBAAE;oBAAQ;oBAAQ;gBAAS,GAAG,IAAA;gBACzD;YACF;YAEA,OAAO,IAAI,wBAAwB;gBACjC;gBACA;gBACA;gBACA;gBACA;gBACA;gBACA;gBACA;gBACA;gBACA;gBACA;gBACA;gBACA;gBACA;gBACA,SAAS,MAAM,OAAA;gBACf,KAAAd;gBACA;gBACA,YAAAL;YACF;QACF;IACF;AACF;AAiBA,IAAM,0BAAN;IAoBE,YAAY,EACV,MAAA,EACA,QAAA,EACA,WAAA,EACA,OAAA,EACA,QAAA,EACA,YAAA,EACA,QAAA,EACA,YAAA,EACA,SAAA,EACA,gBAAA,EACA,QAAA,EACA,aAAA,EACA,SAAA,EACA,cAAA,EACA,OAAA,EACA,KAAAK,IAAAA,EACA,WAAA,EACA,YAAAL,WAAAA,EACF,CA4BG;QACD,IAAA,CAAK,QAAA,GAAW;QAChB,IAAA,CAAK,WAAA,GAAc;QAGnB,MAAM,EAAE,SAAS,YAAA,EAAc,SAAS,YAAA,EAAa,GACnD;QACF,IAAA,CAAK,KAAA,GAAQ;QAGb,MAAM,EAAE,SAAS,mBAAA,EAAqB,SAAS,mBAAA,EAAoB,GACjE;QACF,IAAA,CAAK,YAAA,GAAe;QAGpB,MAAM,EAAE,SAAS,WAAA,EAAa,SAAS,WAAA,EAAY,GACjD;QACF,IAAA,CAAK,IAAA,GAAO;QAGZ,MAAM,EAAE,SAAS,gBAAA,EAAkB,SAAS,gBAAA,EAAiB,GAC3D;QACF,IAAA,CAAK,SAAA,GAAY;QAGjB,MAAM,EAAE,SAAS,kBAAA,EAAoB,SAAS,kBAAA,EAAmB,GAC/D;QACF,IAAA,CAAK,WAAA,GAAc;QAGnB,MAAM,EAAE,SAAS,YAAA,EAAc,SAAS,YAAA,EAAa,GACnD;QACF,IAAA,CAAK,KAAA,GAAQ;QAGb,MAAM,EACJ,SAAS,uBAAA,EACT,SAAS,uBAAA,EACX,GAAI;QACJ,IAAA,CAAK,6BAAA,GAAgC;QAGrC,MAAM,EAAE,SAAS,eAAA,EAAiB,SAAS,eAAA,EAAgB,GACzD;QACF,IAAA,CAAK,QAAA,GAAW;QAGhB,MAAM,EACJ,SAAS,uBAAA,EACT,SAAS,uBAAA,EACX,GACE;QACF,IAAA,CAAK,gBAAA,GAAmB;QAGxB,MAAM,EACJ,QAAQ,gBAAA,EACR,SAAA,EACA,OAAO,qBAAA,EACT,GAAI;QAEJ,IAAA,CAAK,cAAA,GAAiB;QAGtB,MAAM,cAAmC,EAAC;QAE1C,MAAM,OAAO,IAAA;QAGb,SAAS,cAAc,EACrB,QAAAgB,OAAAA,EACA,cAAA,EACA,cAAAV,aAAAA,EACA,WAAA,EACA,gBAAAS,eAAAA,EACA,QAAQ;YACN,cAAc;YACd,kBAAkB;YAClB,aAAa;QACf,CAAA,EACA,QAAA,EACA,mBAAmB,EAAA,EACrB;YAUE,MAAM,gBAAqC,EAAC;YAC5C,MAAM,kBAAyC,EAAC;YAChD,IAAI,mBAAiC;YACrC,IAAI,YAAgC;gBAClC,cAAc;gBACd,kBAAkB;gBAClB,aAAa;YACf;YACA,IAAI;YACJ,IAAI,iBAAiB;YACrB,IAAI,WAAW;YACf,IAAI,eAAe,aAAa,aAAa,mBAAmB;YAChE,IAAI;YACJ,IAAI,eAAiE;gBACnE,IAAIf;gBACJ,WAAW;gBACX;YACF;YAGA,IAAI,cAAc;YAClB,IAAI,qBAAqB;YAEzB,eAAe,iBAAiB,EAC9B,UAAA,EACA,KAAA,EACF;gBAIE,WAAW,OAAA,CAAQ;gBAEnB,YAAY,MAAM,SAAA;gBAClB,gBAAgB,MAAM,SAAA;gBACtB,qBAAqB;gBAErB,MAAA,CAAM,WAAA,OAAA,KAAA,IAAA,QAAU;oBAAE;gBAAM,EAAA;YAC1B;YAEA,UACEgB,QAAO,WAAA,CACL,IAAI,gBAGF;gBACA,MAAM,WAAU,KAAA,EAAO,UAAA;oBA1nBnC,IAAA7B,MAAA,IAAA;oBA4nBc,IAAI,gBAAgB;wBAClB,MAAM,iBAAiBkB,SAAQ;wBAE/B,iBAAiB;wBAEjBC,cAAa,QAAA,CAAS,wBAAwB;4BAC5C,8BAA8B;4BAAA,cAAA;4BAG9B,4BAA4B;wBAC9B;wBAEAA,cAAa,aAAA,CAAc;4BACzB,8BAA8B;4BAAA,cAAA;4BAG9B,4BAA4B;wBAC9B;oBACF;oBAGA,IAAI,MAAM,IAAA,KAAS,gBAAgB,MAAM,SAAA,CAAU,MAAA,KAAW,GAAG;wBAC/D;oBACF;oBAEA,MAAM,YAAY,MAAM,IAAA;oBACxB,OAAQ;wBACN,KAAK;4BAAc;gCACjB,IAAI,eAAe;oCACjB,eAAe,MAAM,SAAA;oCAErB,MAAM,QAAQ,sBAAsB;oCAGpC,IAAI,SAAS,MAAM;wCACjB,cAAc,MAAM,MAAA;wCAEpB,MAAM,iBAAiB;4CACrB;4CACA,OAAO;gDACL,MAAM;gDACN,WAAW,MAAM,MAAA,GAAS,MAAM,UAAA;4CAClC;wCACF;oCACF;gCACF,OAAO;oCACL,MAAM,iBAAiB;wCAAE;wCAAY;oCAAM;gCAC7C;gCAEA;4BACF;wBAEA,KAAK;4BAAa;gCAChB,WAAW,OAAA,CAAQ;gCAEnB,cAAc,IAAA,CAAK;gCACnB,MAAA,CAAM,WAAA,OAAA,KAAA,IAAA,QAAU;oCAAE;gCAAM,EAAA;gCACxB;4BACF;wBAEA,KAAK;4BAAe;gCAClB,WAAW,OAAA,CAAQ;gCAEnB,gBAAgB,IAAA,CAAK;gCAErB,MAAA,CAAM,WAAA,OAAA,KAAA,IAAA,QAAU;oCAAE;gCAAoB,EAAA;gCACtC;4BACF;wBAEA,KAAK;4BAAqB;gCACxB,eAAe;oCACb,IAAA,CAAInB,OAAA,MAAM,EAAA,KAAN,OAAAA,OAAY,aAAa,EAAA;oCAC7B,WAAA,CAAW,KAAA,MAAM,SAAA,KAAN,OAAA,KAAmB,aAAa,SAAA;oCAC3C,SAAA,CAAS,KAAA,MAAM,OAAA,KAAN,OAAA,KAAiB,aAAa,OAAA;gCACzC;gCACA;4BACF;wBAEA,KAAK;4BAAU;gCAGb,YAAY,MAAM,KAAA;gCAClB,mBAAmB,MAAM,YAAA;gCACzB,uBAAuB,MAAM,6BAAA;gCAC7B,eAAe,MAAM,QAAA;gCAIrB,MAAM,aAAakB,SAAQ;gCAC3BC,cAAa,QAAA,CAAS;gCACtBA,cAAa,aAAA,CAAc;oCACzB,0BAA0B;oCAC1B,4CACG,MAAO,UAAU,gBAAA,GAAoB;gCAC1C;gCAEA;4BACF;wBAEA,KAAK;wBACL,KAAK;4BAAmB;gCACtB,WAAW,OAAA,CAAQ;gCACnB,MAAA,CAAM,WAAA,OAAA,KAAA,IAAA,QAAU;oCAAE;gCAAM,EAAA;gCACxB;4BACF;wBAEA,KAAK;4BAAS;gCACZ,WAAW,OAAA,CAAQ;gCACnB,mBAAmB;gCACnB;4BACF;wBAEA;4BAAS;gCACP,MAAM,kBAAyB;gCAC/B,MAAM,IAAI,MAAM,CAAA,oBAAA,EAAuB,gBAAe,CAAE;4BAC1D;oBACF;gBACF;gBAAA,8FAAA;gBAGA,MAAM,OAAM,UAAA;oBACV,MAAM,oBACJ,cAAc,MAAA,GAAS,IACnB,KAAK,SAAA,CAAU,iBACf,KAAA;oBAGN,IAAI,eAAoD;oBACxD,IAAI,cAAc,IAAI,UAAU;wBAC9B,IACE,iBACA,qBAAqB,YAAA,kDAAA;wBAErB,cAAc,MAAA,KAAW,GACzB;4BACA,eAAe;wBACjB,OAAA,IAAA,wBAAA;wBAEE,cAAc,MAAA,GAAS,KAAA,uCAAA;wBAEvB,gBAAgB,MAAA,KAAW,cAAc,MAAA,EACzC;4BACA,eAAe;wBACjB;oBACF;oBAIA,IACE,iBACA,YAAY,MAAA,GAAS,KAAA,CACpB,iBAAiB,cAAA,2DAAA;oBACf,aAAa,cAAc,CAAC,kBAAA,GAC/B;wBACA,MAAM,iBAAiB;4BACrB;4BACA,OAAO;gCACL,MAAM;gCACN,WAAW;4BACb;wBACF;wBACA,cAAc;oBAChB;oBAGA,IAAI;wBACFA,cAAa,aAAA,CACX,0BAA0B;4BACxB;4BACA,YAAY;gCACV,4BAA4B;gCAC5B,oBAAoB;oCAAE,QAAQ,IAAM;gCAAS;gCAC7C,yBAAyB;oCACvB,QAAQ,IAAM;gCAChB;gCACA,kBAAkB,aAAa,EAAA;gCAC/B,qBAAqB,aAAa,OAAA;gCAClC,yBACE,aAAa,SAAA,CAAU,WAAA;gCAEzB,yBAAyB,UAAU,YAAA;gCACnC,6BAA6B,UAAU,gBAAA;gCAAA,aAAA;gCAGvC,mBAAmB;gCACnB,kBAAkB;oCAAE,QAAQ,IAAM;gCAAS;gCAC3C,uBAAuB;oCACrB,QAAQ,IAAM;gCAChB;gCAAA,2CAAA;gCAGA,kCAAkC;oCAAC;iCAAgB;gCACnD,sBAAsB,aAAa,EAAA;gCACnC,yBAAyB,aAAa,OAAA;gCACtC,6BAA6B,UAAU,YAAA;gCACvC,8BAA8B,UAAU,gBAAA;4BAC1C;wBACF;oBAEJ,EAAA,OAAS,OAAO,CAEhB,SAAE;wBAEAA,cAAa,GAAA;oBACf;oBAEA,WAAW,OAAA,CAAQ;wBACjB,MAAM;wBACN,cAAc;wBACd,OAAO;wBACP,+BAA+B;wBAC/B,UAAU;wBACV,UAAU;wBACV,aAAa,iBAAiB;oBAChC;oBAEA,MAAM,aAAgC;wBACpC;wBACA,MAAM;wBACN,WAAW;wBACX,aAAa;wBACb,cAAc;wBACd,OAAO;wBACP,UAAU,KAAK,QAAA;wBACf,UAAU;wBACV,UAAU;wBACV,aAAa,KAAK,WAAA;wBAClB,+BAA+B;wBAC/B,aAAa,iBAAiB;oBAChC;oBAEA,YAAY,IAAA,CAAK;oBAEjB,MAAA,CAAM,gBAAA,OAAA,KAAA,IAAA,aAAe,WAAA;oBAErB,MAAM,gBAAgB;wBACpB,cAAc,MAAM,YAAA,GAAe,UAAU,YAAA;wBAC7C,kBACE,MAAM,gBAAA,GAAmB,UAAU,gBAAA;wBACrC,aAAa,MAAM,WAAA,GAAc,UAAU,WAAA;oBAC7C;oBAEA,IAAI,iBAAiB,QAAQ;wBAE3B,IAAI,aAAa,YAAY;4BAI3B,MAAM,oBAAoBS,eAAAA,CACxBA,gBAAe,MAAA,GAAS,EAC1B;4BAIA,kBAAkB,OAAA,CAAQ,IAAA,CAAK;gCAC7B,MAAM;gCACN,MAAM;4BACR;wBACF,OAAO;4BAILA,gBAAe,IAAA,IACV,mBAAmB;gCACpB,MAAM;gCACN,WAAW;gCACX,aAAa;4BACf,GAAG,GAAA,CAAI,CAAA,UACL,8BAA8B,SAAS;wBAG7C;wBAGA,MAAM,EACJ,MAAA,EACA,cAAAT,aAAAA,EACA,kBAAkBc,eAAAA,EACpB,GAAI,MAAM,UAAU;4BAClB,YAAY;4BACZ,gBAAAL;wBACF;wBAGA,KAAK,QAAA,GAAW,OAAO,QAAA;wBACvB,KAAK,WAAA,GAAc,OAAO,WAAA;wBAG1B,cAAc;4BACZ,QAAQ,OAAO,MAAA;4BACf,gBAAAK;4BACA,cAAAd;4BACA,aAAa,cAAc;4BAC3B,gBAAAS;4BACA,OAAO;4BACP,UAAU;4BACV,kBAAkB;wBACpB;wBAEA;oBACF;oBAEA,IAAI;wBAEF,WAAW,OAAA,CAAQ;4BACjB,MAAM;4BACN,cAAc;4BACd,OAAO;4BACP,+BAA+B;4BAC/B,UAAU;4BACV,UAAU;wBACZ;wBAGA;wBAGA,SAAS,aAAA,CACP,0BAA0B;4BACxB;4BACA,YAAY;gCACV,4BAA4B;gCAC5B,oBAAoB;oCAAE,QAAQ,IAAM;gCAAa;gCACjD,yBAAyB;oCACvB,QAAQ,IAAM;gCAChB;gCAEA,yBAAyB,cAAc,YAAA;gCACvC,6BACE,cAAc,gBAAA;gCAAA,aAAA;gCAGhB,mBAAmB;gCACnB,kBAAkB;oCAAE,QAAQ,IAAM;gCAAa;gCAC/C,uBAAuB;oCACrB,QAAQ,IAAM;gCAChB;4BACF;wBACF;wBAIF,MAAM,mBAAmB,YAAY,MAAA,CAEnC,CAACM,mBAAkB;4BACnB,IAAI,KAAK,QAAA,KAAa,YAAY;gCAIhC,MAAM,sBACJA,kBAAiB,GAAA;gCAEnB,IAAI,OAAO,oBAAoB,OAAA,KAAY,UAAU;oCACnD,oBAAoB,OAAA,IAAW,KAAK,IAAA;gCACtC,OAAO;oCACL,oBAAoB,OAAA,CAAQ,IAAA,CAAK;wCAC/B,MAAM,KAAK,IAAA;wCACX,MAAM;oCACR;gCACF;gCAEA,OAAO;uCAAIA;oCAAkB;iCAAmB;4BAClD;4BAEA,OAAO;mCACFA;mCACA,mBAAmB;oCACpB,MAAM,KAAK,IAAA;oCACX,WAAW,KAAK,SAAA;oCAChB,aAAa,KAAK,WAAA;gCACpB;6BACF;wBACF,GAAG,EAAE;wBAGL,aAAa;wBACb,oBAAoB;wBACpB,YAAY;wBACZ,iBAAiB;wBACjB,wBAAwB;wBACxB,mBAAmB;wBACnB,gBAAgB;4BACd,GAAG,YAAA;4BACH,SAAS,eAAA,OAAA,KAAA,IAAA,YAAa,OAAA;wBACxB;wBACA,aAAa;wBACb,wBAAwB;wBAGxB,MAAA,CAAM,YAAA,OAAA,KAAA,IAAA,SAAW;4BACf,cAAc;4BACd,UAAU;4BACV,OAAO;4BACP,MAAM;4BACN,WAAW;4BAAA,oEAAA;4BAAA,kEAAA;4BAAA,sEAAA;4BAAA,4DAAA;4BAKX,aAAa;4BACb;4BACA,UAAU;gCACR,GAAG,YAAA;gCACH,SAAS,eAAA,OAAA,KAAA,IAAA,YAAa,OAAA;4BACxB;4BACA;4BACA,+BAA+B;4BAC/B,OAAO;4BACP;wBACF,EAAA;oBACF,EAAA,OAAS,OAAO;wBACd,WAAW,KAAA,CAAM;oBACnB,SAAE;wBACA,SAAS,GAAA;oBACX;gBACF;YACF;QAGN;QAGA,cAAc;YACZ;YACA,gBAAgB;YAChB;YACA,aAAa;YACb;YACA,OAAO,KAAA;YACP,UAAU;QACZ;IACF;IAAA;;;;;;;KAAA,GAUQ,YAAY;QAClB,MAAM,CAAC,SAAS,QAAO,GAAI,IAAA,CAAK,cAAA,CAAe,GAAA;QAC/C,IAAA,CAAK,cAAA,GAAiB;QACtB,OAAO;IACT;IAEA,IAAI,aAA0C;QAC5C,OAAO,0BAA0B,IAAA,CAAK,SAAA,IAAa;YACjD,WAAU,KAAA,EAAO,UAAA;gBACf,IAAI,MAAM,IAAA,KAAS,cAAc;oBAC/B,WAAW,OAAA,CAAQ,MAAM,SAAS;gBACpC,OAAA,IAAW,MAAM,IAAA,KAAS,SAAS;oBACjC,WAAW,KAAA,CAAM,MAAM,KAAK;gBAC9B;YACF;QACF;IACF;IAEA,IAAI,aAAyD;QAC3D,OAAO,0BAA0B,IAAA,CAAK,SAAA,IAAa;YACjD,WAAU,KAAA,EAAO,UAAA;gBACf,WAAW,OAAA,CAAQ;YACrB;QACF;IACF;IAEA,WAAW,YAAyC,CAAC,CAAA,EAAG;QACtD,OAAO,IAAA,CAAK,oBAAA,CAAqB;YAAE;QAAU;IAC/C;IAEQ,qBAAqB,EAC3B,YAAY,CAAC,CAAA,EACb,iBAAAzB,mBAAkB,IAAM,EAAA,EAAA,4CAAA;IACxB,YAAY,IAAA,EACd,GAII,CAAC,CAAA,EAAG;QACN,IAAI,qBAAqB;QAEzB,MAAM,sBAAsB,IAAI,gBAG9B;YACA,MAAM;gBACJ,IAAI,UAAU,OAAA,EAAS,MAAM,UAAU,OAAA;YACzC;YAEA,MAAM,WAAU,KAAA,EAAO,UAAA;gBACrB,WAAW,OAAA,CAAQ;gBAEnB,IAAI,MAAM,IAAA,KAAS,cAAc;oBAC/B,MAAM,YAAY,MAAM,SAAA;oBAExB,sBAAsB;oBAEtB,IAAI,UAAU,OAAA,EAAS,MAAM,UAAU,OAAA,CAAQ;oBAC/C,IAAI,UAAU,MAAA,EAAQ,MAAM,UAAU,MAAA,CAAO;gBAC/C;YACF;YAEA,MAAM;gBACJ,IAAI,UAAU,YAAA,EACZ,MAAM,UAAU,YAAA,CAAa;gBAC/B,IAAI,UAAU,OAAA,EAAS,MAAM,UAAU,OAAA,CAAQ;YACjD;QACF;QAEA,MAAM,yBAAyB,IAAI,gBAGjC;YACA,WAAW,OAAO,OAAO;gBACvB,MAAM,YAAY,MAAM,IAAA;gBACxB,OAAQ;oBACN,KAAK;wBAAc;4BACjB,WAAW,OAAA,CAAQ,CAAA,GAAA,8KAAA,CAAA,mBAAA,EAAiB,QAAQ,MAAM,SAAS;4BAC3D;wBACF;oBAEA,KAAK;wBAA6B;4BAChC,WAAW,OAAA,CACT,CAAA,GAAA,8KAAA,CAAA,mBAAA,EAAiB,6BAA6B;gCAC5C,YAAY,MAAM,UAAA;gCAClB,UAAU,MAAM,QAAA;4BAClB;4BAEF;wBACF;oBAEA,KAAK;wBAAmB;4BACtB,WAAW,OAAA,CACT,CAAA,GAAA,8KAAA,CAAA,mBAAA,EAAiB,mBAAmB;gCAClC,YAAY,MAAM,UAAA;gCAClB,eAAe,MAAM,aAAA;4BACvB;4BAEF;wBACF;oBAEA,KAAK;wBAAa;4BAChB,WAAW,OAAA,CACT,CAAA,GAAA,8KAAA,CAAA,mBAAA,EAAiB,aAAa;gCAC5B,YAAY,MAAM,UAAA;gCAClB,UAAU,MAAM,QAAA;gCAChB,MAAM,MAAM,IAAA;4BACd;4BAEF;wBACF;oBAEA,KAAK;wBAAe;4BAClB,WAAW,OAAA,CACT,CAAA,GAAA,8KAAA,CAAA,mBAAA,EAAiB,eAAe;gCAC9B,YAAY,MAAM,UAAA;gCAClB,QAAQ,MAAM,MAAA;4BAChB;4BAEF;wBACF;oBAEA,KAAK;wBAAS;4BACZ,WAAW,OAAA,CACT,CAAA,GAAA,8KAAA,CAAA,mBAAA,EAAiB,SAASA,iBAAgB,MAAM,KAAK;4BAEvD;wBACF;oBAEA,KAAK;wBAAe;4BAClB,WAAW,OAAA,CACT,CAAA,GAAA,8KAAA,CAAA,mBAAA,EAAiB,eAAe;gCAC9B,cAAc,MAAM,YAAA;gCACpB,OAAO,YACH;oCACE,cAAc,MAAM,KAAA,CAAM,YAAA;oCAC1B,kBAAkB,MAAM,KAAA,CAAM,gBAAA;gCAChC,IACA,KAAA;gCACJ,aAAa,MAAM,WAAA;4BACrB;4BAEF;wBACF;oBAEA,KAAK;wBAAU;4BACb,WAAW,OAAA,CACT,CAAA,GAAA,8KAAA,CAAA,mBAAA,EAAiB,kBAAkB;gCACjC,cAAc,MAAM,YAAA;gCACpB,OAAO,YACH;oCACE,cAAc,MAAM,KAAA,CAAM,YAAA;oCAC1B,kBAAkB,MAAM,KAAA,CAAM,gBAAA;gCAChC,IACA,KAAA;4BACN;4BAEF;wBACF;oBAEA;wBAAS;4BACP,MAAM,kBAAyB;4BAC/B,MAAM,IAAI,MAAM,CAAA,oBAAA,EAAuB,gBAAe,CAAE;wBAC1D;gBACF;YACF;QACF;QAEA,OAAO,IAAA,CAAK,UAAA,CACT,WAAA,CAAY,qBACZ,WAAA,CAAY,wBACZ,WAAA,CAAY,IAAI;IACrB;IAEA,uBACE,QAAA,EACA,IAAA,EACM;QACN,OAAO,IAAA,CAAK,wBAAA,CAAyB,UAAU;IACjD;IAEA,yBACE,QAAA,EACA,OAAA,EAQA;QACA,MAAM,OACJ,WAAW,OACP,KAAA,IACA,UAAU,UACV,QAAQ,IAAA,GACR;YACE,SAAS,aAAa,UAAU,QAAQ,OAAA,GAAU,KAAA;YAClD,QAAQ,YAAY,UAAU,QAAQ,MAAA,GAAS,KAAA;YAC/C,YACE,gBAAgB,UAAU,QAAQ,UAAA,GAAa,KAAA;QACnD;QAEN,MAAM,OACJ,WAAW,OACP,KAAA,IACA,UAAU,UACV,QAAQ,IAAA,GACR,KAAA;QAEN,MAAMA,mBACJ,WAAW,OACP,KAAA,IACA,qBAAqB,UACrB,QAAQ,eAAA,GACR,KAAA;QAEN,MAAM,YACJ,WAAW,OACP,KAAA,IACA,eAAe,UACf,QAAQ,SAAA,GACR,KAAA;QAEN,sBAAsB;YACpB;YACA,QAAQ,QAAA,OAAA,KAAA,IAAA,KAAM,MAAA;YACd,YAAY,QAAA,OAAA,KAAA,IAAA,KAAM,UAAA;YAClB,SAAS,2BAA2B,MAAM;gBACxC,aAAa;gBACb,mBAAmB;YACrB;YACA,QAAQ,IAAA,CAAK,YAAA,CAAa;gBAAE;gBAAM,iBAAAA;gBAAiB;YAAU;QAC/D;IACF;IAEA,yBAAyB,QAAA,EAA0B,IAAA,EAAqB;QACtE,sBAAsB;YACpB;YACA,QAAQ,QAAA,OAAA,KAAA,IAAA,KAAM,MAAA;YACd,YAAY,QAAA,OAAA,KAAA,IAAA,KAAM,UAAA;YAClB,SAAS,2BAA2B,MAAM;gBACxC,aAAa;YACf;YACA,QAAQ,IAAA,CAAK,UAAA,CAAW,WAAA,CAAY,IAAI;QAC1C;IACF;IAEA,mBACE,OAAA,EACU;QACV,OAAO,IAAA,CAAK,oBAAA,CAAqB;IACnC;IAEA,aAAa,OAAA,EAIV;QACD,MAAM,SAAS,IAAA,CAAK,oBAAA,CAAqB;YACvC,iBAAiB,WAAA,OAAA,KAAA,IAAA,QAAS,eAAA;YAC1B,WAAW,WAAA,OAAA,KAAA,IAAA,QAAS,SAAA;QACtB;QAEA,OAAA,CAAO,WAAA,OAAA,KAAA,IAAA,QAAS,IAAA,IAAO,aAAa,WAAA,OAAA,KAAA,IAAA,QAAS,IAAA,CAAK,MAAA,EAAQ,UAAU;IACtE;IAEA,qBACE,OAAA,EAQU;QA10Cd,IAAAT;QA20CI,MAAM,OACJ,WAAW,OACP,KAAA,IACA,UAAU,UACV,QAAQ,IAAA,GACR;YACE,SAAS,aAAa,UAAU,QAAQ,OAAA,GAAU,KAAA;YAClD,QAAQ,YAAY,UAAU,QAAQ,MAAA,GAAS,KAAA;YAC/C,YACE,gBAAgB,UAAU,QAAQ,UAAA,GAAa,KAAA;QACnD;QAEN,MAAM,OACJ,WAAW,OACP,KAAA,IACA,UAAU,UACV,QAAQ,IAAA,GACR,KAAA;QAEN,MAAMS,mBACJ,WAAW,OACP,KAAA,IACA,qBAAqB,UACrB,QAAQ,eAAA,GACR,KAAA;QAEN,MAAM,YACJ,WAAW,OACP,KAAA,IACA,eAAe,UACf,QAAQ,SAAA,GACR,KAAA;QAEN,OAAO,IAAI,SACT,IAAA,CAAK,YAAA,CAAa;YAAE;YAAM,iBAAAA;YAAiB;QAAU,IACrD;YACE,QAAA,CAAQT,OAAA,QAAA,OAAA,KAAA,IAAA,KAAM,MAAA,KAAN,OAAAA,OAAgB;YACxB,YAAY,QAAA,OAAA,KAAA,IAAA,KAAM,UAAA;YAClB,SAAS,uBAAuB,MAAM;gBACpC,aAAa;gBACb,mBAAmB;YACrB;QACF;IAEJ;IAEA,qBAAqB,IAAA,EAA+B;QAz3CtD,IAAAA;QA03CI,OAAO,IAAI,SAAS,IAAA,CAAK,UAAA,CAAW,WAAA,CAAY,IAAI,sBAAsB;YACxE,QAAA,CAAQA,OAAA,QAAA,OAAA,KAAA,IAAA,KAAM,MAAA,KAAN,OAAAA,OAAgB;YACxB,SAAS,uBAAuB,MAAM;gBACpC,aAAa;YACf;QACF;IACF;AACF;AAKO,IAAM,0BAA0B;;AIv3ChC,IAAM,iCAAiC,CAAC,EAC7C,KAAA,EACA,YAAY,EAAE,eAAA,EAAiB,YAAA,EAAc,UAAA,EAAW,EACxD,OAAA,EACA,UAAA,EACF;IAME,eAAe,YAAY,EACzB,MAAA,EACA,IAAA,EACF;QAIE,OAAO,kBAAkB,MAAM,gBAAgB;YAAE;YAAQ;QAAK,KAAK;IACrE;IAEA,OAAO;QACL,sBAAsB;QAEtB,UAAU,cAAA,OAAA,aAAc,MAAM,QAAA;QAC9B,SAAS,WAAA,OAAA,UAAW,MAAM,OAAA;QAE1B,6BAA6B,MAAM,2BAAA;QACnC,mBAAmB,MAAM,iBAAA;QACzB,2BAA2B,MAAM,yBAAA;QAEjC,MAAM,YACJ,MAAA;YAEA,MAAM,oBAAoB,MAAM,YAAY;gBAAE;gBAAQ,MAAM;YAAW;YACvE,MAAM,aAAa,UAAY,MAAM,UAAA,CAAW;YAChD,OAAO,eACH,aAAa;gBAAE;gBAAY,QAAQ;gBAAmB;YAAM,KAC5D;QACN;QAEA,MAAM,UACJ,MAAA;YAEA,MAAM,oBAAoB,MAAM,YAAY;gBAAE;gBAAQ,MAAM;YAAS;YACrE,MAAM,WAAW,UAAY,MAAM,QAAA,CAAS;YAC5C,OAAO,aACH,WAAW;gBAAE;gBAAU,QAAQ;gBAAmB;YAAM,KACxD;QACN;IACF;AACF;;ACpDO,SAAS,mBAAmB,WAAA;IAdnC,IAAAA,MAAA,IAAA;IAeE,MAAM,QAAuB,EAAC;IAE9B,KAAA,MAAW,cAAc,YAAa;QACpC,IAAI;QAEJ,IAAI;YACF,MAAM,IAAI,IAAI,WAAW,GAAG;QAC9B,EAAA,OAAS,OAAO;YACd,MAAM,IAAI,MAAM,CAAA,aAAA,EAAgB,WAAW,GAAG,CAAA,CAAE;QAClD;QAEA,OAAQ,IAAI,QAAA;YACV,KAAK;YACL,KAAK;gBAAU;oBACb,IAAA,CAAIA,OAAA,WAAW,WAAA,KAAX,OAAA,KAAA,IAAAA,KAAwB,UAAA,CAAW,WAAW;wBAChD,MAAM,IAAA,CAAK;4BAAE,MAAM;4BAAS,OAAO;wBAAI;oBACzC,OAAO;wBACL,IAAI,CAAC,WAAW,WAAA,EAAa;4BAC3B,MAAM,IAAI,MACR;wBAEJ;wBAEA,MAAM,IAAA,CAAK;4BACT,MAAM;4BACN,MAAM;4BACN,UAAU,WAAW,WAAA;wBACvB;oBACF;oBACA;gBACF;YAEA,KAAK;gBAAS;oBACZ,IAAI;oBACJ,IAAI;oBACJ,IAAI;oBAEJ,IAAI;wBACF,CAAC,QAAQ,cAAa,GAAI,WAAW,GAAA,CAAI,KAAA,CAAM;wBAC/C,WAAW,OAAO,KAAA,CAAM,IAAG,CAAE,EAAC,CAAE,KAAA,CAAM,IAAG,CAAE,EAAC;oBAC9C,EAAA,OAAS,OAAO;wBACd,MAAM,IAAI,MAAM,CAAA,2BAAA,EAA8B,WAAW,GAAG,CAAA,CAAE;oBAChE;oBAEA,IAAI,YAAY,QAAQ,iBAAiB,MAAM;wBAC7C,MAAM,IAAI,MAAM,CAAA,yBAAA,EAA4B,WAAW,GAAG,CAAA,CAAE;oBAC9D;oBAEA,IAAA,CAAI,KAAA,WAAW,WAAA,KAAX,OAAA,KAAA,IAAA,GAAwB,UAAA,CAAW,WAAW;wBAChD,MAAM,IAAA,CAAK;4BACT,MAAM;4BACN,OAAO,+BAA+B;wBACxC;oBACF,OAAA,IAAA,CAAW,KAAA,WAAW,WAAA,KAAX,OAAA,KAAA,IAAA,GAAwB,UAAA,CAAW,UAAU;wBACtD,MAAM,IAAA,CAAK;4BACT,MAAM;4BACN,MAAM,wBACJ,+BAA+B;wBAEnC;oBACF,OAAO;wBACL,IAAI,CAAC,WAAW,WAAA,EAAa;4BAC3B,MAAM,IAAI,MACR;wBAEJ;wBAEA,MAAM,IAAA,CAAK;4BACT,MAAM;4BACN,MAAM;4BACN,UAAU,WAAW,WAAA;wBACvB;oBACF;oBAEA;gBACF;YAEA;gBAAS;oBACP,MAAM,IAAI,MAAM,CAAA,0BAAA,EAA6B,IAAI,QAAQ,CAAA,CAAE;gBAC7D;QACF;IACF;IAEA,OAAO;AACT;;AC1EO,SAAS,sBAAsB,QAAA;IACpC,MAAM,eAA8B,EAAC;IAErC,KAAA,MAAW,WAAW,SAAU;QAC9B,MAAM,EAAE,IAAA,EAAM,OAAA,EAAS,eAAA,EAAiB,wBAAA,EAAyB,GAC/D;QAEF,OAAQ;YACN,KAAK;gBAAU;oBACb,aAAa,IAAA,CAAK;wBAChB,MAAM;wBACN;oBACF;oBACA;gBACF;YAEA,KAAK;gBAAQ;oBACX,aAAa,IAAA,CAAK;wBAChB,MAAM;wBACN,SAAS,2BACL;4BACE;gCAAE,MAAM;gCAAQ,MAAM;4BAAQ;+BAC3B,mBAAmB;yBACxB,GACA;oBACN;oBACA;gBACF;YAEA,KAAK;gBAAa;oBAChB,IAAI,mBAAmB,MAAM;wBAC3B,aAAa,IAAA,CAAK;4BAAE,MAAM;4BAAa;wBAAQ;wBAC/C;oBACF;oBAGA,aAAa,IAAA,CAAK;wBAChB,MAAM;wBACN,SAAS;4BACP;gCAAE,MAAM;gCAAQ,MAAM;4BAAQ;+BAC3B,gBAAgB,GAAA,CAAI,CAAC,EAAE,UAAA,EAAY,QAAA,EAAU,IAAA,EAAK,GAAA,CAAO;oCAC1D,MAAM;oCACN;oCACA;oCACA;gCACF,CAAA;yBACF;oBACF;oBAGA,aAAa,IAAA,CAAK;wBAChB,MAAM;wBACN,SAAS,gBAAgB,GAAA,CAAI,CAAA;4BAC3B,IAAI,CAAA,CAAE,YAAY,cAAA,GAAiB;gCACjC,MAAM,IAAI,uBAAuB;oCAC/B,iBAAiB;oCACjB,SACE,wCACA,KAAK,SAAA,CAAU;gCACnB;4BACF;4BAEA,MAAM,EAAE,UAAA,EAAY,QAAA,EAAU,IAAA,EAAM,MAAA,EAAO,GAAI;4BAE/C,OAAO;gCACL,MAAM;gCACN;gCACA;gCACA;gCACA;4BACF;wBACF;oBACF;oBAEA;gBACF;YAEA,KAAK;YACL,KAAK;YACL,KAAK;gBAAQ;oBAEX;gBACF;YAEA;gBAAS;oBACP,MAAM,mBAA0B;oBAChC,MAAM,IAAI,uBAAuB;wBAC/B,iBAAiB;wBACjB,SAAS,CAAA,kBAAA,EAAqB,iBAAgB,CAAA;oBAChD;gBACF;QACF;IACF;IAEA,OAAO;AACT;;ACzGO,SAAS,4BAA4B,EAC1C,cAAA,EACA,mBAAA,EACA,gBAAA,EACF;IAKE,OAAO;QACL,eAAc,OAAA;YACZ,IAAI,kBAAkB,QAAQ,WAAW,gBAAgB;gBACvD,OAAO,cAAA,CAAe,QAAO;YAC/B;YAEA,IAAI,kBAAkB;gBACpB,OAAO,iBAAiB,aAAA,CAAc;YACxC;YAEA,MAAM,IAAImC,iLAAAA,CAAAA,mBAAAA,CAAiB;gBAAE;gBAAS,WAAW;YAAgB;QACnE;QAEA,oBAAmB,OAAA;YACjB,IAAI,uBAAuB,QAAQ,WAAW,qBAAqB;gBACjE,OAAO,mBAAA,CAAoB,QAAO;YACpC;YAEA,IAAI,kBAAkB;gBACpB,OAAO,iBAAiB,kBAAA,CAAmB;YAC7C;YAEA,MAAM,IAAIA,iLAAAA,CAAAA,mBAAAA,CAAiB;gBAAE;gBAAS,WAAW;YAAqB;QACxE;IACF;AACF;;AC/CA,IAAMlC,SAAO;AACb,IAAMK,WAAS,CAAA,gBAAA,EAAmBL,OAAI,CAAA;AACtC,IAAMM,WAAS,OAAO,GAAA,CAAID;AAJ1B,IAAAN;AAMO,IAAM,sBAAN,cAAkCmC,iLAAAA,CAAAA,mBAAAA;IAMvC,YAAY,EACV,OAAA,EACA,SAAA,EACA,UAAA,EACA,kBAAA,EACA,UAAU,CAAA,kBAAA,EAAqB,WAAU,uBAAA,EAA0B,mBAAmB,IAAA,GAAM,CAAA,CAAA,EAC9F,CAMG;QACD,KAAA,CAAM;YAAE,WAAWlC;YAAM;YAAS;YAAW;QAAQ;QAlBvD,IAAA,CAAkBD,KAAAA,GAAU;QAoB1B,IAAA,CAAK,UAAA,GAAa;QAClB,IAAA,CAAK,kBAAA,GAAqB;IAC5B;IAEA,OAAO,WAAW,KAAA,EAA8C;QAC9D,OAAOQ,iLAAAA,CAAAA,aAAAA,CAAW,SAAA,CAAU,OAAOF;IACrC;IAAA;;GAAA,GAKA,OAAO,sBAAsB,KAAA,EAA8C;QACzE,OACE,iBAAiB,SACjB,MAAM,IAAA,KAASL,UACf,OAAQ,MAA8B,UAAA,KAAe,YACrD,MAAM,OAAA,CAAS,MAA8B,kBAAkB;IAEnE;IAAA;;GAAA,GAKA,SAAS;QACP,OAAO;YACL,MAAM,IAAA,CAAK,IAAA;YACX,SAAS,IAAA,CAAK,OAAA;YACd,OAAO,IAAA,CAAK,KAAA;YAEZ,SAAS,IAAA,CAAK,OAAA;YACd,WAAW,IAAA,CAAK,SAAA;YAEhB,YAAY,IAAA,CAAK,UAAA;YACjB,oBAAoB,IAAA,CAAK,kBAAA;QAC3B;IACF;AACF;AAxDoBD,OAAAO;;ACab,SAAS,oCACd,SAAA;IAEA,MAAM,WAAW,IAAI;IAErB,KAAA,MAAW,CAAC,IAAI,SAAQ,IAAK,OAAO,OAAA,CAAQ,WAAY;QACtD,SAAS,gBAAA,CAAiB;YAAE;YAAI;QAAS;IAC3C;IAEA,OAAO;AACT;AAKO,IAAM,mCACX;AAEF,IAAM,0BAAN;IAAA,aAAA;QACE,IAAA,CAAQ,SAAA,GAA8D,CAAC;IAAA;IAEvE,iBAAiB,EACf,EAAA,EACA,QAAA,EACF,EAGS;QACP,IAAA,CAAK,SAAA,CAAU,GAAE,GAAI;IACvB;IAEQ,YAAY,EAAA,EAA8C;QAChE,MAAM,WAAW,IAAA,CAAK,SAAA,CAAU,GAAE;QAElC,IAAI,YAAY,MAAM;YACpB,MAAM,IAAI,oBAAoB;gBAC5B,SAAS;gBACT,WAAW;gBACX,YAAY;gBACZ,oBAAoB,OAAO,IAAA,CAAK,IAAA,CAAK,SAAS;YAChD;QACF;QAEA,OAAO;IACT;IAEQ,QACN,EAAA,EACA,SAAA,EACkB;QAClB,MAAM,QAAQ,GAAG,OAAA,CAAQ;QAEzB,IAAI,UAAU,CAAA,GAAI;YAChB,MAAM,IAAI4B,iLAAAA,CAAAA,mBAAAA,CAAiB;gBACzB,SAAS;gBACT;gBACA,SACE,CAAA,QAAA,EAAW,UAAS,kBAAA,EAAqB,GAAE,6CAAA,CAAA;YAE/C;QACF;QAEA,OAAO;YAAC,GAAG,KAAA,CAAM,GAAG;YAAQ,GAAG,KAAA,CAAM,QAAQ;SAAE;IACjD;IAEA,cAAc,EAAA,EAA2B;QArF3C,IAAAnC,MAAA;QAsFI,MAAM,CAAC,YAAY,QAAO,GAAI,IAAA,CAAK,OAAA,CAAQ,IAAI;QAC/C,MAAM,QAAA,CAAQ,KAAA,CAAAA,OAAA,IAAA,CAAK,WAAA,CAAY,WAAU,EAAE,aAAA,KAA7B,OAAA,KAAA,IAAA,GAAA,IAAA,CAAAA,MAA6C;QAE3D,IAAI,SAAS,MAAM;YACjB,MAAM,IAAImC,iLAAAA,CAAAA,mBAAAA,CAAiB;gBAAE,SAAS;gBAAI,WAAW;YAAgB;QACvE;QAEA,OAAO;IACT;IAEA,mBAAmB,EAAA,EAAoC;QAhGzD,IAAAnC,MAAA,IAAA;QAiGI,MAAM,CAAC,YAAY,QAAO,GAAI,IAAA,CAAK,OAAA,CAAQ,IAAI;QAC/C,MAAM,WAAW,IAAA,CAAK,WAAA,CAAY;QAElC,MAAM,QAAA,CACJ,KAAA,CAAAA,OAAA,SAAS,kBAAA,KAAT,OAAA,KAAA,IAAAA,KAAA,IAAA,CAAA,UAA8B,QAAA,KAA9B,OAAA,KACC,mBAAmB,WAAA,CAChB,KAAA,SAAS,aAAA,KAAT,OAAA,KAAA,IAAA,GAAA,IAAA,CAAA,UAAyB,WACzB,KAAA;QAEN,IAAI,SAAS,MAAM;YACjB,MAAM,IAAImC,iLAAAA,CAAAA,mBAAAA,CAAiB;gBACzB,SAAS;gBACT,WAAW;YACb;QACF;QAEA,OAAO;IACT;IAAA;;GAAA,GAKA,cAAc,EAAA,EAAoC;QAChD,OAAO,IAAA,CAAK,kBAAA,CAAmB;IACjC;AACF;;AClEO,SAAS,KACdd,KAAAA;IAEA,OAAOA;AACT;;AClDO,SAAS,iBAAiB,OAAA,EAAmB,OAAA;IAClD,IAAI,QAAQ,MAAA,KAAW,QAAQ,MAAA,EAAQ;QACrC,MAAM,IAAI,MACR,CAAA,4CAAA,EAA+C,QAAQ,MAAM,CAAA,oBAAA,EAAuB,QAAQ,MAAM,CAAA,UAAA,CAAA;IAEtG;IAEA,OACE,WAAW,SAAS,WAAO,CAAK,UAAU,WAAW,UAAU,QAAO;AAE1E;AAQA,SAAS,WAAW,OAAA,EAAmB,OAAA;IACrC,OAAO,QAAQ,MAAA,CACb,CAAC,aAAqB,OAAe,QACnC,cAAc,QAAQ,OAAA,CAAQ,MAAK,EACrC;AAEJ;AAOA,SAAS,UAAU,MAAA;IACjB,OAAO,KAAK,IAAA,CAAK,WAAW,QAAQ;AACtC;;AC8BO,SAAS,6BACd,YAAA;IAEA,MAAM,cAAc,IAAI;IACxB,IAAI;IAEJ,OAAO,IAAI,gBAAgB;QACzB,MAAM,OAAM,UAAA;YACV,oBAAoB,CAAA,GAAA,wJAAA,CAAA,eAAA,EAClB,CAAC;gBACC,IACG,UAAU,SACT,MAAM,IAAA,KAAS,WACf,MAAM,IAAA,KAAS,YAAA,6DAAA;gBAAA,4CAAA;gBAGhB,MAAc,KAAA,KAAU,QACzB;oBACA,WAAW,SAAA;oBACX;gBACF;gBAEA,IAAI,UAAU,OAAO;oBACnB,MAAM,gBAAgB,eAClB,aAAa,MAAM,IAAA,EAAM;wBACvB,OAAO,MAAM,KAAA;oBACf,KACA,MAAM,IAAA;oBACV,IAAI,eAAe,WAAW,OAAA,CAAQ;gBACxC;YACF;QAEJ;QAEA,WAAU,KAAA;YACR,kBAAkB,IAAA,CAAK,YAAY,MAAA,CAAO;QAC5C;IACF;AACF;AAwBO,SAAS,2BACd,EAAA;IAEA,MAAM,cAAc,IAAI;IACxB,IAAI,qBAAqB;IACzB,MAAM,YAAY,MAAM,CAAC;IAEzB,OAAO,IAAI,gBAAgB;QACzB,MAAM;YACJ,IAAI,UAAU,OAAA,EAAS,MAAM,UAAU,OAAA;QACzC;QAEA,MAAM,WAAU,OAAA,EAAS,UAAA;YACvB,MAAM,UAAU,OAAO,YAAY,WAAW,UAAU,QAAQ,OAAA;YAEhE,WAAW,OAAA,CAAQ,YAAY,MAAA,CAAO;YAEtC,sBAAsB;YAEtB,IAAI,UAAU,OAAA,EAAS,MAAM,UAAU,OAAA,CAAQ;YAC/C,IAAI,UAAU,MAAA,IAAU,OAAO,YAAY,UAAU;gBACnD,MAAM,UAAU,MAAA,CAAO;YACzB;QACF;QAEA,MAAM;YACJ,MAAM,oBAAoB,8BAA8B;YAGxD,IAAI,UAAU,YAAA,EAAc;gBAC1B,MAAM,UAAU,YAAA,CAAa;YAC/B;YAEA,IAAI,UAAU,OAAA,IAAW,CAAC,mBAAmB;gBAC3C,MAAM,UAAU,OAAA,CAAQ;YAC1B;QACF;IACF;AACF;AAEA,SAAS,8BACP,SAAA;IAEA,OAAO,iCAAiC;AAC1C;AAgBO,SAAS;IACd,IAAI,gBAAgB;IAEpB,OAAO,CAAC;QACN,IAAI,eAAe;YACjB,OAAO,KAAK,SAAA;YACZ,IAAI,MAAM,gBAAgB;QAC5B;QACA,OAAO;IACT;AACF;AAoBO,SAAS,SACd,QAAA,EACA,YAAA,EACA,SAAA;IAEA,IAAI,CAAC,SAAS,EAAA,EAAI;QAChB,IAAI,SAAS,IAAA,EAAM;YACjB,MAAM,SAAS,SAAS,IAAA,CAAK,SAAA;YAC7B,OAAO,IAAI,eAAe;gBACxB,MAAM,OAAM,UAAA;oBACV,MAAM,EAAE,IAAA,EAAM,KAAA,EAAM,GAAI,MAAM,OAAO,IAAA;oBACrC,IAAI,CAAC,MAAM;wBACT,MAAM,YAAY,IAAI,cAAc,MAAA,CAAO;wBAC3C,WAAW,KAAA,CAAM,IAAI,MAAM,CAAA,gBAAA,EAAmB,UAAS,CAAE;oBAC3D;gBACF;YACF;QACF,OAAO;YACL,OAAO,IAAI,eAAe;gBACxB,OAAM,UAAA;oBACJ,WAAW,KAAA,CAAM,IAAI,MAAM;gBAC7B;YACF;QACF;IACF;IAEA,MAAM,qBAAqB,SAAS,IAAA,IAAQ;IAE5C,OAAO,mBACJ,WAAA,CAAY,6BAA6B,eACzC,WAAA,CAAY,2BAA2B;AAC5C;AAeA,SAAS;IACP,OAAO,IAAI,eAAe;QACxB,OAAM,UAAA;YACJ,WAAW,KAAA;QACb;IACF;AACF;AAMO,SAAS,0BAA6B,QAAA;IAC3C,IAAI,KAAK,QAAA,CAAS,OAAO,aAAa,CAAA;IACtC,OAAO,IAAI,eAAkB;QAC3B,MAAM,MAAK,UAAA;YACT,MAAM,EAAE,IAAA,EAAM,KAAA,EAAM,GAAI,MAAM,GAAG,IAAA;YACjC,IAAI,MAAM,WAAW,KAAA;iBAChB,WAAW,OAAA,CAAQ;QAC1B;QAEA,MAAM,QAAO,MAAA;YApSjB,IAAArB;YAqSM,MAAA,CAAA,CAAMA,OAAA,GAAG,MAAA,KAAH,OAAA,KAAA,IAAAA,KAAA,IAAA,CAAA,IAAY,OAAA;QACpB;IACF;AACF;;;AEpSO,IAAM,iCAAiC,KAAK;;ADE5C,IAAMoC,cAAN;IASL,aAAc;QARd,IAAA,CAAQ,OAAA,GAAU,IAAI;QAEtB,IAAA,CAAQ,UAAA,GAA0D;QAGlE,IAAA,CAAQ,QAAA,GAAoB;QAC5B,IAAA,CAAQ,cAAA,GAAwC;QAG9C,MAAM,OAAO,IAAA;QAEb,IAAA,CAAK,MAAA,GAAS,IAAI,eAAe;YAC/B,OAAO,OAAM;gBACX,KAAK,UAAA,GAAa;gBAGlB,wCAA4C;oBAC1C,KAAK,cAAA,GAAiB,WAAW;wBAC/B,QAAQ,IAAA,CACN;oBAEJ,GAAG;gBACL;YACF;YACA,MAAM,CAAA,cAEN;YACA,QAAQ,CAAA;gBACN,IAAA,CAAK,QAAA,GAAW;YAClB;QACF;IACF;IAEA,MAAM,QAAuB;QAC3B,IAAI,IAAA,CAAK,QAAA,EAAU;YACjB,MAAM,IAAI,MAAM;QAClB;QAEA,IAAI,CAAC,IAAA,CAAK,UAAA,EAAY;YACpB,MAAM,IAAI,MAAM;QAClB;QAEA,IAAA,CAAK,UAAA,CAAW,KAAA;QAChB,IAAA,CAAK,QAAA,GAAW;QAGhB,IAAI,IAAA,CAAK,cAAA,EAAgB;YACvB,aAAa,IAAA,CAAK,cAAc;QAClC;IACF;IAEA,OAAO,KAAA,EAAwB;QAC7B,IAAI,IAAA,CAAK,QAAA,EAAU;YACjB,MAAM,IAAI,MAAM;QAClB;QAEA,IAAI,CAAC,IAAA,CAAK,UAAA,EAAY;YACpB,MAAM,IAAI,MAAM;QAClB;QAEA,IAAA,CAAK,UAAA,CAAW,OAAA,CACd,IAAA,CAAK,OAAA,CAAQ,MAAA,CAAOC,CAAAA,GAAAA,8KAAAA,CAAAA,mBAAAA,EAAiB,QAAQ;YAAC;SAAM;IAExD;IAEA,wBAAwB,KAAA,EAAwB;QAC9C,IAAI,IAAA,CAAK,QAAA,EAAU;YACjB,MAAM,IAAI,MAAM;QAClB;QAEA,IAAI,CAAC,IAAA,CAAK,UAAA,EAAY;YACpB,MAAM,IAAI,MAAM;QAClB;QAEA,IAAA,CAAK,UAAA,CAAW,OAAA,CACd,IAAA,CAAK,OAAA,CAAQ,MAAA,CAAOA,CAAAA,GAAAA,8KAAAA,CAAAA,mBAAAA,EAAiB,uBAAuB;YAAC;SAAM;IAEvE;AACF;AAMO,SAAS;IACd,MAAM,UAAU,IAAI;IACpB,MAAM,UAAU,IAAI;IACpB,OAAO,IAAI,gBAAgB;QACzB,WAAW,OAAO,OAAO;YACvB,MAAM,UAAU,QAAQ,MAAA,CAAO;YAC/B,WAAW,OAAA,CAAQ,QAAQ,MAAA,CAAOA,CAAAA,GAAAA,8KAAAA,CAAAA,mBAAAA,EAAiB,QAAQ;QAC7D;IACF;AACF;AAKO,IAAM,0BAAN,cAAsCD;AAAY;;AEgBzD,SAAS;IACP,IAAI,WAAW;IAEf,OAAO,CAAA;QACL,MAAM,OAAO,KAAK,KAAA,CAAM;QAGxB,IAAI,WAAW,MAAM;YACnB,MAAM,IAAI,MAAM,CAAA,EAAG,KAAK,KAAA,CAAM,IAAI,CAAA,EAAA,EAAK,KAAK,KAAA,CAAM,OAAO,CAAA,CAAE;QAC7D;QAGA,IAAI,CAAA,CAAE,gBAAgB,IAAA,GAAO;YAC3B;QACF;QAMA,MAAM,OAAO,KAAK,UAAA;QAClB,IACE,CAAC,YACA,KAAK,MAAA,GAAS,SAAS,MAAA,IAAU,KAAK,UAAA,CAAW,WAClD;YACA,MAAM,QAAQ,KAAK,KAAA,CAAM,SAAS,MAAM;YACxC,WAAW;YAEX,OAAO;QACT;QAEA,OAAO;IACT;AACF;AAEA,gBAAgB,WACd,MAAA;IAEA,WAAA,MAAiB,SAAS,OAAQ;QAChC,IAAI,gBAAgB,OAAO;YAEzB,MAAM,OAAO,MAAM,UAAA;YACnB,IAAI,MAAM,MAAM;QAClB,OAAA,IAAW,WAAW,OAAO;YAE3B,MAAM,EAAE,KAAA,EAAM,GAAI;YAClB,IAAI,UAAU,OAAO;gBACnB,MAAM,OAAO,MAAM,IAAA;gBACnB,IAAI,MAAM,MAAM;YAClB;QACF;IACF;AACF;AASO,SAAS,gBACd,GAAA,EAIA,EAAA;IAEA,IAAI,OAAO,aAAA,IAAiB,KAAK;QAC/B,OAAO,0BAA0B,WAAW,MACzC,WAAA,CAAY,2BAA2B,KACvC,WAAA,CAAY;IACjB,OAAO;QACL,OAAO,SAAS,KAAK,wBAAwB,IAAI,WAAA,CAC/C;IAEJ;AACF;;AC5IO,SAAS,kBACd,EAAE,QAAA,EAAU,SAAA,EAAU,EACtBE,QAAAA;IAEA,MAAM,SAAS,IAAI,eAAe;QAChC,MAAM,OAAM,UAAA;YA/DhB,IAAAtC;YAgEM,MAAM,cAAc,IAAI;YAExB,MAAM,cAAc,CAAC;gBACnB,WAAW,OAAA,CACT,YAAY,MAAA,CAAOqC,CAAAA,GAAAA,8KAAAA,CAAAA,mBAAAA,EAAiB,qBAAqB;YAE7D;YAEA,MAAM,kBAAkB,CAAC;gBACvB,WAAW,OAAA,CACT,YAAY,MAAA,CAAOA,CAAAA,GAAAA,8KAAAA,CAAAA,mBAAAA,EAAiB,gBAAgB;YAExD;YAEA,MAAM,YAAY,CAAC;gBACjB,WAAW,OAAA,CACT,YAAY,MAAA,CAAOA,CAAAA,GAAAA,8KAAAA,CAAAA,mBAAAA,EAAiB,SAAS;YAEjD;YAEA,MAAM,gBAAgB,OAAOR;gBApFnC,IAAA7B,MAAA;gBAqFQ,IAAI,SAA0B,KAAA;gBAE9B,WAAA,MAAiB,SAAS6B,QAAQ;oBAChC,OAAQ,MAAM,KAAA;wBACZ,KAAK;4BAA0B;gCAC7B,WAAW,OAAA,CACT,YAAY,MAAA,CACVQ,CAAAA,GAAAA,8KAAAA,CAAAA,mBAAAA,EAAiB,qBAAqB;oCACpC,IAAI,MAAM,IAAA,CAAK,EAAA;oCACf,MAAM;oCACN,SAAS;wCAAC;4CAAE,MAAM;4CAAQ,MAAM;gDAAE,OAAO;4CAAG;wCAAE;qCAAC;gCACjD;gCAGJ;4BACF;wBAEA,KAAK;4BAAwB;gCAC3B,MAAM,UAAA,CAAUrC,OAAA,MAAM,IAAA,CAAK,KAAA,CAAM,OAAA,KAAjB,OAAA,KAAA,IAAAA,IAAAA,CAA2B,EAAA;gCAE3C,IAAA,CAAI,WAAA,OAAA,KAAA,IAAA,QAAS,IAAA,MAAS,UAAA,CAAA,CAAU,KAAA,QAAQ,IAAA,KAAR,OAAA,KAAA,IAAA,GAAc,KAAA,KAAS,MAAM;oCAC3D,WAAW,OAAA,CACT,YAAY,MAAA,CACVqC,CAAAA,GAAAA,8KAAAA,CAAAA,mBAAAA,EAAiB,QAAQ,QAAQ,IAAA,CAAK,KAAK;gCAGjD;gCAEA;4BACF;wBAEA,KAAK;wBACL,KAAK;4BAA8B;gCACjC,SAAS,MAAM,IAAA;gCACf;4BACF;oBACF;gBACF;gBAEA,OAAO;YACT;YAGA,WAAW,OAAA,CACT,YAAY,MAAA,CACVA,CAAAA,GAAAA,8KAAAA,CAAAA,mBAAAA,EAAiB,0BAA0B;gBACzC;gBACA;YACF;YAIJ,IAAI;gBACF,MAAMC,SAAQ;oBACZ;oBACA;oBACA;oBACA;oBACA;gBACF;YACF,EAAA,OAAS,OAAO;gBACd,UAAA,CAAWtC,OAAA,MAAc,OAAA,KAAd,OAAAA,OAAyB,CAAA,EAAG,MAAK,CAAE;YAChD,SAAE;gBACA,WAAW,KAAA;YACb;QACF;QACA,MAAK,UAAA,GAAa;QAClB,WAAU;IACZ;IAEA,OAAO,IAAI,SAAS,QAAQ;QAC1B,QAAQ;QACR,SAAS;YACP,gBAAgB;QAClB;IACF;AACF;AAKO,IAAM,iCAAiC;;ACzJ9C,gBAAgB,gBACd,QAAA,EACA,yBAAA;IAfF,IAAAA,MAAA;IAiBE,MAAM,UAAU,IAAI;IACpB,WAAA,MAAiB,SAAA,CAASA,OAAA,SAAS,IAAA,KAAT,OAAAA,OAAiB,EAAC,CAAG;QAC7C,MAAM,QAAA,CAAQ,KAAA,MAAM,KAAA,KAAN,OAAA,KAAA,IAAA,GAAa,KAAA;QAE3B,IAAI,SAAS,MAAM;YACjB,MAAM,YAAY,QAAQ,MAAA,CAAO;YACjC,MAAM,YAAY,KAAK,KAAA,CAAM;YAC7B,MAAM,QAAQ,0BAA0B;YAExC,IAAI,SAAS,MAAM;gBACjB,MAAM;YACR;QACF;IACF;AACF;AAEO,SAAS,kCACd,QAAA,EACA,SAAA;IAEA,OAAO,iBAAiB,UAAU,WAAW,CAAA;QArC/C,IAAAA;QAqCwD,OAAA,CAAAA,OAAA,MAAM,KAAA,KAAN,OAAA,KAAA,IAAAA,KAAa,IAAA;IAAA;AACrE;AAEO,SAAS,0BACd,QAAA,EACA,SAAA;IAEA,OAAO,iBAAiB,UAAU,WAAW,CAAA,QAAS,MAAM,UAAU;AACxE;AAEO,SAAS,uBACd,QAAA,EACA,SAAA;IAEA,OAAO,iBAAiB,UAAU,WAAW,CAAA,QAAS,SAAA,OAAA,KAAA,IAAA,MAAO,IAAI;AACnE;AAEO,SAAS,uBACd,QAAA,EACA,SAAA;IAEA,OAAO,iBAAiB,UAAU,WAAW,CAAA,QAAS,MAAM,UAAU;AACxE;AAEO,SAAS,iBACd,QAAA,EACA,SAAA,EACA,yBAAA;IAEA,OAAO,0BACL,gBAAgB,UAAU,4BAEzB,WAAA,CAAY,2BAA2B,YACvC,WAAA,CAAY;AACjB;;AChEA,IAAM,cAAc,IAAI,YAAY;AAepC,eAAe,aACb,KAAA,EACA,UAAA;IAEA,KAAA,MAAW,QAAQ,MAAO;QACxB,MAAM,EAAE,IAAA,EAAM,WAAA,EAAY,GAAI,KAAK,KAAA,CAAM;QAGzC,IAAI,CAAC,aAAa;YAChB,WAAW,OAAA,CAAQ;QACrB;IACF;AACF;AAEA,eAAe,oBACb,MAAA,EACA,UAAA;IAEA,IAAI,UAAU;IAEd,MAAO,KAAM;QACX,MAAM,EAAE,OAAO,KAAA,EAAO,IAAA,EAAK,GAAI,MAAM,OAAO,IAAA;QAC5C,IAAI,MAAM;YACR;QACF;QAEA,WAAW,YAAY,MAAA,CAAO,OAAO;YAAE,QAAQ;QAAK;QAEpD,MAAM,aAAa,QAAQ,KAAA,CAAM;QACjC,UAAU,WAAW,GAAA,MAAS;QAE9B,MAAM,aAAa,YAAY;IACjC;IAEA,IAAI,SAAS;QACX,MAAM,aAAa;YAAC;SAAO;QAC3B,MAAM,aAAa,YAAY;IACjC;IAEA,WAAW,KAAA;AACb;AAEA,SAASuC,cAAa,GAAA;IAhEtB,IAAAvC;IAiEE,MAAM,SAAA,CAASA,OAAA,IAAI,IAAA,KAAJ,OAAA,KAAA,IAAAA,KAAU,SAAA;IAEzB,OAAO,IAAI,eAAuB;QAChC,MAAM,OAAM,UAAA;YACV,IAAI,CAAC,QAAQ;gBACX,WAAW,KAAA;gBACX;YACF;YAEA,MAAM,oBAAoB,QAAQ;QACpC;IACF;AACF;AAEA,gBAAgBwC,YAAW,MAAA;IACzB,WAAA,MAAiB,SAAS,OAAQ;QAChC,IAAI,MAAM,SAAA,KAAc,mBAAmB;YACzC,MAAM,OAAO,MAAM,IAAA;YACnB,IAAI,MAAM,MAAM;QAClB;IACF;AACF;AAEO,SAAS,aACd,MAAA,EACA,SAAA;IAEA,IAAI,OAAO,aAAA,IAAiB,QAAQ;QAClC,OAAO,0BAA0BA,YAAW,SACzC,WAAA,CAAY,2BAA2B,YACvC,WAAA,CAAY;IACjB,OAAO;QACL,OAAOD,cAAa,QACjB,WAAA,CAAY,2BAA2B,YACvC,WAAA,CAAY;IACjB;AACF;;ACrEA,gBAAgBC,YAAW,QAAA;IAhC3B,IAAAxC,MAAA,IAAA;IAmCE,WAAA,MAAiB,SAAS,SAAS,MAAA,CAAQ;QACzC,MAAM,QAAA,CAAQ,KAAA,CAAA,KAAA,CAAAA,OAAA,MAAM,UAAA,KAAN,OAAA,KAAA,IAAAA,IAAAA,CAAmB,EAAA,KAAnB,OAAA,KAAA,IAAA,GAAuB,OAAA,KAAvB,OAAA,KAAA,IAAA,GAAgC,KAAA;QAE9C,IAAI,UAAU,KAAA,GAAW;YACvB;QACF;QAEA,MAAM,YAAY,KAAA,CAAM,EAAC;QAEzB,IAAI,OAAO,UAAU,IAAA,KAAS,UAAU;YACtC,MAAM,UAAU,IAAA;QAClB;IACF;AACF;AAKO,SAAS,yBACd,QAAA,EAGA,EAAA;IAEA,OAAO,0BAA0BwC,YAAW,WACzC,WAAA,CAAY,2BAA2B,KACvC,WAAA,CAAY;AACjB;;ACvDA,SAASD,cAAa,GAAA;IACpB,MAAM,oBAAoB;IAC1B,OAAO,IAAI,eAAuB;QAChC,MAAM,MAAK,UAAA;YAVf,IAAAvC,MAAA;YAWM,MAAM,EAAE,KAAA,EAAO,IAAA,EAAK,GAAI,MAAM,IAAI,IAAA;YAElC,IAAI,MAAM;gBACR,WAAW,KAAA;gBACX;YACF;YAEA,MAAM,OAAO,kBAAA,CAAkB,KAAA,CAAAA,OAAA,MAAM,KAAA,KAAN,OAAA,KAAA,IAAAA,KAAa,IAAA,KAAb,OAAA,KAAqB;YACpD,IAAI,CAAC,MAAM;YAGX,IAAI,MAAM,cAAA,IAAkB,QAAQ,MAAM,cAAA,CAAe,MAAA,GAAS,GAAG;gBACnE;YACF;YAKA,IAAI,SAAS,UAAU,SAAS,mBAAmB,SAAS,WAAW;gBACrE;YACF;YAEA,WAAW,OAAA,CAAQ;QACrB;IACF;AACF;AAEO,SAAS,kBACd,GAAA,EACA,SAAA;IAEA,OAAOuC,cAAa,KACjB,WAAA,CAAY,2BAA2B,YACvC,WAAA,CAAY;AACjB;;ACnBO,SAAS,aACd,GAAA,EACA,SAAA;IAEA,IAAI,CAAC,IAAI,IAAA,EAAM;QACb,MAAM,IAAI,MAAM;IAClB;IAEA,IAAI,kBAAkB;IACtB,IAAI;IAEJ,MAAM,oBAAoC,CAAC,MAAc;QArC3D,IAAAvC,MAAA;QAsCI,MAAM,EAAE,KAAA,EAAM,GAAI;QAElB,IAAI,UAAU,iBAAiB;YAC7B,gBAAgB,KAAK,KAAA,CAAM;YAC3B,CAAAA,OAAA,aAAA,OAAA,KAAA,IAAA,UAAW,cAAA,KAAX,OAAA,KAAA,IAAAA,KAAA,IAAA,CAAA,WAA4B;QAC9B;QAEA,IAAI,UAAU,iBAAiB;YAC7B,MAAM,qBAAqB,KAAK,KAAA,CAAM;YACtC,kBAAA,CAAkB,KAAA,mBAAmB,eAAA,KAAnB,OAAA,KAAsC;YACxD,OAAO,mBAAmB,aAAA;QAC5B;QACA;IACF;IAEA,IAAI,EAAE,cAAA,EAAgB,GAAG,sBAAqB,GAAI,aAAa,CAAC;IAGhE,uBAAuB;QACrB,GAAG,oBAAA;QACH,SAAS,CAAA;YA1Db,IAAAA;YA2DM,MAAM,wBAA+C;gBACnD;gBACA;YACF;YACA,CAAAA,OAAA,aAAA,OAAA,KAAA,IAAA,UAAW,OAAA,KAAX,OAAA,KAAA,IAAAA,KAAA,IAAA,CAAA,WAAqB,YAAY;QACnC;IACF;IAEA,OAAO,SAAS,KAAK,mBAAmB,sBAAsB,WAAA,CAC5D;AAEJ;;ACtEA,IAAA,4BAAA,CAAA;AAAA,SAAA,2BAAA;IAAA,YAAA,IAAA;IAAA,cAAA,IAAA;IAAA,sBAAA,IAAA;AAAA;AAwDO,SAAS,WACd,MAAA,EAIA,SAAA;IAEA,OAAO,aAAa,QAAQ;AAC9B;AASO,SAAS,aACd,MAAA,EAIA,SAAA;IAEA,OAAO,OACJ,WAAA,CACC,IAAI,gBAEF;QACA,WAAW,OAAO,OAAO;YArFjC,IAAAA;YAuFU,IAAI,OAAO,UAAU,UAAU;gBAC7B,WAAW,OAAA,CAAQ;gBACnB;YACF;YAGA,IAAI,WAAW,OAAO;gBAEpB,IAAI,MAAM,KAAA,KAAU,wBAAwB;oBAC1C,sBAAA,CACEA,OAAA,MAAM,IAAA,KAAN,OAAA,KAAA,IAAAA,KAAY,KAAA,EACZ;gBAEJ;gBACA;YACF;YAGA,sBAAsB,OAAO;QAC/B;IACF,IAED,WAAA,CAAY,2BAA2B,YACvC,WAAA,CAAY;AACjB;AAEO,SAAS,qBACd,MAAA,EAIA,OAAA;IAtHF,IAAAA;IA4HE,MAAM,aAAa,aAAa,QAAQ,WAAA,OAAA,KAAA,IAAA,QAAS,SAAS;IAC1D,MAAM,OAAO,WAAA,OAAA,KAAA,IAAA,QAAS,IAAA;IACtB,MAAM,OAAO,WAAA,OAAA,KAAA,IAAA,QAAS,IAAA;IAEtB,MAAM,iBAAiB,OACnB,aAAa,KAAK,MAAA,EAAQ,cAC1B;IAEJ,OAAO,IAAI,SAAS,gBAAgB;QAClC,QAAA,CAAQA,OAAA,QAAA,OAAA,KAAA,IAAA,KAAM,MAAA,KAAN,OAAAA,OAAgB;QACxB,YAAY,QAAA,OAAA,KAAA,IAAA,KAAM,UAAA;QAClB,SAAS,uBAAuB,MAAM;YACpC,aAAa;YACb,mBAAmB;QACrB;IACF;AACF;AAEA,SAAS,sBACP,KAAA,EACA,UAAA;IAEA,IAAI,OAAO,MAAM,OAAA,KAAY,UAAU;QACrC,WAAW,OAAA,CAAQ,MAAM,OAAO;IAClC,OAAO;QACL,MAAM,UAA4C,MAAM,OAAA;QACxD,KAAA,MAAW,QAAQ,QAAS;YAC1B,IAAI,KAAK,IAAA,KAAS,QAAQ;gBACxB,WAAW,OAAA,CAAQ,KAAK,IAAI;YAC9B;QACF;IACF;AACF;;AC5JA,IAAA,6BAAA,CAAA;AAAA,SAAA,4BAAA;IAAA,cAAA,IAAAyC;IAAA,sBAAA,IAAAC;AAAA;AAaO,SAASD,cACd,MAAA,EACA,SAAA;IAEA,OAAO,iBAAiB,QACrB,WAAA,CAAY,2BAA2B,YACvC,WAAA,CAAY;AACjB;AAEO,SAASC,sBACd,MAAA,EACA,UAII,CAAC,CAAA;IA5BP,IAAA1C;IA8BE,MAAM,EAAE,IAAA,EAAM,IAAA,EAAM,SAAA,EAAU,GAAI;IAClC,MAAM,aAAayC,cAAa,QAAQ;IACxC,MAAM,iBAAiB,OACnB,aAAa,KAAK,MAAA,EAAQ,cAC1B;IAEJ,OAAO,IAAI,SAAS,gBAAgB;QAClC,QAAA,CAAQzC,OAAA,QAAA,OAAA,KAAA,IAAA,KAAM,MAAA,KAAN,OAAAA,OAAgB;QACxB,YAAY,QAAA,OAAA,KAAA,IAAA,KAAM,UAAA;QAClB,SAAS,uBAAuB,MAAM;YACpC,aAAa;YACb,mBAAmB;QACrB;IACF;AACF;AAEA,SAAS,iBAAiB,GAAA;IACxB,MAAM,KAAK,GAAA,CAAI,OAAO,aAAa,CAAA;IACnC,MAAM,oBAAoB;IAE1B,OAAO,IAAI,eAAuB;QAChC,MAAM,MAAK,UAAA;YAnDf,IAAAA;YAoDM,MAAM,EAAE,KAAA,EAAO,IAAA,EAAK,GAAI,MAAM,GAAG,IAAA;YACjC,IAAI,MAAM;gBACR,WAAW,KAAA;gBACX;YACF;YACA,MAAM,OAAO,kBAAA,CAAkBA,OAAA,MAAM,KAAA,KAAN,OAAAA,OAAe;YAC9C,IAAI,MAAM;gBACR,WAAW,OAAA,CAAQ;YACrB;QACF;IACF;AACF;;ACtDO,SAAS,gBAAgB,SAAA;IAC9B,MAAM,SAAS,IAAI;IACnB,MAAM,SAAS,OAAO,QAAA,CAAS,SAAA;IAE/B,MAAM,OAAO,aAAA,GAAA,IAAI;IAEjB,MAAM,cAAc,OAAO,GAAU;QACnC,KAAK,MAAA,CAAO;QACZ,MAAM,OAAO,KAAA;QACb,MAAM,OAAO,KAAA,CAAM;IACrB;IAEA,MAAM,cAAc,OAAO;QACzB,KAAK,GAAA,CAAI;IACX;IAEA,MAAM,YAAY,OAAO;QACvB,KAAK,MAAA,CAAO;QAEZ,IAAI,KAAK,IAAA,KAAS,GAAG;YACnB,MAAM,OAAO,KAAA;YACb,MAAM,OAAO,KAAA;QACf;IACF;IAEA,OAAO;QACL,QAAQ,OAAO,QAAA,CACZ,WAAA,CAAY,2BAA2B,YACvC,WAAA,CAAY;QACf;QACA,UAAU;YACR,mBAAmB,OAAO;gBACxB,MAAM,OAAO,KAAA;gBACb,MAAM,OAAO,KAAA,CAAM;YACrB;YACA,gBAAgB,OAAO,MAAW,UAAoB;gBACpD,YAAY;YACd;YACA,cAAc,OAAO,SAAc;gBACjC,MAAM,UAAU;YAClB;YACA,gBAAgB,OAAO,GAAU;gBAC/B,MAAM,YAAY,GAAG;YACvB;YACA,kBAAkB,OAAO,QAAa,SAAc;gBAClD,YAAY;YACd;YACA,gBAAgB,OAAO,UAAe;gBACpC,MAAM,UAAU;YAClB;YACA,kBAAkB,OAAO,GAAU;gBACjC,MAAM,YAAY,GAAG;YACvB;YACA,iBAAiB,OAAO,OAAY,QAAgB;gBAClD,YAAY;YACd;YACA,eAAe,OAAO,SAAiB;gBACrC,MAAM,UAAU;YAClB;YACA,iBAAiB,OAAO,GAAU;gBAChC,MAAM,YAAY,GAAG;YACvB;QACF;IACF;AACF;;ACrCA,gBAAgBwC,YAAW,MAAA;IApC3B,IAAAxC,MAAA;IAqCE,WAAA,MAAiB,SAAS,OAAQ;QAChC,MAAM,UAAA,CAAU,KAAA,CAAAA,OAAA,MAAM,OAAA,CAAQ,EAAC,KAAf,OAAA,KAAA,IAAAA,KAAkB,KAAA,KAAlB,OAAA,KAAA,IAAA,GAAyB,OAAA;QAEzC,IAAI,YAAY,KAAA,KAAa,YAAY,IAAI;YAC3C;QACF;QAEA,MAAM;IACR;AACF;AAKO,SAAS,cACd,QAAA,EACA,SAAA;IAEA,MAAM,SAAS,0BAA0BwC,YAAW;IACpD,OAAO,OACJ,WAAA,CAAY,2BAA2B,YACvC,WAAA,CAAY;AACjB;;ACkNA,SAAS;IAGP,MAAM,UAAU;IAChB,OAAO,CAAA,OAAQ,QAAQ,KAAK,KAAA,CAAM;AACpC;AAOA,gBAAgBA,YAAW,MAAA;IACzB,MAAM,UAAU;IAEhB,WAAA,IAAe,SAAS,OAAQ;QAG9B,IAAI,yBAAyB,OAAO;YAClC,QAAQ;gBACN,IAAI,MAAM,EAAA;gBACV,SAAS,MAAM,OAAA,CAAQ,OAAA;gBACvB,QAAS,MAAc,MAAA;gBAAA,2BAAA;gBACvB,OAAQ,MAAc,KAAA;gBAAA,2BAAA;gBACtB,SAAS,MAAM,OAAA,CAAQ,GAAA,CAAI,CAAA;oBArSnC,IAAAxC,MAAA,IAAA,IAAA,IAAA,IAAA,IAAA;oBAqS8C,OAAA;wBACpC,OAAO;4BACL,SAAA,CAASA,OAAA,OAAO,KAAA,KAAP,OAAA,KAAA,IAAAA,KAAc,OAAA;4BACvB,eAAA,CAAe,KAAA,OAAO,KAAA,KAAP,OAAA,KAAA,IAAA,GAAc,YAAA;4BAC7B,MAAA,CAAM,KAAA,OAAO,KAAA,KAAP,OAAA,KAAA,IAAA,GAAc,IAAA;4BACpB,YAAA,CAAA,CAAY,KAAA,CAAA,KAAA,OAAO,KAAA,KAAP,OAAA,KAAA,IAAA,GAAc,SAAA,KAAd,OAAA,KAAA,IAAA,GAAyB,MAAA,IAAA,CACjC,KAAA,CAAA,KAAA,OAAO,KAAA,KAAP,OAAA,KAAA,IAAA,GAAc,SAAA,KAAd,OAAA,KAAA,IAAA,GAAyB,GAAA,CAAI,CAAC,UAAU,QAAA,CAAW;oCACjD;oCACA,IAAI,SAAS,EAAA;oCACb,UAAU,SAAS,QAAA;oCACnB,MAAM,SAAS,IAAA;gCACjB,CAAA,KACA,KAAA;wBACN;wBACA,eAAe,OAAO,YAAA;wBACtB,OAAO,OAAO,KAAA;oBAChB;gBAAA;YACF;QACF;QAEA,MAAM,OAAO,QAAQ;QAErB,IAAI,MAAM,MAAM;IAClB;AACF;AAEA,SAAS;IAGP,MAAM,oBAAoB;IAC1B,IAAI;IACJ,OAAO,CAAA;QApUT,IAAAA,MAAA,IAAA,IAAA,IAAA,IAAA,IAAA,IAAA,IAAA,IAAA,IAAA,IAAA,IAAA,IAAA,IAAA,IAAA,IAAA,IAAA;QAqUI,IAAI,sBAAsB,OAAO;YAC/B,MAAM,QAAA,CAAQA,OAAA,KAAK,OAAA,CAAQ,EAAC,KAAd,OAAA,KAAA,IAAAA,KAAiB,KAAA;YAC/B,IAAA,CAAI,KAAA,MAAM,aAAA,KAAN,OAAA,KAAA,IAAA,GAAqB,IAAA,EAAM;gBAC7B,wBAAwB;gBACxB,OAAO;oBACL,QAAQ;oBACR,SAAS,CAAA,4BAAA,EAA+B,MAAM,aAAA,CAAc,IAAI,CAAA,iBAAA,CAAA;gBAClE;YACF,OAAA,IAAA,CAAW,KAAA,CAAA,KAAA,CAAA,KAAA,MAAM,UAAA,KAAN,OAAA,KAAA,IAAA,EAAA,CAAmB,EAAA,KAAnB,OAAA,KAAA,IAAA,GAAuB,QAAA,KAAvB,OAAA,KAAA,IAAA,GAAiC,IAAA,EAAM;gBAChD,wBAAwB;gBACxB,MAAM,WAAW,MAAM,UAAA,CAAW,EAAC;gBACnC,IAAI,SAAS,KAAA,KAAU,GAAG;oBACxB,OAAO;wBACL,QAAQ;wBACR,SAAS,CAAA,wBAAA,EAA2B,SAAS,EAAE,CAAA,6CAAA,EAAA,CAAgD,KAAA,SAAS,QAAA,KAAT,OAAA,KAAA,IAAA,GAAmB,IAAI,CAAA,iBAAA,CAAA;oBACxH;gBACF,OAAO;oBACL,OAAO;wBACL,QAAQ;wBACR,SAAS,CAAA,aAAA,EAAgB,SAAS,EAAE,CAAA,6CAAA,EAAA,CAAgD,KAAA,SAAS,QAAA,KAAT,OAAA,KAAA,IAAA,GAAmB,IAAI,CAAA,iBAAA,CAAA;oBAC7G;gBACF;YACF,OAAA,IAAA,CAAW,KAAA,MAAM,aAAA,KAAN,OAAA,KAAA,IAAA,GAAqB,SAAA,EAAW;gBACzC,OAAO;oBACL,QAAQ;oBACR,SAAS,iBAAA,CAAiB,KAAA,MAAM,aAAA,KAAN,OAAA,KAAA,IAAA,GAAqB,SAAS;gBAC1D;YACF,OAAA,IAAA,CAAW,KAAA,CAAA,KAAA,CAAA,KAAA,MAAM,UAAA,KAAN,OAAA,KAAA,IAAA,EAAA,CAAmB,EAAA,KAAnB,OAAA,KAAA,IAAA,GAAuB,QAAA,KAAvB,OAAA,KAAA,IAAA,GAAiC,SAAA,EAAW;gBACrD,OAAO;oBACL,QAAQ;oBACR,SAAS,iBAAA,CAAiB,KAAA,CAAA,KAAA,CAAA,KAAA,MAAM,UAAA,KAAN,OAAA,KAAA,IAAA,EAAA,CAAmB,EAAA,KAAnB,OAAA,KAAA,IAAA,GAAuB,QAAA,KAAvB,OAAA,KAAA,IAAA,GAAiC,SAAS;gBACtE;YACF,OAAA,IACE,yBAAA,CAAA,CAAA,CACC,KAAA,KAAK,OAAA,CAAQ,EAAC,KAAd,OAAA,KAAA,IAAA,GAAiB,aAAA,MAAkB,mBAAA,CAAA,CAClC,KAAA,KAAK,OAAA,CAAQ,EAAC,KAAd,OAAA,KAAA,IAAA,GAAiB,aAAA,MAAkB,MAAA,GACrC;gBACA,wBAAwB;gBACxB,OAAO;oBACL,QAAQ;oBACR,SAAS;gBACX;YACF,OAAA,IACE,yBAAA,CAAA,CACA,KAAA,KAAK,OAAA,CAAQ,EAAC,KAAd,OAAA,KAAA,IAAA,GAAiB,aAAA,MAAkB,cACnC;gBACA,wBAAwB;gBACxB,OAAO;oBACL,QAAQ;oBACR,SAAS;gBACX;YACF;QACF;QAEA,MAAM,OAAO,kBACX,sBAAsB,SAAS,KAAK,OAAA,CAAQ,EAAC,CAAE,KAAA,CAAM,OAAA,GACjD,KAAK,OAAA,CAAQ,EAAC,CAAE,KAAA,CAAM,OAAA,GACtB,aAAa,QACb,KAAK,OAAA,CAAQ,EAAC,CAAE,IAAA,GAChB;QAGN,OAAO;IACT;IAEA,SAAS,iBAAiB,aAAA;QACxB,IAAI,qBAAqB,cACtB,OAAA,CAAQ,OAAO,QACf,OAAA,CAAQ,OAAO,OACf,OAAA,CAAQ,MAAM,OACd,OAAA,CAAQ,OAAO,OACf,OAAA,CAAQ,OAAO,OACf,OAAA,CAAQ,OAAO,OACf,OAAA,CAAQ,OAAO;QAElB,OAAO,CAAA,EAAG,mBAAkB,CAAA;IAC9B;AACF;AAEA,IAAM,qCAAqC,OACzC;AAaF,SAAS,sBACP,IAAA;IAEA,OACE,aAAa,QACb,KAAK,OAAA,IACL,KAAK,OAAA,CAAQ,EAAC,IACd,WAAW,KAAK,OAAA,CAAQ,EAAC;AAE7B;AAEA,SAAS,aAAa,IAAA;IACpB,OACE,aAAa,QACb,KAAK,OAAA,IACL,KAAK,OAAA,CAAQ,EAAC,IACd,UAAU,KAAK,OAAA,CAAQ,EAAC;AAE5B;AAKO,SAAS,aACd,GAAA,EACA,SAAA;IAGA,MAAM,KAIG;IAET,IAAI;IACJ,IAAI,OAAO,aAAA,IAAiB,KAAK;QAC/B,SAAS,0BAA0BwC,YAAW,MAAM,WAAA,CAClD,2BAAA,CACE,MAAA,OAAA,KAAA,IAAA,GAAI,2BAAA,KAAA,CAA+B,MAAA,OAAA,KAAA,IAAA,GAAI,uBAAA,IACnC;YACE,GAAG,EAAA;YACH,SAAS,KAAA;QACX,IACA;YACE,GAAG,EAAA;QACL;IAGV,OAAO;QACL,SAAS,SACP,KACA,qBAAkB,CAClB,MAAA,OAAA,KAAA,IAAA,GAAI,2BAAA,KAAA,CAA+B,MAAA,OAAA,KAAA,IAAA,GAAI,uBAAA,IACnC;YACE,GAAG,EAAA;YACH,SAAS,KAAA;QACX,IACA;YACE,GAAG,EAAA;QACL;IAER;IAEA,IAAI,MAAA,CAAO,GAAG,2BAAA,IAA+B,GAAG,uBAAA,GAA0B;QACxE,MAAM,0BAA0B,8BAA8B;QAC9D,OAAO,OAAO,WAAA,CAAY;IAC5B,OAAO;QACL,OAAO,OAAO,WAAA,CAAY;IAC5B;AACF;AAEA,SAAS,8BACP,SAAA;IAIA,MAAM,cAAc,IAAI;IACxB,IAAI,eAAe;IACnB,IAAI,qBAAqB;IACzB,IAAI,oCAAoC;IACxC,IAAI,wBAAwB;IAE5B,IAAI,uBACF,SAAA,CAAU,mCAAkC,IAAK,EAAC;IAEpD,MAAM,SAAS,CAAA,GAAA,8KAAA,CAAA,qBAAA;IAEf,OAAO,IAAI,gBAAgB;QACzB,MAAM,WAAU,KAAA,EAAO,UAAA;YACrB,MAAM,UAAU,OAAO;YACvB,qCAAqC;YAErC,MAAM,yBACJ,gBAAA,CACC,QAAQ,UAAA,CAAW,wBAClB,QAAQ,UAAA,CAAW,iBAAgB;YAEvC,IAAI,wBAAwB;gBAC1B,wBAAwB;gBACxB,sBAAsB;gBACtB,eAAe;gBACf;YACF;YAGA,IAAI,CAAC,uBAAuB;gBAC1B,WAAW,OAAA,CACT,YAAY,MAAA,CAAOH,CAAAA,GAAAA,8KAAAA,CAAAA,mBAAAA,EAAiB,QAAQ;gBAE9C;YACF,OAAO;gBACL,sBAAsB;YACxB;QACF;QACA,MAAM,OAAM,UAAA;YACV,IAAI;gBACF,IACE,CAAC,gBACD,yBAAA,CACC,UAAU,2BAAA,IACT,UAAU,uBAAA,GACZ;oBACA,wBAAwB;oBACxB,MAAM,UAAU,KAAK,KAAA,CAAM;oBAE3B,IAAI,0BAA2C;2BAC1C;qBACL;oBAEA,IAAI,mBAMY,KAAA;oBAEhB,IAAI,UAAU,2BAAA,EAA6B;wBAIzC,IAAI,QAAQ,aAAA,KAAkB,KAAA,GAAW;4BACvC,QAAQ,IAAA,CACN;wBAEJ;wBAEA,MAAM,mBAAmB,KAAK,KAAA,CAC5B,QAAQ,aAAA,CAAc,SAAA;wBAGxB,mBAAmB,MAAM,UAAU,2BAAA,CACjC;4BACE,MAAM,QAAQ,aAAA,CAAc,IAAA;4BAC5B,WAAW;wBACb,GACA,CAAA;4BAEE,0BAA0B;mCACrB;gCACH;oCACE,MAAM;oCACN,SAAS;oCACT,eAAe,QAAQ,aAAA;gCACzB;gCACA;oCACE,MAAM;oCACN,MAAM,QAAQ,aAAA,CAAc,IAAA;oCAC5B,SAAS,KAAK,SAAA,CAAU;gCAC1B;6BACF;4BAEA,OAAO;wBACT;oBAEJ;oBACA,IAAI,UAAU,uBAAA,EAAyB;wBACrC,MAAM,YAA6B;4BACjC,OAAO,EAAC;wBACV;wBACA,KAAA,MAAWhB,SAAQ,QAAQ,UAAA,CAAY;4BACrC,UAAU,KAAA,CAAM,IAAA,CAAK;gCACnB,IAAIA,MAAK,EAAA;gCACT,MAAM;gCACN,MAAM;oCACJ,MAAMA,MAAK,QAAA,CAAS,IAAA;oCACpB,WAAW,KAAK,KAAA,CAAMA,MAAK,QAAA,CAAS,SAAS;gCAC/C;4BACF;wBACF;wBACA,IAAI,gBAAgB;wBACpB,IAAI;4BACF,mBAAmB,MAAM,UAAU,uBAAA,CACjC,WACA,CAAA;gCACE,IAAI,QAAQ;oCACV,MAAM,EAAE,YAAA,EAAc,aAAA,EAAe,gBAAA,EAAiB,GACpD;oCAEF,0BAA0B;2CACrB;wCAAA,+DAAA;2CAEC,kBAAkB,IAClB;4CACE;gDACE,MAAM;gDACN,SAAS;gDACT,YAAY,QAAQ,UAAA,CAAW,GAAA,CAC7B,CAAC,KAAA,CAAkB;wDACjB,IAAI,GAAG,EAAA;wDACP,MAAM;wDACN,UAAU;4DACR,MAAM,GAAG,QAAA,CAAS,IAAA;4DAAA,wGAAA;4DAElB,WAAW,KAAK,SAAA,CACd,GAAG,QAAA,CAAS,SAAA;wDAEhB;oDACF,CAAA;4CAEJ;yCACF,GACA,EAAC;wCAAA,0CAAA;wCAEL;4CACE,MAAM;4CACN;4CACA,MAAM;4CACN,SAAS,KAAK,SAAA,CAAU;wCAC1B;qCACF;oCACA;gCACF;gCAEA,OAAO;4BACT;wBAEJ,EAAA,OAAS,GAAG;4BACV,QAAQ,KAAA,CAAM,0CAA0C;wBAC1D;oBACF;oBAEA,IAAI,CAAC,kBAAkB;wBAIrB,WAAW,OAAA,CACT,YAAY,MAAA,CACVgB,CAAAA,GAAAA,8KAAAA,CAAAA,mBAAAA,EACE,QAAQ,aAAA,GAAgB,kBAAkB,cAAA,oCAAA;wBAE1C,KAAK,KAAA,CAAM;wBAIjB;oBACF,OAAA,IAAW,OAAO,qBAAqB,UAAU;wBAE/C,WAAW,OAAA,CACT,YAAY,MAAA,CAAOA,CAAAA,GAAAA,8KAAAA,CAAAA,mBAAAA,EAAiB,QAAQ;wBAE9C,oCAAoC;wBACpC;oBACF;oBAOA,MAAM,oBAA2C;wBAC/C,GAAG,SAAA;wBACH,SAAS,KAAA;oBACX;oBAEA,UAAU,OAAA,GAAU,KAAA;oBAEpB,MAAM,eAAe,aAAa,kBAAkB;wBAClD,GAAG,iBAAA;wBACH,CAAC,mCAAkC,EAAG;oBACxC;oBAEA,MAAM,SAAS,aAAa,SAAA;oBAE5B,MAAO,KAAM;wBACX,MAAM,EAAE,IAAA,EAAM,KAAA,EAAM,GAAI,MAAM,OAAO,IAAA;wBACrC,IAAI,MAAM;4BACR;wBACF;wBACA,WAAW,OAAA,CAAQ;oBACrB;gBACF;YACF,SAAE;gBACA,IAAI,UAAU,OAAA,IAAW,mCAAmC;oBAC1D,MAAM,UAAU,OAAA,CAAQ;gBAC1B;YACF;QACF;IACF;AACF;;AC9pBA,eAAsB,gBACpB,GAAA,EACA,EAAA,EACA,OAAA;IAlDF,IAAArC;IAsDE,MAAM,MAAA,CAAMA,OAAA,IAAI,IAAA,KAAJ,OAAA,KAAA,IAAAA,KAAU,MAAA;IAEtB,IAAI,CAAC,KAAK;QACR,IAAI,IAAI,KAAA,EAAO,MAAM,IAAI,MAAM,IAAI,KAAK;aACnC,MAAM,IAAI,MAAM;IACvB;IAEA,MAAM,cAAc,MAAM,MAAM,KAAK;QACnC,QAAQ;QACR,SAAS;YACP,QAAQ;YACR,GAAG,WAAA,OAAA,KAAA,IAAA,QAAS,OAAA;QACd;IACF;IAEA,OAAO,SAAS,aAAa,KAAA,GAAW,IAAI,WAAA,CAC1C;AAEJ;;AC/DO,SAAS,iBACd,GAAA,EACA,QAAA,EACA,IAAA,EACA,IAAA;IAbF,IAAAA;IAeE,SAAS,SAAA,CAAA,CAAUA,OAAA,QAAA,OAAA,KAAA,IAAA,KAAM,MAAA,KAAN,OAAAA,OAAgB,KAAK;QACtC,gBAAgB;QAChB,GAAG,QAAA,OAAA,KAAA,IAAA,KAAM,OAAA;IACX;IAEA,IAAI,kBAAkB;IAEtB,IAAI,MAAM;QACR,kBAAkB,aAAa,KAAK,MAAA,EAAQ;IAC9C;IAEA,MAAM,SAAS,gBAAgB,SAAA;IAC/B,SAAS;QACP,OAAO,IAAA,GAAO,IAAA,CAAK,CAAC,EAAE,IAAA,EAAM,KAAA,EAAM;YAChC,IAAI,MAAM;gBACR,SAAS,GAAA;gBACT;YACF;YACA,SAAS,KAAA,CAAM;YACf;QACF;IACF;IACA;AACF;;ACrBO,IAAM,wBAAN,cAAoC;IACzC,YAAY,GAAA,EAAqB,IAAA,EAAqB,IAAA,CAAmB;QACvE,IAAI,kBAAkB;QAEtB,IAAI,MAAM;YACR,kBAAkB,aAAa,KAAK,MAAA,EAAQ;QAC9C;QAEA,KAAA,CAAM,iBAAwB;YAC5B,GAAG,IAAA;YACH,QAAQ;YACR,SAAS,uBAAuB,MAAM;gBACpC,aAAa;YACf;QACF;IACF;AACF;;AnFDO,IAAMa,cAAa,0LAAA,CAAA,aAAA;AAMnB,IAAM,SAAS,0LAAA,CAAA,aAAA"}},
    {"offset": {"line": 5923, "column": 0}, "map": {"version":3,"sources":[],"names":[],"mappings":"A"}}]
}